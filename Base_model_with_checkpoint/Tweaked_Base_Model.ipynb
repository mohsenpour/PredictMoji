{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "kjt9iuwEgm5T",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "tGMIDRTHgm5b",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "26_C6S4Ugm5e",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    header = ['tweet','label']\n",
    "    data_set = pandas.read_csv('cleaned_data.txt',delimiter='\\t',names = header)\n",
    "    return data_set\n",
    "\n",
    "def split_tweet(tweet):\n",
    "    return tweet.split()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "bmOzSIDpgm5h",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "data_set = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ey347C1Zgm5l",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "glove = torchtext.vocab.GloVe(name=\"twitter.27B\",dim=50)\n",
    "\n",
    "# inser padding character into glove embedding, we overwrite the first element and that's okay\n",
    "# because we don't use the first element in our vocab\n",
    "glove.vectors[0] = torch.tensor(np.zeros(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "sQ8JIzR4gm5q",
    "colab_type": "code",
    "outputId": "9ad48256-ca70-48b8-f9cf-544457f38db6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250951"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "tA4Ahaifgm53",
    "colab_type": "code",
    "outputId": "319c1ef4-f50b-41ef-df4d-5f9f24a26e7c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1969.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hoping i dont screw up this interview</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel like a baby kangaroo stuck in its mothe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>girl ppl should be happy i even remembered her...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh ,  the irony if misha wins the choice tv sc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i miss you to  ,  you so fake now</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i miss you to  ,  you so fake now</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i know</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bacolod please ? !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>things can change so quickly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>all me &amp;amp ;  vic do is laugh .  .  anybody o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>do u have to remind me i was bored right after...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>its ok love .  you are sweet ,  but i know you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nainai has said bitch twice in 4 minutes and i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>louis thank you for everything you do you dese...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>impractical jokers its almost always on in our...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>my eyes are in pain .  cant get over the movie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>daydrinking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>genitin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>like like like</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>eye need to get these brows done asap</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>i almost choked on my retainer just now</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ms .  mendiola works miracles! shes so great</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>oh and parking dont even get me started on the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>oh and parking dont even get me started on the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>trying to stay my ass in the burbs but im gett...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>oh my god riley is so cute</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>why doe</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>i cant wait to be on the beach tomorrow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>- warrior basti also will have his claws cut .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>not bein selfish .  .  im only workin wit one arm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250921</th>\n",
       "      <td>my cousin got her pregnant belly on me and her...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250922</th>\n",
       "      <td>my cousin got her pregnant belly on me and her...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250923</th>\n",
       "      <td>cory wouldnt lie to us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250924</th>\n",
       "      <td>cory wouldnt lie to us</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250925</th>\n",
       "      <td>sometimes i get mad at ethan for what he said ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250926</th>\n",
       "      <td>sometimes i get mad at ethan for what he said ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250927</th>\n",
       "      <td>i really wish paula patton would update her tw...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250928</th>\n",
       "      <td>i really wish paula patton would update her tw...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250929</th>\n",
       "      <td>same</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250930</th>\n",
       "      <td>same</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250931</th>\n",
       "      <td>if you want something new and interesting ,  y...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250932</th>\n",
       "      <td>if you want something new and interesting ,  y...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250933</th>\n",
       "      <td>forgot how scared i get when i sleep in an emp...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250934</th>\n",
       "      <td>forgot how scared i get when i sleep in an emp...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250935</th>\n",
       "      <td>i just seen him a few days ago .  i hate to te...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250936</th>\n",
       "      <td>i just seen him a few days ago .  i hate to te...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250937</th>\n",
       "      <td>no loseee</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250938</th>\n",
       "      <td>no loseee</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250939</th>\n",
       "      <td>oops im sure itll come back soon .  keep waiting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250940</th>\n",
       "      <td>oops im sure itll come back soon .  keep waiting</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250941</th>\n",
       "      <td>since you wont ill call her out! what are you ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250942</th>\n",
       "      <td>since you wont ill call her out! what are you ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250943</th>\n",
       "      <td>i guess the nba still rigged</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250944</th>\n",
       "      <td>i guess the nba still rigged</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250945</th>\n",
       "      <td>cant wait</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250946</th>\n",
       "      <td>cant wait</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250947</th>\n",
       "      <td>we gotta get our tickets for the concert</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250948</th>\n",
       "      <td>we gotta get our tickets for the concert</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250949</th>\n",
       "      <td>cant wait to get mine .  its bigger than the s...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250950</th>\n",
       "      <td>cant wait to get mine .  its bigger than the s...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250951 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet  label\n",
       "0                   hoping i dont screw up this interview      0\n",
       "1       i feel like a baby kangaroo stuck in its mothe...      1\n",
       "2       girl ppl should be happy i even remembered her...      0\n",
       "3       oh ,  the irony if misha wins the choice tv sc...      2\n",
       "4                       i miss you to  ,  you so fake now      2\n",
       "5                       i miss you to  ,  you so fake now      4\n",
       "6                                                  i know      1\n",
       "7                                      bacolod please ? !      0\n",
       "8                            things can change so quickly      1\n",
       "9       all me &amp ;  vic do is laugh .  .  anybody o...      1\n",
       "10      do u have to remind me i was bored right after...      0\n",
       "11      its ok love .  you are sweet ,  but i know you...      0\n",
       "12      nainai has said bitch twice in 4 minutes and i...      0\n",
       "13      louis thank you for everything you do you dese...      0\n",
       "14      impractical jokers its almost always on in our...      0\n",
       "15      my eyes are in pain .  cant get over the movie...      1\n",
       "16                                            daydrinking      0\n",
       "17                                                genitin      0\n",
       "18                                         like like like      0\n",
       "19                  eye need to get these brows done asap      1\n",
       "20                i almost choked on my retainer just now      1\n",
       "21           ms .  mendiola works miracles! shes so great      1\n",
       "22      oh and parking dont even get me started on the...      2\n",
       "23      oh and parking dont even get me started on the...      4\n",
       "24      trying to stay my ass in the burbs but im gett...      1\n",
       "25                             oh my god riley is so cute      0\n",
       "26                                                why doe      0\n",
       "27                i cant wait to be on the beach tomorrow      0\n",
       "28        - warrior basti also will have his claws cut .       0\n",
       "29      not bein selfish .  .  im only workin wit one arm      0\n",
       "...                                                   ...    ...\n",
       "250921  my cousin got her pregnant belly on me and her...      3\n",
       "250922  my cousin got her pregnant belly on me and her...      5\n",
       "250923                             cory wouldnt lie to us      1\n",
       "250924                             cory wouldnt lie to us      5\n",
       "250925  sometimes i get mad at ethan for what he said ...      3\n",
       "250926  sometimes i get mad at ethan for what he said ...      5\n",
       "250927  i really wish paula patton would update her tw...      3\n",
       "250928  i really wish paula patton would update her tw...      5\n",
       "250929                                               same      2\n",
       "250930                                               same      5\n",
       "250931  if you want something new and interesting ,  y...      2\n",
       "250932  if you want something new and interesting ,  y...      5\n",
       "250933  forgot how scared i get when i sleep in an emp...      3\n",
       "250934  forgot how scared i get when i sleep in an emp...      5\n",
       "250935  i just seen him a few days ago .  i hate to te...      1\n",
       "250936  i just seen him a few days ago .  i hate to te...      5\n",
       "250937                                          no loseee      3\n",
       "250938                                          no loseee      5\n",
       "250939   oops im sure itll come back soon .  keep waiting      2\n",
       "250940   oops im sure itll come back soon .  keep waiting      5\n",
       "250941  since you wont ill call her out! what are you ...      3\n",
       "250942  since you wont ill call her out! what are you ...      5\n",
       "250943                       i guess the nba still rigged      2\n",
       "250944                       i guess the nba still rigged      5\n",
       "250945                                          cant wait      2\n",
       "250946                                          cant wait      5\n",
       "250947           we gotta get our tickets for the concert      3\n",
       "250948           we gotta get our tickets for the concert      5\n",
       "250949  cant wait to get mine .  its bigger than the s...      3\n",
       "250950  cant wait to get mine .  its bigger than the s...      5\n",
       "\n",
       "[250951 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "cWYeV80Sgm59",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def data_set_to_glove_index(glove_dict):\n",
    "    tweets_ints, encoded_labels = [],[]\n",
    "    for i in range(len(data_set)):\n",
    "        tweet = data_set['tweet'][i]\n",
    "        label = data_set['label'][i]\n",
    "        if(type(tweet) != str):\n",
    "            continue\n",
    "        idxs = [glove_dict.stoi[w]        # lookup the index of word\n",
    "            for w in tweet.split()\n",
    "            if w in glove_dict.stoi] # keep words that has an embedding\n",
    "        if not idxs: # ignore tweets without any word with an embedding\n",
    "            continue\n",
    "        tweets_ints.append(idxs)\n",
    "        encoded_labels.append(label)\n",
    "    return tweets_ints, encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "uci6Anmugm5-",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "tweets_ints, encoded_labels = data_set_to_glove_index(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "sB-kS3issuZ8",
    "colab_type": "code",
    "outputId": "6acf9391-814c-4bf6-cebe-03f15a4fc9a5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length tweets: 0\n",
      "Maximum tweet length: 62\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# outlier review stats\n",
    "tweets_lens = Counter([len(x) for x in tweets_ints])\n",
    "print(\"Zero-length tweets: {}\".format(tweets_lens[0]))\n",
    "print(\"Maximum tweet length: {}\".format(max(tweets_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "CxaLejWos_Tw",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def pad_features(tweets_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(tweets_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(tweets_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Wy8MFokXs_9K",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "seq_length = max(tweets_lens)\n",
    "\n",
    "features = pad_features(tweets_ints, seq_length=seq_length)\n",
    "encoded_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "R9siA7B4tAc7",
    "colab_type": "code",
    "outputId": "6991aba0-810a-499b-de76-320b7e8ea5cd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...    85    53  2706]\n",
      " [    0     0     0 ...   221  9193 50306]\n",
      " [    0     0     0 ...   316   226   325]\n",
      " ...\n",
      " [    0     0     0 ...    13  1863   328]\n",
      " [    0     0     0 ... 58381  1465     1]\n",
      " [    0     0     0 ...  1104    96  4799]]\n"
     ]
    }
   ],
   "source": [
    "print(features[:30,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gm-jUzQ8teCo",
    "colab_type": "text"
   },
   "source": [
    "### Train, Validation, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "8yZGBcHztBGC",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "features, encoded_labels = shuffle(features, encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "0nYglhndtgYi",
    "colab_type": "code",
    "outputId": "97567e35-351d-4c1e-be5c-f55e6a2e6841",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(199848, 62) \n",
      "Validation set: \t(24981, 62) \n",
      "Test set: \t\t(24981, 62)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BV-wQQmb5d7j",
    "colab_type": "text"
   },
   "source": [
    "### DataLoader and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "FHgkWFzb5eRF",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "DxNTXSGU5mpX",
    "colab_type": "code",
    "outputId": "3aaf53ca-6996-47e2-a6d2-d1de10ebddc5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3996"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkI6wmX8gm6U",
    "colab_type": "text"
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "TdnI3kw0gm6V",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class TweetLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, use_gpu):\n",
    "        super(TweetLSTM, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size,bidirectional=True,batch_first=True)\n",
    "        self.fc = nn.Linear(2 * hidden_size, num_classes) # 2 * hidden_size because LSTM is bidrectional \n",
    "    def forward(self, x):\n",
    "        # Look up the embedding\n",
    "        x = self.emb(x)\n",
    "        # Set an initial hidden state and cell state\n",
    "        \n",
    "        if self.use_gpu:\n",
    "          h0 = torch.zeros(2, x.size(0), self.hidden_size).cuda()\n",
    "          c0 = torch.zeros(2, x.size(0), self.hidden_size).cuda()\n",
    "        else:\n",
    "          h0 = torch.zeros(2, x.size(0), self.hidden_size)\n",
    "          c0 = torch.zeros(2, x.size(0), self.hidden_size)\n",
    "        # Forward propagate the LSTM\n",
    "        out, _ = self.rnn(x, (h0, c0))\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "num_classes = 6\n",
    "model = TweetLSTM(input_size=50, hidden_size=50, num_classes=num_classes,use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "90sHCtA9gm6c",
    "colab_type": "code",
    "outputId": "db0acbaf-729f-437d-d0d8-332bd840aac5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14993987975951903"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_accuracy(model, data_loader, use_gpu):\n",
    "    if use_gpu:\n",
    "      model.cuda()\n",
    "    correct, total = 0, 0\n",
    "    for tweets, labels in data_loader:\n",
    "        if use_gpu:\n",
    "          tweets,labels  = tweets.cuda(), labels.cuda()\n",
    "        output = model(tweets)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += labels.shape[0]\n",
    "    return float(correct) / float(total)\n",
    "  \n",
    "\n",
    "get_accuracy(model, test_loader,use_gpu = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "XUd3LjcAgm6g",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def train_rnn_network(model, train, valid, num_epochs=5, learning_rate=1e-5,use_gpu=True):\n",
    "    if use_gpu:\n",
    "      model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses, train_acc, valid_acc = [], [], []\n",
    "    epochs = []\n",
    "    counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_loss = []\n",
    "        for tweets, labels in train:\n",
    "            if use_gpu:\n",
    "              tweets,labels  = tweets.cuda(), labels.cuda()\n",
    "            counter += 1\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(tweets)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(float(loss))\n",
    "            if counter % 100 == 0:\n",
    "              print(\"Step %d of Epoch: %d; Loss %f \" % ( counter/100 ,epoch+1,np.mean(batch_loss)))\n",
    "        counter = 0\n",
    "        epoch_loss = np.mean(batch_loss)\n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "        epochs.append(epoch)\n",
    "        train_acc.append(get_accuracy(model, train_loader,use_gpu))\n",
    "        valid_acc.append(get_accuracy(model, valid_loader,use_gpu))\n",
    "        print(\"Final Result for Epoch %d: Loss %f; Val Acc %f; Train Acc %f\" % (\n",
    "              epoch+1, epoch_loss, valid_acc[-1], train_acc[-1]))\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "        \n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(losses, label=\"Train\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, train_acc, label=\"Train\")\n",
    "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "aPz9Hioigm6i",
    "colab_type": "code",
    "outputId": "88a732b2-68b3-4e48-fdfb-92baae388088",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70472.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 of Epoch: 1; Loss 1.760004 \n",
      "Step 2 of Epoch: 1; Loss 1.747461 \n",
      "Step 3 of Epoch: 1; Loss 1.733787 \n",
      "Step 4 of Epoch: 1; Loss 1.723912 \n",
      "Step 5 of Epoch: 1; Loss 1.718774 \n",
      "Step 6 of Epoch: 1; Loss 1.712716 \n",
      "Step 7 of Epoch: 1; Loss 1.706982 \n",
      "Step 8 of Epoch: 1; Loss 1.703413 \n",
      "Step 9 of Epoch: 1; Loss 1.700090 \n",
      "Step 10 of Epoch: 1; Loss 1.698075 \n",
      "Step 11 of Epoch: 1; Loss 1.695362 \n",
      "Step 12 of Epoch: 1; Loss 1.692588 \n",
      "Step 13 of Epoch: 1; Loss 1.690595 \n",
      "Step 14 of Epoch: 1; Loss 1.688041 \n",
      "Step 15 of Epoch: 1; Loss 1.685510 \n",
      "Step 16 of Epoch: 1; Loss 1.683589 \n",
      "Step 17 of Epoch: 1; Loss 1.682424 \n",
      "Step 18 of Epoch: 1; Loss 1.681629 \n",
      "Step 19 of Epoch: 1; Loss 1.680230 \n",
      "Step 20 of Epoch: 1; Loss 1.679206 \n",
      "Step 21 of Epoch: 1; Loss 1.677623 \n",
      "Step 22 of Epoch: 1; Loss 1.676729 \n",
      "Step 23 of Epoch: 1; Loss 1.675892 \n",
      "Step 24 of Epoch: 1; Loss 1.674796 \n",
      "Step 25 of Epoch: 1; Loss 1.673623 \n",
      "Step 26 of Epoch: 1; Loss 1.672891 \n",
      "Step 27 of Epoch: 1; Loss 1.672220 \n",
      "Step 28 of Epoch: 1; Loss 1.671478 \n",
      "Step 29 of Epoch: 1; Loss 1.670742 \n",
      "Step 30 of Epoch: 1; Loss 1.669650 \n",
      "Step 31 of Epoch: 1; Loss 1.668688 \n",
      "Step 32 of Epoch: 1; Loss 1.668053 \n",
      "Step 33 of Epoch: 1; Loss 1.667173 \n",
      "Step 34 of Epoch: 1; Loss 1.666597 \n",
      "Step 35 of Epoch: 1; Loss 1.665975 \n",
      "Step 36 of Epoch: 1; Loss 1.665311 \n",
      "Step 37 of Epoch: 1; Loss 1.664961 \n",
      "Step 38 of Epoch: 1; Loss 1.664587 \n",
      "Step 39 of Epoch: 1; Loss 1.663711 \n",
      "Final Result for Epoch 1: Loss 1.663513; Val Acc 0.319719; Train Acc 0.326211\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 2; Loss 1.635147 \n",
      "Step 2 of Epoch: 2; Loss 1.629239 \n",
      "Step 3 of Epoch: 2; Loss 1.635300 \n",
      "Step 4 of Epoch: 2; Loss 1.634905 \n",
      "Step 5 of Epoch: 2; Loss 1.634960 \n",
      "Step 6 of Epoch: 2; Loss 1.637151 \n",
      "Step 7 of Epoch: 2; Loss 1.634776 \n",
      "Step 8 of Epoch: 2; Loss 1.635785 \n",
      "Step 9 of Epoch: 2; Loss 1.635757 \n",
      "Step 10 of Epoch: 2; Loss 1.635836 \n",
      "Step 11 of Epoch: 2; Loss 1.636109 \n",
      "Step 12 of Epoch: 2; Loss 1.636879 \n",
      "Step 13 of Epoch: 2; Loss 1.635193 \n",
      "Step 14 of Epoch: 2; Loss 1.634588 \n",
      "Step 15 of Epoch: 2; Loss 1.635055 \n",
      "Step 16 of Epoch: 2; Loss 1.634954 \n",
      "Step 17 of Epoch: 2; Loss 1.634944 \n",
      "Step 18 of Epoch: 2; Loss 1.635197 \n",
      "Step 19 of Epoch: 2; Loss 1.634644 \n",
      "Step 20 of Epoch: 2; Loss 1.634827 \n",
      "Step 21 of Epoch: 2; Loss 1.633598 \n",
      "Step 22 of Epoch: 2; Loss 1.633360 \n",
      "Step 23 of Epoch: 2; Loss 1.633264 \n",
      "Step 24 of Epoch: 2; Loss 1.632751 \n",
      "Step 25 of Epoch: 2; Loss 1.632708 \n",
      "Step 26 of Epoch: 2; Loss 1.632387 \n",
      "Step 27 of Epoch: 2; Loss 1.632401 \n",
      "Step 28 of Epoch: 2; Loss 1.632005 \n",
      "Step 29 of Epoch: 2; Loss 1.631199 \n",
      "Step 30 of Epoch: 2; Loss 1.631165 \n",
      "Step 31 of Epoch: 2; Loss 1.630886 \n",
      "Step 32 of Epoch: 2; Loss 1.630373 \n",
      "Step 33 of Epoch: 2; Loss 1.629939 \n",
      "Step 34 of Epoch: 2; Loss 1.629631 \n",
      "Step 35 of Epoch: 2; Loss 1.629476 \n",
      "Step 36 of Epoch: 2; Loss 1.629250 \n",
      "Step 37 of Epoch: 2; Loss 1.629094 \n",
      "Step 38 of Epoch: 2; Loss 1.629098 \n",
      "Step 39 of Epoch: 2; Loss 1.628751 \n",
      "Final Result for Epoch 2: Loss 1.628510; Val Acc 0.326212; Train Acc 0.339299\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 3; Loss 1.608805 \n",
      "Step 2 of Epoch: 3; Loss 1.620863 \n",
      "Step 3 of Epoch: 3; Loss 1.621435 \n",
      "Step 4 of Epoch: 3; Loss 1.617075 \n",
      "Step 5 of Epoch: 3; Loss 1.615279 \n",
      "Step 6 of Epoch: 3; Loss 1.614641 \n",
      "Step 7 of Epoch: 3; Loss 1.614861 \n",
      "Step 8 of Epoch: 3; Loss 1.616230 \n",
      "Step 9 of Epoch: 3; Loss 1.615609 \n",
      "Step 10 of Epoch: 3; Loss 1.615620 \n",
      "Step 11 of Epoch: 3; Loss 1.616344 \n",
      "Step 12 of Epoch: 3; Loss 1.615630 \n",
      "Step 13 of Epoch: 3; Loss 1.615118 \n",
      "Step 14 of Epoch: 3; Loss 1.615077 \n",
      "Step 15 of Epoch: 3; Loss 1.614652 \n",
      "Step 16 of Epoch: 3; Loss 1.615195 \n",
      "Step 17 of Epoch: 3; Loss 1.614539 \n",
      "Step 18 of Epoch: 3; Loss 1.614238 \n",
      "Step 19 of Epoch: 3; Loss 1.614318 \n",
      "Step 20 of Epoch: 3; Loss 1.615067 \n",
      "Step 21 of Epoch: 3; Loss 1.613928 \n",
      "Step 22 of Epoch: 3; Loss 1.614147 \n",
      "Step 23 of Epoch: 3; Loss 1.613815 \n",
      "Step 24 of Epoch: 3; Loss 1.613683 \n",
      "Step 25 of Epoch: 3; Loss 1.613870 \n",
      "Step 26 of Epoch: 3; Loss 1.613783 \n",
      "Step 27 of Epoch: 3; Loss 1.613785 \n",
      "Step 28 of Epoch: 3; Loss 1.613534 \n",
      "Step 29 of Epoch: 3; Loss 1.613537 \n",
      "Step 30 of Epoch: 3; Loss 1.613695 \n",
      "Step 31 of Epoch: 3; Loss 1.613284 \n",
      "Step 32 of Epoch: 3; Loss 1.613462 \n",
      "Step 33 of Epoch: 3; Loss 1.613346 \n",
      "Step 34 of Epoch: 3; Loss 1.613646 \n",
      "Step 35 of Epoch: 3; Loss 1.613635 \n",
      "Step 36 of Epoch: 3; Loss 1.613705 \n",
      "Step 37 of Epoch: 3; Loss 1.613585 \n",
      "Step 38 of Epoch: 3; Loss 1.613563 \n",
      "Step 39 of Epoch: 3; Loss 1.613728 \n",
      "Final Result for Epoch 3: Loss 1.613241; Val Acc 0.329138; Train Acc 0.345220\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 4; Loss 1.609253 \n",
      "Step 2 of Epoch: 4; Loss 1.605741 \n",
      "Step 3 of Epoch: 4; Loss 1.606751 \n",
      "Step 4 of Epoch: 4; Loss 1.605275 \n",
      "Step 5 of Epoch: 4; Loss 1.603744 \n",
      "Step 6 of Epoch: 4; Loss 1.603260 \n",
      "Step 7 of Epoch: 4; Loss 1.603320 \n",
      "Step 8 of Epoch: 4; Loss 1.602324 \n",
      "Step 9 of Epoch: 4; Loss 1.601444 \n",
      "Step 10 of Epoch: 4; Loss 1.599954 \n",
      "Step 11 of Epoch: 4; Loss 1.600557 \n",
      "Step 12 of Epoch: 4; Loss 1.601234 \n",
      "Step 13 of Epoch: 4; Loss 1.600769 \n",
      "Step 14 of Epoch: 4; Loss 1.601078 \n",
      "Step 15 of Epoch: 4; Loss 1.600864 \n",
      "Step 16 of Epoch: 4; Loss 1.599843 \n",
      "Step 17 of Epoch: 4; Loss 1.599652 \n",
      "Step 18 of Epoch: 4; Loss 1.600307 \n",
      "Step 19 of Epoch: 4; Loss 1.600353 \n",
      "Step 20 of Epoch: 4; Loss 1.600771 \n",
      "Step 21 of Epoch: 4; Loss 1.600756 \n",
      "Step 22 of Epoch: 4; Loss 1.600579 \n",
      "Step 23 of Epoch: 4; Loss 1.600715 \n",
      "Step 24 of Epoch: 4; Loss 1.601100 \n",
      "Step 25 of Epoch: 4; Loss 1.600634 \n",
      "Step 26 of Epoch: 4; Loss 1.599981 \n",
      "Step 27 of Epoch: 4; Loss 1.600018 \n",
      "Step 28 of Epoch: 4; Loss 1.600396 \n",
      "Step 29 of Epoch: 4; Loss 1.600513 \n",
      "Step 30 of Epoch: 4; Loss 1.600414 \n",
      "Step 31 of Epoch: 4; Loss 1.600685 \n",
      "Step 32 of Epoch: 4; Loss 1.600821 \n",
      "Step 33 of Epoch: 4; Loss 1.600730 \n",
      "Step 34 of Epoch: 4; Loss 1.600892 \n",
      "Step 35 of Epoch: 4; Loss 1.601236 \n",
      "Step 36 of Epoch: 4; Loss 1.601394 \n",
      "Step 37 of Epoch: 4; Loss 1.601334 \n",
      "Step 38 of Epoch: 4; Loss 1.601533 \n",
      "Step 39 of Epoch: 4; Loss 1.601738 \n",
      "Final Result for Epoch 4: Loss 1.601738; Val Acc 0.333427; Train Acc 0.353979\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 5; Loss 1.582517 \n",
      "Step 2 of Epoch: 5; Loss 1.586779 \n",
      "Step 3 of Epoch: 5; Loss 1.587348 \n",
      "Step 4 of Epoch: 5; Loss 1.585644 \n",
      "Step 5 of Epoch: 5; Loss 1.584665 \n",
      "Step 6 of Epoch: 5; Loss 1.586856 \n",
      "Step 7 of Epoch: 5; Loss 1.587546 \n",
      "Step 8 of Epoch: 5; Loss 1.587311 \n",
      "Step 9 of Epoch: 5; Loss 1.587592 \n",
      "Step 10 of Epoch: 5; Loss 1.587762 \n",
      "Step 11 of Epoch: 5; Loss 1.588816 \n",
      "Step 12 of Epoch: 5; Loss 1.589460 \n",
      "Step 13 of Epoch: 5; Loss 1.590322 \n",
      "Step 14 of Epoch: 5; Loss 1.588943 \n",
      "Step 15 of Epoch: 5; Loss 1.588900 \n",
      "Step 16 of Epoch: 5; Loss 1.589058 \n",
      "Step 17 of Epoch: 5; Loss 1.588509 \n",
      "Step 18 of Epoch: 5; Loss 1.588796 \n",
      "Step 19 of Epoch: 5; Loss 1.588928 \n",
      "Step 20 of Epoch: 5; Loss 1.590200 \n",
      "Step 21 of Epoch: 5; Loss 1.590440 \n",
      "Step 22 of Epoch: 5; Loss 1.590762 \n",
      "Step 23 of Epoch: 5; Loss 1.590321 \n",
      "Step 24 of Epoch: 5; Loss 1.589917 \n",
      "Step 25 of Epoch: 5; Loss 1.590138 \n",
      "Step 26 of Epoch: 5; Loss 1.590353 \n",
      "Step 27 of Epoch: 5; Loss 1.590332 \n",
      "Step 28 of Epoch: 5; Loss 1.590679 \n",
      "Step 29 of Epoch: 5; Loss 1.590473 \n",
      "Step 30 of Epoch: 5; Loss 1.591117 \n",
      "Step 31 of Epoch: 5; Loss 1.590975 \n",
      "Step 32 of Epoch: 5; Loss 1.590923 \n",
      "Step 33 of Epoch: 5; Loss 1.590902 \n",
      "Step 34 of Epoch: 5; Loss 1.590980 \n",
      "Step 35 of Epoch: 5; Loss 1.591176 \n",
      "Step 36 of Epoch: 5; Loss 1.591563 \n",
      "Step 37 of Epoch: 5; Loss 1.591668 \n",
      "Step 38 of Epoch: 5; Loss 1.591553 \n",
      "Step 39 of Epoch: 5; Loss 1.591812 \n",
      "Final Result for Epoch 5: Loss 1.592000; Val Acc 0.334028; Train Acc 0.358639\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 6; Loss 1.566663 \n",
      "Step 2 of Epoch: 6; Loss 1.571961 \n",
      "Step 3 of Epoch: 6; Loss 1.578326 \n",
      "Step 4 of Epoch: 6; Loss 1.579952 \n",
      "Step 5 of Epoch: 6; Loss 1.579341 \n",
      "Step 6 of Epoch: 6; Loss 1.580043 \n",
      "Step 7 of Epoch: 6; Loss 1.579223 \n",
      "Step 8 of Epoch: 6; Loss 1.579522 \n",
      "Step 9 of Epoch: 6; Loss 1.580389 \n",
      "Step 10 of Epoch: 6; Loss 1.579519 \n",
      "Step 11 of Epoch: 6; Loss 1.579300 \n",
      "Step 12 of Epoch: 6; Loss 1.579750 \n",
      "Step 13 of Epoch: 6; Loss 1.580836 \n",
      "Step 14 of Epoch: 6; Loss 1.580083 \n",
      "Step 15 of Epoch: 6; Loss 1.580818 \n",
      "Step 16 of Epoch: 6; Loss 1.580773 \n",
      "Step 17 of Epoch: 6; Loss 1.579162 \n",
      "Step 18 of Epoch: 6; Loss 1.579840 \n",
      "Step 19 of Epoch: 6; Loss 1.580116 \n",
      "Step 20 of Epoch: 6; Loss 1.580082 \n",
      "Step 21 of Epoch: 6; Loss 1.580562 \n",
      "Step 22 of Epoch: 6; Loss 1.581081 \n",
      "Step 23 of Epoch: 6; Loss 1.581840 \n",
      "Step 24 of Epoch: 6; Loss 1.581321 \n",
      "Step 25 of Epoch: 6; Loss 1.581641 \n",
      "Step 26 of Epoch: 6; Loss 1.581729 \n",
      "Step 27 of Epoch: 6; Loss 1.581510 \n",
      "Step 28 of Epoch: 6; Loss 1.581485 \n",
      "Step 29 of Epoch: 6; Loss 1.581617 \n",
      "Step 30 of Epoch: 6; Loss 1.581975 \n",
      "Step 31 of Epoch: 6; Loss 1.582229 \n",
      "Step 32 of Epoch: 6; Loss 1.582132 \n",
      "Step 33 of Epoch: 6; Loss 1.582486 \n",
      "Step 34 of Epoch: 6; Loss 1.582558 \n",
      "Step 35 of Epoch: 6; Loss 1.582776 \n",
      "Step 36 of Epoch: 6; Loss 1.582827 \n",
      "Step 37 of Epoch: 6; Loss 1.582910 \n",
      "Step 38 of Epoch: 6; Loss 1.582845 \n",
      "Step 39 of Epoch: 6; Loss 1.583224 \n",
      "Final Result for Epoch 6: Loss 1.583677; Val Acc 0.338717; Train Acc 0.363438\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 7; Loss 1.571337 \n",
      "Step 2 of Epoch: 7; Loss 1.571222 \n",
      "Step 3 of Epoch: 7; Loss 1.571927 \n",
      "Step 4 of Epoch: 7; Loss 1.574059 \n",
      "Step 5 of Epoch: 7; Loss 1.573858 \n",
      "Step 6 of Epoch: 7; Loss 1.571184 \n",
      "Step 7 of Epoch: 7; Loss 1.571663 \n",
      "Step 8 of Epoch: 7; Loss 1.572328 \n",
      "Step 9 of Epoch: 7; Loss 1.572366 \n",
      "Step 10 of Epoch: 7; Loss 1.571058 \n",
      "Step 11 of Epoch: 7; Loss 1.571601 \n",
      "Step 12 of Epoch: 7; Loss 1.571812 \n",
      "Step 13 of Epoch: 7; Loss 1.572025 \n",
      "Step 14 of Epoch: 7; Loss 1.571584 \n",
      "Step 15 of Epoch: 7; Loss 1.571645 \n",
      "Step 16 of Epoch: 7; Loss 1.571059 \n",
      "Step 17 of Epoch: 7; Loss 1.571305 \n",
      "Step 18 of Epoch: 7; Loss 1.572263 \n",
      "Step 19 of Epoch: 7; Loss 1.571863 \n",
      "Step 20 of Epoch: 7; Loss 1.571810 \n",
      "Step 21 of Epoch: 7; Loss 1.572423 \n",
      "Step 22 of Epoch: 7; Loss 1.572941 \n",
      "Step 23 of Epoch: 7; Loss 1.573312 \n",
      "Step 24 of Epoch: 7; Loss 1.573058 \n",
      "Step 25 of Epoch: 7; Loss 1.572974 \n",
      "Step 26 of Epoch: 7; Loss 1.572731 \n",
      "Step 27 of Epoch: 7; Loss 1.572789 \n",
      "Step 28 of Epoch: 7; Loss 1.572841 \n",
      "Step 29 of Epoch: 7; Loss 1.573039 \n",
      "Step 30 of Epoch: 7; Loss 1.573301 \n",
      "Step 31 of Epoch: 7; Loss 1.573451 \n",
      "Step 32 of Epoch: 7; Loss 1.573381 \n",
      "Step 33 of Epoch: 7; Loss 1.574272 \n",
      "Step 34 of Epoch: 7; Loss 1.574882 \n",
      "Step 35 of Epoch: 7; Loss 1.574690 \n",
      "Step 36 of Epoch: 7; Loss 1.575268 \n",
      "Step 37 of Epoch: 7; Loss 1.575248 \n",
      "Step 38 of Epoch: 7; Loss 1.575034 \n",
      "Step 39 of Epoch: 7; Loss 1.575273 \n",
      "Final Result for Epoch 7: Loss 1.575480; Val Acc 0.334108; Train Acc 0.365631\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 8; Loss 1.555301 \n",
      "Step 2 of Epoch: 8; Loss 1.559839 \n",
      "Step 3 of Epoch: 8; Loss 1.562268 \n",
      "Step 4 of Epoch: 8; Loss 1.561430 \n",
      "Step 5 of Epoch: 8; Loss 1.562252 \n",
      "Step 6 of Epoch: 8; Loss 1.563143 \n",
      "Step 7 of Epoch: 8; Loss 1.561474 \n",
      "Step 8 of Epoch: 8; Loss 1.561371 \n",
      "Step 9 of Epoch: 8; Loss 1.561226 \n",
      "Step 10 of Epoch: 8; Loss 1.562659 \n",
      "Step 11 of Epoch: 8; Loss 1.563552 \n",
      "Step 12 of Epoch: 8; Loss 1.563120 \n",
      "Step 13 of Epoch: 8; Loss 1.564240 \n",
      "Step 14 of Epoch: 8; Loss 1.564197 \n",
      "Step 15 of Epoch: 8; Loss 1.563947 \n",
      "Step 16 of Epoch: 8; Loss 1.564898 \n",
      "Step 17 of Epoch: 8; Loss 1.564908 \n",
      "Step 18 of Epoch: 8; Loss 1.565016 \n",
      "Step 19 of Epoch: 8; Loss 1.565212 \n",
      "Step 20 of Epoch: 8; Loss 1.566035 \n",
      "Step 21 of Epoch: 8; Loss 1.566553 \n",
      "Step 22 of Epoch: 8; Loss 1.566274 \n",
      "Step 23 of Epoch: 8; Loss 1.566669 \n",
      "Step 24 of Epoch: 8; Loss 1.567245 \n",
      "Step 25 of Epoch: 8; Loss 1.567416 \n",
      "Step 26 of Epoch: 8; Loss 1.567477 \n",
      "Step 27 of Epoch: 8; Loss 1.567188 \n",
      "Step 28 of Epoch: 8; Loss 1.567105 \n",
      "Step 29 of Epoch: 8; Loss 1.567358 \n",
      "Step 30 of Epoch: 8; Loss 1.567738 \n",
      "Step 31 of Epoch: 8; Loss 1.567964 \n",
      "Step 32 of Epoch: 8; Loss 1.568441 \n",
      "Step 33 of Epoch: 8; Loss 1.568814 \n",
      "Step 34 of Epoch: 8; Loss 1.568626 \n",
      "Step 35 of Epoch: 8; Loss 1.568805 \n",
      "Step 36 of Epoch: 8; Loss 1.568857 \n",
      "Step 37 of Epoch: 8; Loss 1.568953 \n",
      "Step 38 of Epoch: 8; Loss 1.569120 \n",
      "Step 39 of Epoch: 8; Loss 1.569267 \n",
      "Final Result for Epoch 8: Loss 1.568990; Val Acc 0.333507; Train Acc 0.370681\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 9; Loss 1.555081 \n",
      "Step 2 of Epoch: 9; Loss 1.551752 \n",
      "Step 3 of Epoch: 9; Loss 1.554777 \n",
      "Step 4 of Epoch: 9; Loss 1.554548 \n",
      "Step 5 of Epoch: 9; Loss 1.553919 \n",
      "Step 6 of Epoch: 9; Loss 1.556606 \n",
      "Step 7 of Epoch: 9; Loss 1.558067 \n",
      "Step 8 of Epoch: 9; Loss 1.557999 \n",
      "Step 9 of Epoch: 9; Loss 1.558391 \n",
      "Step 10 of Epoch: 9; Loss 1.558318 \n",
      "Step 11 of Epoch: 9; Loss 1.557697 \n",
      "Step 12 of Epoch: 9; Loss 1.557212 \n",
      "Step 13 of Epoch: 9; Loss 1.557744 \n",
      "Step 14 of Epoch: 9; Loss 1.558166 \n",
      "Step 15 of Epoch: 9; Loss 1.557939 \n",
      "Step 16 of Epoch: 9; Loss 1.558573 \n",
      "Step 17 of Epoch: 9; Loss 1.559769 \n",
      "Step 18 of Epoch: 9; Loss 1.560344 \n",
      "Step 19 of Epoch: 9; Loss 1.560625 \n",
      "Step 20 of Epoch: 9; Loss 1.560182 \n",
      "Step 21 of Epoch: 9; Loss 1.560576 \n",
      "Step 22 of Epoch: 9; Loss 1.560349 \n",
      "Step 23 of Epoch: 9; Loss 1.560656 \n",
      "Step 24 of Epoch: 9; Loss 1.560359 \n",
      "Step 25 of Epoch: 9; Loss 1.560495 \n",
      "Step 26 of Epoch: 9; Loss 1.560291 \n",
      "Step 27 of Epoch: 9; Loss 1.560079 \n",
      "Step 28 of Epoch: 9; Loss 1.560375 \n",
      "Step 29 of Epoch: 9; Loss 1.560730 \n",
      "Step 30 of Epoch: 9; Loss 1.560748 \n",
      "Step 31 of Epoch: 9; Loss 1.560865 \n",
      "Step 32 of Epoch: 9; Loss 1.560639 \n",
      "Step 33 of Epoch: 9; Loss 1.560960 \n",
      "Step 34 of Epoch: 9; Loss 1.561055 \n",
      "Step 35 of Epoch: 9; Loss 1.561207 \n",
      "Step 36 of Epoch: 9; Loss 1.561318 \n",
      "Step 37 of Epoch: 9; Loss 1.561398 \n",
      "Step 38 of Epoch: 9; Loss 1.561770 \n",
      "Step 39 of Epoch: 9; Loss 1.562174 \n",
      "Final Result for Epoch 9: Loss 1.562317; Val Acc 0.335351; Train Acc 0.373539\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 10; Loss 1.554480 \n",
      "Step 2 of Epoch: 10; Loss 1.561112 \n",
      "Step 3 of Epoch: 10; Loss 1.560770 \n",
      "Step 4 of Epoch: 10; Loss 1.554000 \n",
      "Step 5 of Epoch: 10; Loss 1.557050 \n",
      "Step 6 of Epoch: 10; Loss 1.556774 \n",
      "Step 7 of Epoch: 10; Loss 1.556849 \n",
      "Step 8 of Epoch: 10; Loss 1.555889 \n",
      "Step 9 of Epoch: 10; Loss 1.555773 \n",
      "Step 10 of Epoch: 10; Loss 1.553950 \n",
      "Step 11 of Epoch: 10; Loss 1.554727 \n",
      "Step 12 of Epoch: 10; Loss 1.555257 \n",
      "Step 13 of Epoch: 10; Loss 1.555619 \n",
      "Step 14 of Epoch: 10; Loss 1.555321 \n",
      "Step 15 of Epoch: 10; Loss 1.554246 \n",
      "Step 16 of Epoch: 10; Loss 1.554824 \n",
      "Step 17 of Epoch: 10; Loss 1.555295 \n",
      "Step 18 of Epoch: 10; Loss 1.555157 \n",
      "Step 19 of Epoch: 10; Loss 1.554990 \n",
      "Step 20 of Epoch: 10; Loss 1.554995 \n",
      "Step 21 of Epoch: 10; Loss 1.555598 \n",
      "Step 22 of Epoch: 10; Loss 1.555435 \n",
      "Step 23 of Epoch: 10; Loss 1.555152 \n",
      "Step 24 of Epoch: 10; Loss 1.555269 \n",
      "Step 25 of Epoch: 10; Loss 1.555272 \n",
      "Step 26 of Epoch: 10; Loss 1.554867 \n",
      "Step 27 of Epoch: 10; Loss 1.554858 \n",
      "Step 28 of Epoch: 10; Loss 1.555580 \n",
      "Step 29 of Epoch: 10; Loss 1.555315 \n",
      "Step 30 of Epoch: 10; Loss 1.555076 \n",
      "Step 31 of Epoch: 10; Loss 1.555115 \n",
      "Step 32 of Epoch: 10; Loss 1.555431 \n",
      "Step 33 of Epoch: 10; Loss 1.555821 \n",
      "Step 34 of Epoch: 10; Loss 1.555958 \n",
      "Step 35 of Epoch: 10; Loss 1.556092 \n",
      "Step 36 of Epoch: 10; Loss 1.555858 \n",
      "Step 37 of Epoch: 10; Loss 1.555773 \n",
      "Step 38 of Epoch: 10; Loss 1.556119 \n",
      "Step 39 of Epoch: 10; Loss 1.556217 \n",
      "Final Result for Epoch 10: Loss 1.556195; Val Acc 0.336954; Train Acc 0.375470\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 11; Loss 1.545184 \n",
      "Step 2 of Epoch: 11; Loss 1.547346 \n",
      "Step 3 of Epoch: 11; Loss 1.543121 \n",
      "Step 4 of Epoch: 11; Loss 1.540076 \n",
      "Step 5 of Epoch: 11; Loss 1.542902 \n",
      "Step 6 of Epoch: 11; Loss 1.543036 \n",
      "Step 7 of Epoch: 11; Loss 1.541730 \n",
      "Step 8 of Epoch: 11; Loss 1.544230 \n",
      "Step 9 of Epoch: 11; Loss 1.543932 \n",
      "Step 10 of Epoch: 11; Loss 1.543714 \n",
      "Step 11 of Epoch: 11; Loss 1.544624 \n",
      "Step 12 of Epoch: 11; Loss 1.544456 \n",
      "Step 13 of Epoch: 11; Loss 1.544304 \n",
      "Step 14 of Epoch: 11; Loss 1.544687 \n",
      "Step 15 of Epoch: 11; Loss 1.545338 \n",
      "Step 16 of Epoch: 11; Loss 1.545511 \n",
      "Step 17 of Epoch: 11; Loss 1.546103 \n",
      "Step 18 of Epoch: 11; Loss 1.547112 \n",
      "Step 19 of Epoch: 11; Loss 1.548125 \n",
      "Step 20 of Epoch: 11; Loss 1.547021 \n",
      "Step 21 of Epoch: 11; Loss 1.547761 \n",
      "Step 22 of Epoch: 11; Loss 1.548069 \n",
      "Step 23 of Epoch: 11; Loss 1.548575 \n",
      "Step 24 of Epoch: 11; Loss 1.548313 \n",
      "Step 25 of Epoch: 11; Loss 1.548825 \n",
      "Step 26 of Epoch: 11; Loss 1.549147 \n",
      "Step 27 of Epoch: 11; Loss 1.549152 \n",
      "Step 28 of Epoch: 11; Loss 1.548718 \n",
      "Step 29 of Epoch: 11; Loss 1.548834 \n",
      "Step 30 of Epoch: 11; Loss 1.549167 \n",
      "Step 31 of Epoch: 11; Loss 1.549440 \n",
      "Step 32 of Epoch: 11; Loss 1.550016 \n",
      "Step 33 of Epoch: 11; Loss 1.550251 \n",
      "Step 34 of Epoch: 11; Loss 1.550237 \n",
      "Step 35 of Epoch: 11; Loss 1.550305 \n",
      "Step 36 of Epoch: 11; Loss 1.550777 \n",
      "Step 37 of Epoch: 11; Loss 1.550284 \n",
      "Step 38 of Epoch: 11; Loss 1.550042 \n",
      "Step 39 of Epoch: 11; Loss 1.550317 \n",
      "Final Result for Epoch 11: Loss 1.550268; Val Acc 0.333988; Train Acc 0.378999\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 12; Loss 1.553060 \n",
      "Step 2 of Epoch: 12; Loss 1.540527 \n",
      "Step 3 of Epoch: 12; Loss 1.544845 \n",
      "Step 4 of Epoch: 12; Loss 1.536235 \n",
      "Step 5 of Epoch: 12; Loss 1.537372 \n",
      "Step 6 of Epoch: 12; Loss 1.538166 \n",
      "Step 7 of Epoch: 12; Loss 1.538397 \n",
      "Step 8 of Epoch: 12; Loss 1.535798 \n",
      "Step 9 of Epoch: 12; Loss 1.536226 \n",
      "Step 10 of Epoch: 12; Loss 1.537956 \n",
      "Step 11 of Epoch: 12; Loss 1.537601 \n",
      "Step 12 of Epoch: 12; Loss 1.538151 \n",
      "Step 13 of Epoch: 12; Loss 1.538258 \n",
      "Step 14 of Epoch: 12; Loss 1.536606 \n",
      "Step 15 of Epoch: 12; Loss 1.538469 \n",
      "Step 16 of Epoch: 12; Loss 1.539820 \n",
      "Step 17 of Epoch: 12; Loss 1.539904 \n",
      "Step 18 of Epoch: 12; Loss 1.540276 \n",
      "Step 19 of Epoch: 12; Loss 1.539392 \n",
      "Step 20 of Epoch: 12; Loss 1.539879 \n",
      "Step 21 of Epoch: 12; Loss 1.540509 \n",
      "Step 22 of Epoch: 12; Loss 1.540673 \n",
      "Step 23 of Epoch: 12; Loss 1.540979 \n",
      "Step 24 of Epoch: 12; Loss 1.541164 \n",
      "Step 25 of Epoch: 12; Loss 1.541695 \n",
      "Step 26 of Epoch: 12; Loss 1.542682 \n",
      "Step 27 of Epoch: 12; Loss 1.542478 \n",
      "Step 28 of Epoch: 12; Loss 1.542786 \n",
      "Step 29 of Epoch: 12; Loss 1.542974 \n",
      "Step 30 of Epoch: 12; Loss 1.543408 \n",
      "Step 31 of Epoch: 12; Loss 1.543756 \n",
      "Step 32 of Epoch: 12; Loss 1.544130 \n",
      "Step 33 of Epoch: 12; Loss 1.544031 \n",
      "Step 34 of Epoch: 12; Loss 1.544355 \n",
      "Step 35 of Epoch: 12; Loss 1.544186 \n",
      "Step 36 of Epoch: 12; Loss 1.544359 \n",
      "Step 37 of Epoch: 12; Loss 1.544627 \n",
      "Step 38 of Epoch: 12; Loss 1.544748 \n",
      "Step 39 of Epoch: 12; Loss 1.544788 \n",
      "Final Result for Epoch 12: Loss 1.544853; Val Acc 0.334389; Train Acc 0.380275\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 13; Loss 1.552438 \n",
      "Step 2 of Epoch: 13; Loss 1.542484 \n",
      "Step 3 of Epoch: 13; Loss 1.537121 \n",
      "Step 4 of Epoch: 13; Loss 1.531203 \n",
      "Step 5 of Epoch: 13; Loss 1.533450 \n",
      "Step 6 of Epoch: 13; Loss 1.535330 \n",
      "Step 7 of Epoch: 13; Loss 1.534777 \n",
      "Step 8 of Epoch: 13; Loss 1.533920 \n",
      "Step 9 of Epoch: 13; Loss 1.534396 \n",
      "Step 10 of Epoch: 13; Loss 1.535365 \n",
      "Step 11 of Epoch: 13; Loss 1.535662 \n",
      "Step 12 of Epoch: 13; Loss 1.537267 \n",
      "Step 13 of Epoch: 13; Loss 1.538807 \n",
      "Step 14 of Epoch: 13; Loss 1.538810 \n",
      "Step 15 of Epoch: 13; Loss 1.538158 \n",
      "Step 16 of Epoch: 13; Loss 1.536952 \n",
      "Step 17 of Epoch: 13; Loss 1.536096 \n",
      "Step 18 of Epoch: 13; Loss 1.536534 \n",
      "Step 19 of Epoch: 13; Loss 1.537491 \n",
      "Step 20 of Epoch: 13; Loss 1.537329 \n",
      "Step 21 of Epoch: 13; Loss 1.538070 \n",
      "Step 22 of Epoch: 13; Loss 1.538745 \n",
      "Step 23 of Epoch: 13; Loss 1.539455 \n",
      "Step 24 of Epoch: 13; Loss 1.538746 \n",
      "Step 25 of Epoch: 13; Loss 1.539275 \n",
      "Step 26 of Epoch: 13; Loss 1.538942 \n",
      "Step 27 of Epoch: 13; Loss 1.538578 \n",
      "Step 28 of Epoch: 13; Loss 1.538427 \n",
      "Step 29 of Epoch: 13; Loss 1.538557 \n",
      "Step 30 of Epoch: 13; Loss 1.538148 \n",
      "Step 31 of Epoch: 13; Loss 1.538287 \n",
      "Step 32 of Epoch: 13; Loss 1.538649 \n",
      "Step 33 of Epoch: 13; Loss 1.538436 \n",
      "Step 34 of Epoch: 13; Loss 1.538529 \n",
      "Step 35 of Epoch: 13; Loss 1.538817 \n",
      "Step 36 of Epoch: 13; Loss 1.538745 \n",
      "Step 37 of Epoch: 13; Loss 1.538861 \n",
      "Step 38 of Epoch: 13; Loss 1.538832 \n",
      "Step 39 of Epoch: 13; Loss 1.539327 \n",
      "Final Result for Epoch 13: Loss 1.539330; Val Acc 0.336232; Train Acc 0.383944\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 14; Loss 1.523734 \n",
      "Step 2 of Epoch: 14; Loss 1.515025 \n",
      "Step 3 of Epoch: 14; Loss 1.519753 \n",
      "Step 4 of Epoch: 14; Loss 1.524269 \n",
      "Step 5 of Epoch: 14; Loss 1.526774 \n",
      "Step 6 of Epoch: 14; Loss 1.527274 \n",
      "Step 7 of Epoch: 14; Loss 1.530911 \n",
      "Step 8 of Epoch: 14; Loss 1.531833 \n",
      "Step 9 of Epoch: 14; Loss 1.531397 \n",
      "Step 10 of Epoch: 14; Loss 1.532613 \n",
      "Step 11 of Epoch: 14; Loss 1.532025 \n",
      "Step 12 of Epoch: 14; Loss 1.531671 \n",
      "Step 13 of Epoch: 14; Loss 1.532250 \n",
      "Step 14 of Epoch: 14; Loss 1.532863 \n",
      "Step 15 of Epoch: 14; Loss 1.532793 \n",
      "Step 16 of Epoch: 14; Loss 1.532737 \n",
      "Step 17 of Epoch: 14; Loss 1.532290 \n",
      "Step 18 of Epoch: 14; Loss 1.533023 \n",
      "Step 19 of Epoch: 14; Loss 1.533342 \n",
      "Step 20 of Epoch: 14; Loss 1.533227 \n",
      "Step 21 of Epoch: 14; Loss 1.533048 \n",
      "Step 22 of Epoch: 14; Loss 1.533778 \n",
      "Step 23 of Epoch: 14; Loss 1.533590 \n",
      "Step 24 of Epoch: 14; Loss 1.533552 \n",
      "Step 25 of Epoch: 14; Loss 1.533427 \n",
      "Step 26 of Epoch: 14; Loss 1.533417 \n",
      "Step 27 of Epoch: 14; Loss 1.533872 \n",
      "Step 28 of Epoch: 14; Loss 1.534158 \n",
      "Step 29 of Epoch: 14; Loss 1.533909 \n",
      "Step 30 of Epoch: 14; Loss 1.533943 \n",
      "Step 31 of Epoch: 14; Loss 1.533674 \n",
      "Step 32 of Epoch: 14; Loss 1.533687 \n",
      "Step 33 of Epoch: 14; Loss 1.533069 \n",
      "Step 34 of Epoch: 14; Loss 1.533461 \n",
      "Step 35 of Epoch: 14; Loss 1.533420 \n",
      "Step 36 of Epoch: 14; Loss 1.533850 \n",
      "Step 37 of Epoch: 14; Loss 1.533736 \n",
      "Step 38 of Epoch: 14; Loss 1.534201 \n",
      "Step 39 of Epoch: 14; Loss 1.534501 \n",
      "Final Result for Epoch 14: Loss 1.534490; Val Acc 0.333387; Train Acc 0.387638\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 15; Loss 1.500347 \n",
      "Step 2 of Epoch: 15; Loss 1.515674 \n",
      "Step 3 of Epoch: 15; Loss 1.520065 \n",
      "Step 4 of Epoch: 15; Loss 1.521170 \n",
      "Step 5 of Epoch: 15; Loss 1.519827 \n",
      "Step 6 of Epoch: 15; Loss 1.521094 \n",
      "Step 7 of Epoch: 15; Loss 1.522646 \n",
      "Step 8 of Epoch: 15; Loss 1.522365 \n",
      "Step 9 of Epoch: 15; Loss 1.522172 \n",
      "Step 10 of Epoch: 15; Loss 1.522905 \n",
      "Step 11 of Epoch: 15; Loss 1.524147 \n",
      "Step 12 of Epoch: 15; Loss 1.524402 \n",
      "Step 13 of Epoch: 15; Loss 1.523852 \n",
      "Step 14 of Epoch: 15; Loss 1.524330 \n",
      "Step 15 of Epoch: 15; Loss 1.523691 \n",
      "Step 16 of Epoch: 15; Loss 1.523649 \n",
      "Step 17 of Epoch: 15; Loss 1.525249 \n",
      "Step 18 of Epoch: 15; Loss 1.525770 \n",
      "Step 19 of Epoch: 15; Loss 1.525782 \n",
      "Step 20 of Epoch: 15; Loss 1.526864 \n",
      "Step 21 of Epoch: 15; Loss 1.527001 \n",
      "Step 22 of Epoch: 15; Loss 1.527512 \n",
      "Step 23 of Epoch: 15; Loss 1.527737 \n",
      "Step 24 of Epoch: 15; Loss 1.528257 \n",
      "Step 25 of Epoch: 15; Loss 1.528252 \n",
      "Step 26 of Epoch: 15; Loss 1.527871 \n",
      "Step 27 of Epoch: 15; Loss 1.528500 \n",
      "Step 28 of Epoch: 15; Loss 1.528171 \n",
      "Step 29 of Epoch: 15; Loss 1.528498 \n",
      "Step 30 of Epoch: 15; Loss 1.528558 \n",
      "Step 31 of Epoch: 15; Loss 1.528290 \n",
      "Step 32 of Epoch: 15; Loss 1.528505 \n",
      "Step 33 of Epoch: 15; Loss 1.528289 \n",
      "Step 34 of Epoch: 15; Loss 1.528546 \n",
      "Step 35 of Epoch: 15; Loss 1.528703 \n",
      "Step 36 of Epoch: 15; Loss 1.529057 \n",
      "Step 37 of Epoch: 15; Loss 1.529232 \n",
      "Step 38 of Epoch: 15; Loss 1.529136 \n",
      "Step 39 of Epoch: 15; Loss 1.529599 \n",
      "Final Result for Epoch 15: Loss 1.529601; Val Acc 0.334950; Train Acc 0.388989\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 16; Loss 1.512871 \n",
      "Step 2 of Epoch: 16; Loss 1.515511 \n",
      "Step 3 of Epoch: 16; Loss 1.514158 \n",
      "Step 4 of Epoch: 16; Loss 1.515283 \n",
      "Step 5 of Epoch: 16; Loss 1.516883 \n",
      "Step 6 of Epoch: 16; Loss 1.517082 \n",
      "Step 7 of Epoch: 16; Loss 1.516026 \n",
      "Step 8 of Epoch: 16; Loss 1.517925 \n",
      "Step 9 of Epoch: 16; Loss 1.517638 \n",
      "Step 10 of Epoch: 16; Loss 1.518492 \n",
      "Step 11 of Epoch: 16; Loss 1.518365 \n",
      "Step 12 of Epoch: 16; Loss 1.517611 \n",
      "Step 13 of Epoch: 16; Loss 1.517928 \n",
      "Step 14 of Epoch: 16; Loss 1.517350 \n",
      "Step 15 of Epoch: 16; Loss 1.517646 \n",
      "Step 16 of Epoch: 16; Loss 1.517771 \n",
      "Step 17 of Epoch: 16; Loss 1.518064 \n",
      "Step 18 of Epoch: 16; Loss 1.518316 \n",
      "Step 19 of Epoch: 16; Loss 1.518730 \n",
      "Step 20 of Epoch: 16; Loss 1.519284 \n",
      "Step 21 of Epoch: 16; Loss 1.519681 \n",
      "Step 22 of Epoch: 16; Loss 1.520164 \n",
      "Step 23 of Epoch: 16; Loss 1.520535 \n",
      "Step 24 of Epoch: 16; Loss 1.521142 \n",
      "Step 25 of Epoch: 16; Loss 1.521135 \n",
      "Step 26 of Epoch: 16; Loss 1.521379 \n",
      "Step 27 of Epoch: 16; Loss 1.521198 \n",
      "Step 28 of Epoch: 16; Loss 1.521318 \n",
      "Step 29 of Epoch: 16; Loss 1.521394 \n",
      "Step 30 of Epoch: 16; Loss 1.522072 \n",
      "Step 31 of Epoch: 16; Loss 1.522436 \n",
      "Step 32 of Epoch: 16; Loss 1.522611 \n",
      "Step 33 of Epoch: 16; Loss 1.523230 \n",
      "Step 34 of Epoch: 16; Loss 1.523737 \n",
      "Step 35 of Epoch: 16; Loss 1.524037 \n",
      "Step 36 of Epoch: 16; Loss 1.524082 \n",
      "Step 37 of Epoch: 16; Loss 1.524336 \n",
      "Step 38 of Epoch: 16; Loss 1.524364 \n",
      "Step 39 of Epoch: 16; Loss 1.524648 \n",
      "Final Result for Epoch 16: Loss 1.524720; Val Acc 0.329780; Train Acc 0.391777\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 17; Loss 1.510273 \n",
      "Step 2 of Epoch: 17; Loss 1.515419 \n",
      "Step 3 of Epoch: 17; Loss 1.515525 \n",
      "Step 4 of Epoch: 17; Loss 1.515587 \n",
      "Step 5 of Epoch: 17; Loss 1.513994 \n",
      "Step 6 of Epoch: 17; Loss 1.513806 \n",
      "Step 7 of Epoch: 17; Loss 1.513447 \n",
      "Step 8 of Epoch: 17; Loss 1.511784 \n",
      "Step 9 of Epoch: 17; Loss 1.513608 \n",
      "Step 10 of Epoch: 17; Loss 1.515733 \n",
      "Step 11 of Epoch: 17; Loss 1.516078 \n",
      "Step 12 of Epoch: 17; Loss 1.514744 \n",
      "Step 13 of Epoch: 17; Loss 1.513305 \n",
      "Step 14 of Epoch: 17; Loss 1.515264 \n",
      "Step 15 of Epoch: 17; Loss 1.515495 \n",
      "Step 16 of Epoch: 17; Loss 1.516208 \n",
      "Step 17 of Epoch: 17; Loss 1.516679 \n",
      "Step 18 of Epoch: 17; Loss 1.516585 \n",
      "Step 19 of Epoch: 17; Loss 1.516898 \n",
      "Step 20 of Epoch: 17; Loss 1.516450 \n",
      "Step 21 of Epoch: 17; Loss 1.516716 \n",
      "Step 22 of Epoch: 17; Loss 1.517845 \n",
      "Step 23 of Epoch: 17; Loss 1.518867 \n",
      "Step 24 of Epoch: 17; Loss 1.518927 \n",
      "Step 25 of Epoch: 17; Loss 1.518535 \n",
      "Step 26 of Epoch: 17; Loss 1.518748 \n",
      "Step 27 of Epoch: 17; Loss 1.519076 \n",
      "Step 28 of Epoch: 17; Loss 1.519553 \n",
      "Step 29 of Epoch: 17; Loss 1.519773 \n",
      "Step 30 of Epoch: 17; Loss 1.519498 \n",
      "Step 31 of Epoch: 17; Loss 1.519698 \n",
      "Step 32 of Epoch: 17; Loss 1.519718 \n",
      "Step 33 of Epoch: 17; Loss 1.519432 \n",
      "Step 34 of Epoch: 17; Loss 1.519458 \n",
      "Step 35 of Epoch: 17; Loss 1.519343 \n",
      "Step 36 of Epoch: 17; Loss 1.519367 \n",
      "Step 37 of Epoch: 17; Loss 1.518963 \n",
      "Step 38 of Epoch: 17; Loss 1.518835 \n",
      "Step 39 of Epoch: 17; Loss 1.519126 \n",
      "Final Result for Epoch 17: Loss 1.520118; Val Acc 0.331303; Train Acc 0.391291\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 18; Loss 1.501036 \n",
      "Step 2 of Epoch: 18; Loss 1.504937 \n",
      "Step 3 of Epoch: 18; Loss 1.505899 \n",
      "Step 4 of Epoch: 18; Loss 1.505736 \n",
      "Step 5 of Epoch: 18; Loss 1.505240 \n",
      "Step 6 of Epoch: 18; Loss 1.506396 \n",
      "Step 7 of Epoch: 18; Loss 1.505883 \n",
      "Step 8 of Epoch: 18; Loss 1.504891 \n",
      "Step 9 of Epoch: 18; Loss 1.503549 \n",
      "Step 10 of Epoch: 18; Loss 1.504635 \n",
      "Step 11 of Epoch: 18; Loss 1.505292 \n",
      "Step 12 of Epoch: 18; Loss 1.505402 \n",
      "Step 13 of Epoch: 18; Loss 1.506887 \n",
      "Step 14 of Epoch: 18; Loss 1.507178 \n",
      "Step 15 of Epoch: 18; Loss 1.508178 \n",
      "Step 16 of Epoch: 18; Loss 1.508459 \n",
      "Step 17 of Epoch: 18; Loss 1.508463 \n",
      "Step 18 of Epoch: 18; Loss 1.509339 \n",
      "Step 19 of Epoch: 18; Loss 1.509632 \n",
      "Step 20 of Epoch: 18; Loss 1.509753 \n",
      "Step 21 of Epoch: 18; Loss 1.510038 \n",
      "Step 22 of Epoch: 18; Loss 1.510746 \n",
      "Step 23 of Epoch: 18; Loss 1.511394 \n",
      "Step 24 of Epoch: 18; Loss 1.512063 \n",
      "Step 25 of Epoch: 18; Loss 1.512024 \n",
      "Step 26 of Epoch: 18; Loss 1.511436 \n",
      "Step 27 of Epoch: 18; Loss 1.512008 \n",
      "Step 28 of Epoch: 18; Loss 1.512388 \n",
      "Step 29 of Epoch: 18; Loss 1.512625 \n",
      "Step 30 of Epoch: 18; Loss 1.512178 \n",
      "Step 31 of Epoch: 18; Loss 1.512967 \n",
      "Step 32 of Epoch: 18; Loss 1.513375 \n",
      "Step 33 of Epoch: 18; Loss 1.513367 \n",
      "Step 34 of Epoch: 18; Loss 1.513812 \n",
      "Step 35 of Epoch: 18; Loss 1.513930 \n",
      "Step 36 of Epoch: 18; Loss 1.514325 \n",
      "Step 37 of Epoch: 18; Loss 1.514631 \n",
      "Step 38 of Epoch: 18; Loss 1.515138 \n",
      "Step 39 of Epoch: 18; Loss 1.515408 \n",
      "Final Result for Epoch 18: Loss 1.515692; Val Acc 0.331423; Train Acc 0.395480\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 19; Loss 1.498901 \n",
      "Step 2 of Epoch: 19; Loss 1.504978 \n",
      "Step 3 of Epoch: 19; Loss 1.506872 \n",
      "Step 4 of Epoch: 19; Loss 1.504329 \n",
      "Step 5 of Epoch: 19; Loss 1.505396 \n",
      "Step 6 of Epoch: 19; Loss 1.507104 \n",
      "Step 7 of Epoch: 19; Loss 1.504518 \n",
      "Step 8 of Epoch: 19; Loss 1.505173 \n",
      "Step 9 of Epoch: 19; Loss 1.505086 \n",
      "Step 10 of Epoch: 19; Loss 1.505618 \n",
      "Step 11 of Epoch: 19; Loss 1.506501 \n",
      "Step 12 of Epoch: 19; Loss 1.506961 \n",
      "Step 13 of Epoch: 19; Loss 1.508273 \n",
      "Step 14 of Epoch: 19; Loss 1.508522 \n",
      "Step 15 of Epoch: 19; Loss 1.508519 \n",
      "Step 16 of Epoch: 19; Loss 1.508949 \n",
      "Step 17 of Epoch: 19; Loss 1.508093 \n",
      "Step 18 of Epoch: 19; Loss 1.508050 \n",
      "Step 19 of Epoch: 19; Loss 1.508380 \n",
      "Step 20 of Epoch: 19; Loss 1.508536 \n",
      "Step 21 of Epoch: 19; Loss 1.508796 \n",
      "Step 22 of Epoch: 19; Loss 1.509729 \n",
      "Step 23 of Epoch: 19; Loss 1.509671 \n",
      "Step 24 of Epoch: 19; Loss 1.509741 \n",
      "Step 25 of Epoch: 19; Loss 1.510529 \n",
      "Step 26 of Epoch: 19; Loss 1.510976 \n",
      "Step 27 of Epoch: 19; Loss 1.511477 \n",
      "Step 28 of Epoch: 19; Loss 1.511530 \n",
      "Step 29 of Epoch: 19; Loss 1.512014 \n",
      "Step 30 of Epoch: 19; Loss 1.511684 \n",
      "Step 31 of Epoch: 19; Loss 1.511652 \n",
      "Step 32 of Epoch: 19; Loss 1.511943 \n",
      "Step 33 of Epoch: 19; Loss 1.511650 \n",
      "Step 34 of Epoch: 19; Loss 1.511730 \n",
      "Step 35 of Epoch: 19; Loss 1.511879 \n",
      "Step 36 of Epoch: 19; Loss 1.511769 \n",
      "Step 37 of Epoch: 19; Loss 1.511858 \n",
      "Step 38 of Epoch: 19; Loss 1.511756 \n",
      "Step 39 of Epoch: 19; Loss 1.512077 \n",
      "Final Result for Epoch 19: Loss 1.511897; Val Acc 0.328657; Train Acc 0.398308\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 20; Loss 1.489594 \n",
      "Step 2 of Epoch: 20; Loss 1.489929 \n",
      "Step 3 of Epoch: 20; Loss 1.498804 \n",
      "Step 4 of Epoch: 20; Loss 1.495930 \n",
      "Step 5 of Epoch: 20; Loss 1.497115 \n",
      "Step 6 of Epoch: 20; Loss 1.499164 \n",
      "Step 7 of Epoch: 20; Loss 1.500601 \n",
      "Step 8 of Epoch: 20; Loss 1.502772 \n",
      "Step 9 of Epoch: 20; Loss 1.501457 \n",
      "Step 10 of Epoch: 20; Loss 1.502006 \n",
      "Step 11 of Epoch: 20; Loss 1.501631 \n",
      "Step 12 of Epoch: 20; Loss 1.501569 \n",
      "Step 13 of Epoch: 20; Loss 1.500753 \n",
      "Step 14 of Epoch: 20; Loss 1.501941 \n",
      "Step 15 of Epoch: 20; Loss 1.501222 \n",
      "Step 16 of Epoch: 20; Loss 1.501255 \n",
      "Step 17 of Epoch: 20; Loss 1.500358 \n",
      "Step 18 of Epoch: 20; Loss 1.500355 \n",
      "Step 19 of Epoch: 20; Loss 1.500675 \n",
      "Step 20 of Epoch: 20; Loss 1.501391 \n",
      "Step 21 of Epoch: 20; Loss 1.501678 \n",
      "Step 22 of Epoch: 20; Loss 1.501737 \n",
      "Step 23 of Epoch: 20; Loss 1.502025 \n",
      "Step 24 of Epoch: 20; Loss 1.502347 \n",
      "Step 25 of Epoch: 20; Loss 1.501999 \n",
      "Step 26 of Epoch: 20; Loss 1.502883 \n",
      "Step 27 of Epoch: 20; Loss 1.503495 \n",
      "Step 28 of Epoch: 20; Loss 1.503368 \n",
      "Step 29 of Epoch: 20; Loss 1.504231 \n",
      "Step 30 of Epoch: 20; Loss 1.504328 \n",
      "Step 31 of Epoch: 20; Loss 1.504021 \n",
      "Step 32 of Epoch: 20; Loss 1.504531 \n",
      "Step 33 of Epoch: 20; Loss 1.505118 \n",
      "Step 34 of Epoch: 20; Loss 1.505442 \n",
      "Step 35 of Epoch: 20; Loss 1.506039 \n",
      "Step 36 of Epoch: 20; Loss 1.506686 \n",
      "Step 37 of Epoch: 20; Loss 1.506850 \n",
      "Step 38 of Epoch: 20; Loss 1.507442 \n",
      "Step 39 of Epoch: 20; Loss 1.508039 \n",
      "Final Result for Epoch 20: Loss 1.508000; Val Acc 0.331864; Train Acc 0.398779\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 21; Loss 1.493432 \n",
      "Step 2 of Epoch: 21; Loss 1.483107 \n",
      "Step 3 of Epoch: 21; Loss 1.480049 \n",
      "Step 4 of Epoch: 21; Loss 1.486932 \n",
      "Step 5 of Epoch: 21; Loss 1.489332 \n",
      "Step 6 of Epoch: 21; Loss 1.489963 \n",
      "Step 7 of Epoch: 21; Loss 1.490231 \n",
      "Step 8 of Epoch: 21; Loss 1.489299 \n",
      "Step 9 of Epoch: 21; Loss 1.490297 \n",
      "Step 10 of Epoch: 21; Loss 1.490924 \n",
      "Step 11 of Epoch: 21; Loss 1.491471 \n",
      "Step 12 of Epoch: 21; Loss 1.491692 \n",
      "Step 13 of Epoch: 21; Loss 1.493316 \n",
      "Step 14 of Epoch: 21; Loss 1.492917 \n",
      "Step 15 of Epoch: 21; Loss 1.493657 \n",
      "Step 16 of Epoch: 21; Loss 1.495054 \n",
      "Step 17 of Epoch: 21; Loss 1.495756 \n",
      "Step 18 of Epoch: 21; Loss 1.496201 \n",
      "Step 19 of Epoch: 21; Loss 1.496991 \n",
      "Step 20 of Epoch: 21; Loss 1.497001 \n",
      "Step 21 of Epoch: 21; Loss 1.497058 \n",
      "Step 22 of Epoch: 21; Loss 1.497737 \n",
      "Step 23 of Epoch: 21; Loss 1.497993 \n",
      "Step 24 of Epoch: 21; Loss 1.499043 \n",
      "Step 25 of Epoch: 21; Loss 1.499573 \n",
      "Step 26 of Epoch: 21; Loss 1.499869 \n",
      "Step 27 of Epoch: 21; Loss 1.500426 \n",
      "Step 28 of Epoch: 21; Loss 1.500639 \n",
      "Step 29 of Epoch: 21; Loss 1.500368 \n",
      "Step 30 of Epoch: 21; Loss 1.500550 \n",
      "Step 31 of Epoch: 21; Loss 1.501109 \n",
      "Step 32 of Epoch: 21; Loss 1.501136 \n",
      "Step 33 of Epoch: 21; Loss 1.501681 \n",
      "Step 34 of Epoch: 21; Loss 1.502131 \n",
      "Step 35 of Epoch: 21; Loss 1.502414 \n",
      "Step 36 of Epoch: 21; Loss 1.502797 \n",
      "Step 37 of Epoch: 21; Loss 1.503594 \n",
      "Step 38 of Epoch: 21; Loss 1.503674 \n",
      "Step 39 of Epoch: 21; Loss 1.504142 \n",
      "Final Result for Epoch 21: Loss 1.504071; Val Acc 0.329018; Train Acc 0.395611\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 22; Loss 1.485936 \n",
      "Step 2 of Epoch: 22; Loss 1.485269 \n",
      "Step 3 of Epoch: 22; Loss 1.493125 \n",
      "Step 4 of Epoch: 22; Loss 1.490440 \n",
      "Step 5 of Epoch: 22; Loss 1.487678 \n",
      "Step 6 of Epoch: 22; Loss 1.487209 \n",
      "Step 7 of Epoch: 22; Loss 1.489112 \n",
      "Step 8 of Epoch: 22; Loss 1.487519 \n",
      "Step 9 of Epoch: 22; Loss 1.488805 \n",
      "Step 10 of Epoch: 22; Loss 1.490088 \n",
      "Step 11 of Epoch: 22; Loss 1.490139 \n",
      "Step 12 of Epoch: 22; Loss 1.490805 \n",
      "Step 13 of Epoch: 22; Loss 1.491472 \n",
      "Step 14 of Epoch: 22; Loss 1.492200 \n",
      "Step 15 of Epoch: 22; Loss 1.492944 \n",
      "Step 16 of Epoch: 22; Loss 1.493545 \n",
      "Step 17 of Epoch: 22; Loss 1.493945 \n",
      "Step 18 of Epoch: 22; Loss 1.494058 \n",
      "Step 19 of Epoch: 22; Loss 1.494981 \n",
      "Step 20 of Epoch: 22; Loss 1.495625 \n",
      "Step 21 of Epoch: 22; Loss 1.494727 \n",
      "Step 22 of Epoch: 22; Loss 1.495007 \n",
      "Step 23 of Epoch: 22; Loss 1.495163 \n",
      "Step 24 of Epoch: 22; Loss 1.496009 \n",
      "Step 25 of Epoch: 22; Loss 1.496033 \n",
      "Step 26 of Epoch: 22; Loss 1.496413 \n",
      "Step 27 of Epoch: 22; Loss 1.496521 \n",
      "Step 28 of Epoch: 22; Loss 1.497494 \n",
      "Step 29 of Epoch: 22; Loss 1.497265 \n",
      "Step 30 of Epoch: 22; Loss 1.497815 \n",
      "Step 31 of Epoch: 22; Loss 1.497657 \n",
      "Step 32 of Epoch: 22; Loss 1.497971 \n",
      "Step 33 of Epoch: 22; Loss 1.498313 \n",
      "Step 34 of Epoch: 22; Loss 1.498609 \n",
      "Step 35 of Epoch: 22; Loss 1.498747 \n",
      "Step 36 of Epoch: 22; Loss 1.499131 \n",
      "Step 37 of Epoch: 22; Loss 1.499539 \n",
      "Step 38 of Epoch: 22; Loss 1.499876 \n",
      "Step 39 of Epoch: 22; Loss 1.500205 \n",
      "Final Result for Epoch 22: Loss 1.500092; Val Acc 0.331383; Train Acc 0.402818\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 23; Loss 1.478021 \n",
      "Step 2 of Epoch: 23; Loss 1.486089 \n",
      "Step 3 of Epoch: 23; Loss 1.480526 \n",
      "Step 4 of Epoch: 23; Loss 1.483380 \n",
      "Step 5 of Epoch: 23; Loss 1.484433 \n",
      "Step 6 of Epoch: 23; Loss 1.483947 \n",
      "Step 7 of Epoch: 23; Loss 1.482914 \n",
      "Step 8 of Epoch: 23; Loss 1.485326 \n",
      "Step 9 of Epoch: 23; Loss 1.485414 \n",
      "Step 10 of Epoch: 23; Loss 1.484496 \n",
      "Step 11 of Epoch: 23; Loss 1.485413 \n",
      "Step 12 of Epoch: 23; Loss 1.487623 \n",
      "Step 13 of Epoch: 23; Loss 1.487022 \n",
      "Step 14 of Epoch: 23; Loss 1.486214 \n",
      "Step 15 of Epoch: 23; Loss 1.486341 \n",
      "Step 16 of Epoch: 23; Loss 1.486630 \n",
      "Step 17 of Epoch: 23; Loss 1.487356 \n",
      "Step 18 of Epoch: 23; Loss 1.489042 \n",
      "Step 19 of Epoch: 23; Loss 1.489321 \n",
      "Step 20 of Epoch: 23; Loss 1.488830 \n",
      "Step 21 of Epoch: 23; Loss 1.489347 \n",
      "Step 22 of Epoch: 23; Loss 1.490226 \n",
      "Step 23 of Epoch: 23; Loss 1.490312 \n",
      "Step 24 of Epoch: 23; Loss 1.491374 \n",
      "Step 25 of Epoch: 23; Loss 1.491852 \n",
      "Step 26 of Epoch: 23; Loss 1.492721 \n",
      "Step 27 of Epoch: 23; Loss 1.492625 \n",
      "Step 28 of Epoch: 23; Loss 1.493347 \n",
      "Step 29 of Epoch: 23; Loss 1.493931 \n",
      "Step 30 of Epoch: 23; Loss 1.493534 \n",
      "Step 31 of Epoch: 23; Loss 1.493165 \n",
      "Step 32 of Epoch: 23; Loss 1.493530 \n",
      "Step 33 of Epoch: 23; Loss 1.493999 \n",
      "Step 34 of Epoch: 23; Loss 1.494694 \n",
      "Step 35 of Epoch: 23; Loss 1.495052 \n",
      "Step 36 of Epoch: 23; Loss 1.495916 \n",
      "Step 37 of Epoch: 23; Loss 1.495577 \n",
      "Step 38 of Epoch: 23; Loss 1.496009 \n",
      "Step 39 of Epoch: 23; Loss 1.496152 \n",
      "Final Result for Epoch 23: Loss 1.496779; Val Acc 0.331623; Train Acc 0.403884\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 24; Loss 1.477799 \n",
      "Step 2 of Epoch: 24; Loss 1.473056 \n",
      "Step 3 of Epoch: 24; Loss 1.475209 \n",
      "Step 4 of Epoch: 24; Loss 1.477618 \n",
      "Step 5 of Epoch: 24; Loss 1.479639 \n",
      "Step 6 of Epoch: 24; Loss 1.478023 \n",
      "Step 7 of Epoch: 24; Loss 1.478039 \n",
      "Step 8 of Epoch: 24; Loss 1.478618 \n",
      "Step 9 of Epoch: 24; Loss 1.480581 \n",
      "Step 10 of Epoch: 24; Loss 1.481015 \n",
      "Step 11 of Epoch: 24; Loss 1.482172 \n",
      "Step 12 of Epoch: 24; Loss 1.482318 \n",
      "Step 13 of Epoch: 24; Loss 1.481122 \n",
      "Step 14 of Epoch: 24; Loss 1.482293 \n",
      "Step 15 of Epoch: 24; Loss 1.483332 \n",
      "Step 16 of Epoch: 24; Loss 1.484311 \n",
      "Step 17 of Epoch: 24; Loss 1.484994 \n",
      "Step 18 of Epoch: 24; Loss 1.485105 \n",
      "Step 19 of Epoch: 24; Loss 1.486121 \n",
      "Step 20 of Epoch: 24; Loss 1.487061 \n",
      "Step 21 of Epoch: 24; Loss 1.487400 \n",
      "Step 22 of Epoch: 24; Loss 1.488052 \n",
      "Step 23 of Epoch: 24; Loss 1.488367 \n",
      "Step 24 of Epoch: 24; Loss 1.489308 \n",
      "Step 25 of Epoch: 24; Loss 1.489134 \n",
      "Step 26 of Epoch: 24; Loss 1.489524 \n",
      "Step 27 of Epoch: 24; Loss 1.489374 \n",
      "Step 28 of Epoch: 24; Loss 1.489233 \n",
      "Step 29 of Epoch: 24; Loss 1.489918 \n",
      "Step 30 of Epoch: 24; Loss 1.490359 \n",
      "Step 31 of Epoch: 24; Loss 1.490488 \n",
      "Step 32 of Epoch: 24; Loss 1.491719 \n",
      "Step 33 of Epoch: 24; Loss 1.491873 \n",
      "Step 34 of Epoch: 24; Loss 1.492248 \n",
      "Step 35 of Epoch: 24; Loss 1.492545 \n",
      "Step 36 of Epoch: 24; Loss 1.492587 \n",
      "Step 37 of Epoch: 24; Loss 1.492675 \n",
      "Step 38 of Epoch: 24; Loss 1.493146 \n",
      "Step 39 of Epoch: 24; Loss 1.493607 \n",
      "Final Result for Epoch 24: Loss 1.493239; Val Acc 0.331102; Train Acc 0.406151\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 25; Loss 1.464607 \n",
      "Step 2 of Epoch: 25; Loss 1.466054 \n",
      "Step 3 of Epoch: 25; Loss 1.470954 \n",
      "Step 4 of Epoch: 25; Loss 1.471385 \n",
      "Step 5 of Epoch: 25; Loss 1.473225 \n",
      "Step 6 of Epoch: 25; Loss 1.475458 \n",
      "Step 7 of Epoch: 25; Loss 1.477535 \n",
      "Step 8 of Epoch: 25; Loss 1.477643 \n",
      "Step 9 of Epoch: 25; Loss 1.477290 \n",
      "Step 10 of Epoch: 25; Loss 1.478416 \n",
      "Step 11 of Epoch: 25; Loss 1.478335 \n",
      "Step 12 of Epoch: 25; Loss 1.479786 \n",
      "Step 13 of Epoch: 25; Loss 1.480036 \n",
      "Step 14 of Epoch: 25; Loss 1.481164 \n",
      "Step 15 of Epoch: 25; Loss 1.482433 \n",
      "Step 16 of Epoch: 25; Loss 1.483898 \n",
      "Step 17 of Epoch: 25; Loss 1.484346 \n",
      "Step 18 of Epoch: 25; Loss 1.484257 \n",
      "Step 19 of Epoch: 25; Loss 1.484899 \n",
      "Step 20 of Epoch: 25; Loss 1.485304 \n",
      "Step 21 of Epoch: 25; Loss 1.485010 \n",
      "Step 22 of Epoch: 25; Loss 1.485069 \n",
      "Step 23 of Epoch: 25; Loss 1.484865 \n",
      "Step 24 of Epoch: 25; Loss 1.485272 \n",
      "Step 25 of Epoch: 25; Loss 1.485909 \n",
      "Step 26 of Epoch: 25; Loss 1.486363 \n",
      "Step 27 of Epoch: 25; Loss 1.486118 \n",
      "Step 28 of Epoch: 25; Loss 1.486888 \n",
      "Step 29 of Epoch: 25; Loss 1.487685 \n",
      "Step 30 of Epoch: 25; Loss 1.488329 \n",
      "Step 31 of Epoch: 25; Loss 1.488424 \n",
      "Step 32 of Epoch: 25; Loss 1.488601 \n",
      "Step 33 of Epoch: 25; Loss 1.488924 \n",
      "Step 34 of Epoch: 25; Loss 1.488510 \n",
      "Step 35 of Epoch: 25; Loss 1.488749 \n",
      "Step 36 of Epoch: 25; Loss 1.489235 \n",
      "Step 37 of Epoch: 25; Loss 1.489088 \n",
      "Step 38 of Epoch: 25; Loss 1.489164 \n",
      "Step 39 of Epoch: 25; Loss 1.489614 \n",
      "Final Result for Epoch 25: Loss 1.489678; Val Acc 0.328697; Train Acc 0.407432\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 26; Loss 1.461916 \n",
      "Step 2 of Epoch: 26; Loss 1.466573 \n",
      "Step 3 of Epoch: 26; Loss 1.471943 \n",
      "Step 4 of Epoch: 26; Loss 1.473105 \n",
      "Step 5 of Epoch: 26; Loss 1.474403 \n",
      "Step 6 of Epoch: 26; Loss 1.476745 \n",
      "Step 7 of Epoch: 26; Loss 1.478003 \n",
      "Step 8 of Epoch: 26; Loss 1.476836 \n",
      "Step 9 of Epoch: 26; Loss 1.477478 \n",
      "Step 10 of Epoch: 26; Loss 1.476632 \n",
      "Step 11 of Epoch: 26; Loss 1.476375 \n",
      "Step 12 of Epoch: 26; Loss 1.478513 \n",
      "Step 13 of Epoch: 26; Loss 1.480217 \n",
      "Step 14 of Epoch: 26; Loss 1.481264 \n",
      "Step 15 of Epoch: 26; Loss 1.481939 \n",
      "Step 16 of Epoch: 26; Loss 1.483251 \n",
      "Step 17 of Epoch: 26; Loss 1.482364 \n",
      "Step 18 of Epoch: 26; Loss 1.482809 \n",
      "Step 19 of Epoch: 26; Loss 1.483144 \n",
      "Step 20 of Epoch: 26; Loss 1.482669 \n",
      "Step 21 of Epoch: 26; Loss 1.483770 \n",
      "Step 22 of Epoch: 26; Loss 1.483116 \n",
      "Step 23 of Epoch: 26; Loss 1.483731 \n",
      "Step 24 of Epoch: 26; Loss 1.483736 \n",
      "Step 25 of Epoch: 26; Loss 1.483927 \n",
      "Step 26 of Epoch: 26; Loss 1.483775 \n",
      "Step 27 of Epoch: 26; Loss 1.484024 \n",
      "Step 28 of Epoch: 26; Loss 1.484235 \n",
      "Step 29 of Epoch: 26; Loss 1.485205 \n",
      "Step 30 of Epoch: 26; Loss 1.486153 \n",
      "Step 31 of Epoch: 26; Loss 1.485888 \n",
      "Step 32 of Epoch: 26; Loss 1.485538 \n",
      "Step 33 of Epoch: 26; Loss 1.485912 \n",
      "Step 34 of Epoch: 26; Loss 1.485902 \n",
      "Step 35 of Epoch: 26; Loss 1.485995 \n",
      "Step 36 of Epoch: 26; Loss 1.486157 \n",
      "Step 37 of Epoch: 26; Loss 1.486306 \n",
      "Step 38 of Epoch: 26; Loss 1.486370 \n",
      "Step 39 of Epoch: 26; Loss 1.486520 \n",
      "Final Result for Epoch 26: Loss 1.486952; Val Acc 0.325932; Train Acc 0.405395\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 27; Loss 1.452372 \n",
      "Step 2 of Epoch: 27; Loss 1.458714 \n",
      "Step 3 of Epoch: 27; Loss 1.461768 \n",
      "Step 4 of Epoch: 27; Loss 1.462058 \n",
      "Step 5 of Epoch: 27; Loss 1.461615 \n",
      "Step 6 of Epoch: 27; Loss 1.460311 \n",
      "Step 7 of Epoch: 27; Loss 1.462113 \n",
      "Step 8 of Epoch: 27; Loss 1.465116 \n",
      "Step 9 of Epoch: 27; Loss 1.467020 \n",
      "Step 10 of Epoch: 27; Loss 1.466382 \n",
      "Step 11 of Epoch: 27; Loss 1.468745 \n",
      "Step 12 of Epoch: 27; Loss 1.469966 \n",
      "Step 13 of Epoch: 27; Loss 1.470638 \n",
      "Step 14 of Epoch: 27; Loss 1.471925 \n",
      "Step 15 of Epoch: 27; Loss 1.472228 \n",
      "Step 16 of Epoch: 27; Loss 1.473283 \n",
      "Step 17 of Epoch: 27; Loss 1.474466 \n",
      "Step 18 of Epoch: 27; Loss 1.475450 \n",
      "Step 19 of Epoch: 27; Loss 1.476519 \n",
      "Step 20 of Epoch: 27; Loss 1.477108 \n",
      "Step 21 of Epoch: 27; Loss 1.477587 \n",
      "Step 22 of Epoch: 27; Loss 1.479182 \n",
      "Step 23 of Epoch: 27; Loss 1.479395 \n",
      "Step 24 of Epoch: 27; Loss 1.479472 \n",
      "Step 25 of Epoch: 27; Loss 1.480145 \n",
      "Step 26 of Epoch: 27; Loss 1.480026 \n",
      "Step 27 of Epoch: 27; Loss 1.480650 \n",
      "Step 28 of Epoch: 27; Loss 1.481276 \n",
      "Step 29 of Epoch: 27; Loss 1.481400 \n",
      "Step 30 of Epoch: 27; Loss 1.481743 \n",
      "Step 31 of Epoch: 27; Loss 1.481721 \n",
      "Step 32 of Epoch: 27; Loss 1.482200 \n",
      "Step 33 of Epoch: 27; Loss 1.482401 \n",
      "Step 34 of Epoch: 27; Loss 1.482367 \n",
      "Step 35 of Epoch: 27; Loss 1.482577 \n",
      "Step 36 of Epoch: 27; Loss 1.483107 \n",
      "Step 37 of Epoch: 27; Loss 1.483090 \n",
      "Step 38 of Epoch: 27; Loss 1.483518 \n",
      "Step 39 of Epoch: 27; Loss 1.483978 \n",
      "Final Result for Epoch 27: Loss 1.484074; Val Acc 0.324168; Train Acc 0.409830\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 28; Loss 1.456074 \n",
      "Step 2 of Epoch: 28; Loss 1.451259 \n",
      "Step 3 of Epoch: 28; Loss 1.463989 \n",
      "Step 4 of Epoch: 28; Loss 1.465800 \n",
      "Step 5 of Epoch: 28; Loss 1.466479 \n",
      "Step 6 of Epoch: 28; Loss 1.467589 \n",
      "Step 7 of Epoch: 28; Loss 1.467319 \n",
      "Step 8 of Epoch: 28; Loss 1.467147 \n",
      "Step 9 of Epoch: 28; Loss 1.468470 \n",
      "Step 10 of Epoch: 28; Loss 1.469114 \n",
      "Step 11 of Epoch: 28; Loss 1.471620 \n",
      "Step 12 of Epoch: 28; Loss 1.470986 \n",
      "Step 13 of Epoch: 28; Loss 1.471146 \n",
      "Step 14 of Epoch: 28; Loss 1.471024 \n",
      "Step 15 of Epoch: 28; Loss 1.472115 \n",
      "Step 16 of Epoch: 28; Loss 1.474008 \n",
      "Step 17 of Epoch: 28; Loss 1.474102 \n",
      "Step 18 of Epoch: 28; Loss 1.474615 \n",
      "Step 19 of Epoch: 28; Loss 1.474881 \n",
      "Step 20 of Epoch: 28; Loss 1.474642 \n",
      "Step 21 of Epoch: 28; Loss 1.475534 \n",
      "Step 22 of Epoch: 28; Loss 1.476208 \n",
      "Step 23 of Epoch: 28; Loss 1.475962 \n",
      "Step 24 of Epoch: 28; Loss 1.475700 \n",
      "Step 25 of Epoch: 28; Loss 1.475698 \n",
      "Step 26 of Epoch: 28; Loss 1.476600 \n",
      "Step 27 of Epoch: 28; Loss 1.477098 \n",
      "Step 28 of Epoch: 28; Loss 1.477065 \n",
      "Step 29 of Epoch: 28; Loss 1.477158 \n",
      "Step 30 of Epoch: 28; Loss 1.477493 \n",
      "Step 31 of Epoch: 28; Loss 1.477827 \n",
      "Step 32 of Epoch: 28; Loss 1.478064 \n",
      "Step 33 of Epoch: 28; Loss 1.478603 \n",
      "Step 34 of Epoch: 28; Loss 1.479049 \n",
      "Step 35 of Epoch: 28; Loss 1.479602 \n",
      "Step 36 of Epoch: 28; Loss 1.479780 \n",
      "Step 37 of Epoch: 28; Loss 1.480144 \n",
      "Step 38 of Epoch: 28; Loss 1.479914 \n",
      "Step 39 of Epoch: 28; Loss 1.480151 \n",
      "Final Result for Epoch 28: Loss 1.480856; Val Acc 0.326052; Train Acc 0.411982\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 29; Loss 1.471014 \n",
      "Step 2 of Epoch: 29; Loss 1.464205 \n",
      "Step 3 of Epoch: 29; Loss 1.465721 \n",
      "Step 4 of Epoch: 29; Loss 1.464838 \n",
      "Step 5 of Epoch: 29; Loss 1.463530 \n",
      "Step 6 of Epoch: 29; Loss 1.464218 \n",
      "Step 7 of Epoch: 29; Loss 1.465445 \n",
      "Step 8 of Epoch: 29; Loss 1.464936 \n",
      "Step 9 of Epoch: 29; Loss 1.464085 \n",
      "Step 10 of Epoch: 29; Loss 1.466353 \n",
      "Step 11 of Epoch: 29; Loss 1.467336 \n",
      "Step 12 of Epoch: 29; Loss 1.468531 \n",
      "Step 13 of Epoch: 29; Loss 1.468773 \n",
      "Step 14 of Epoch: 29; Loss 1.469833 \n",
      "Step 15 of Epoch: 29; Loss 1.469202 \n",
      "Step 16 of Epoch: 29; Loss 1.469462 \n",
      "Step 17 of Epoch: 29; Loss 1.469939 \n",
      "Step 18 of Epoch: 29; Loss 1.469736 \n",
      "Step 19 of Epoch: 29; Loss 1.469621 \n",
      "Step 20 of Epoch: 29; Loss 1.470438 \n",
      "Step 21 of Epoch: 29; Loss 1.470664 \n",
      "Step 22 of Epoch: 29; Loss 1.471676 \n",
      "Step 23 of Epoch: 29; Loss 1.472182 \n",
      "Step 24 of Epoch: 29; Loss 1.472662 \n",
      "Step 25 of Epoch: 29; Loss 1.472871 \n",
      "Step 26 of Epoch: 29; Loss 1.473601 \n",
      "Step 27 of Epoch: 29; Loss 1.473615 \n",
      "Step 28 of Epoch: 29; Loss 1.473898 \n",
      "Step 29 of Epoch: 29; Loss 1.474071 \n",
      "Step 30 of Epoch: 29; Loss 1.474465 \n",
      "Step 31 of Epoch: 29; Loss 1.475422 \n",
      "Step 32 of Epoch: 29; Loss 1.475662 \n",
      "Step 33 of Epoch: 29; Loss 1.475542 \n",
      "Step 34 of Epoch: 29; Loss 1.476147 \n",
      "Step 35 of Epoch: 29; Loss 1.476307 \n",
      "Step 36 of Epoch: 29; Loss 1.477193 \n",
      "Step 37 of Epoch: 29; Loss 1.477397 \n",
      "Step 38 of Epoch: 29; Loss 1.477896 \n",
      "Step 39 of Epoch: 29; Loss 1.478174 \n",
      "Final Result for Epoch 29: Loss 1.478299; Val Acc 0.327174; Train Acc 0.412793\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 30; Loss 1.467430 \n",
      "Step 2 of Epoch: 30; Loss 1.469363 \n",
      "Step 3 of Epoch: 30; Loss 1.462970 \n",
      "Step 4 of Epoch: 30; Loss 1.460212 \n",
      "Step 5 of Epoch: 30; Loss 1.459353 \n",
      "Step 6 of Epoch: 30; Loss 1.463811 \n",
      "Step 7 of Epoch: 30; Loss 1.465858 \n",
      "Step 8 of Epoch: 30; Loss 1.465992 \n",
      "Step 9 of Epoch: 30; Loss 1.466715 \n",
      "Step 10 of Epoch: 30; Loss 1.468323 \n",
      "Step 11 of Epoch: 30; Loss 1.470489 \n",
      "Step 12 of Epoch: 30; Loss 1.470542 \n",
      "Step 13 of Epoch: 30; Loss 1.471475 \n",
      "Step 14 of Epoch: 30; Loss 1.470588 \n",
      "Step 15 of Epoch: 30; Loss 1.470656 \n",
      "Step 16 of Epoch: 30; Loss 1.470416 \n",
      "Step 17 of Epoch: 30; Loss 1.469665 \n",
      "Step 18 of Epoch: 30; Loss 1.469999 \n",
      "Step 19 of Epoch: 30; Loss 1.469645 \n",
      "Step 20 of Epoch: 30; Loss 1.470952 \n",
      "Step 21 of Epoch: 30; Loss 1.471194 \n",
      "Step 22 of Epoch: 30; Loss 1.471663 \n",
      "Step 23 of Epoch: 30; Loss 1.471376 \n",
      "Step 24 of Epoch: 30; Loss 1.472270 \n",
      "Step 25 of Epoch: 30; Loss 1.472305 \n",
      "Step 26 of Epoch: 30; Loss 1.472829 \n",
      "Step 27 of Epoch: 30; Loss 1.472368 \n",
      "Step 28 of Epoch: 30; Loss 1.472968 \n",
      "Step 29 of Epoch: 30; Loss 1.473422 \n",
      "Step 30 of Epoch: 30; Loss 1.473954 \n",
      "Step 31 of Epoch: 30; Loss 1.474328 \n",
      "Step 32 of Epoch: 30; Loss 1.474437 \n",
      "Step 33 of Epoch: 30; Loss 1.474375 \n",
      "Step 34 of Epoch: 30; Loss 1.474640 \n",
      "Step 35 of Epoch: 30; Loss 1.474420 \n",
      "Step 36 of Epoch: 30; Loss 1.474924 \n",
      "Step 37 of Epoch: 30; Loss 1.474923 \n",
      "Step 38 of Epoch: 30; Loss 1.475488 \n",
      "Step 39 of Epoch: 30; Loss 1.475753 \n",
      "Final Result for Epoch 30: Loss 1.475711; Val Acc 0.326774; Train Acc 0.414014\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 31; Loss 1.452830 \n",
      "Step 2 of Epoch: 31; Loss 1.456094 \n",
      "Step 3 of Epoch: 31; Loss 1.457454 \n",
      "Step 4 of Epoch: 31; Loss 1.459349 \n",
      "Step 5 of Epoch: 31; Loss 1.455592 \n",
      "Step 6 of Epoch: 31; Loss 1.457491 \n",
      "Step 7 of Epoch: 31; Loss 1.461283 \n",
      "Step 8 of Epoch: 31; Loss 1.461494 \n",
      "Step 9 of Epoch: 31; Loss 1.461178 \n",
      "Step 10 of Epoch: 31; Loss 1.460722 \n",
      "Step 11 of Epoch: 31; Loss 1.461871 \n",
      "Step 12 of Epoch: 31; Loss 1.464140 \n",
      "Step 13 of Epoch: 31; Loss 1.463649 \n",
      "Step 14 of Epoch: 31; Loss 1.463317 \n",
      "Step 15 of Epoch: 31; Loss 1.464884 \n",
      "Step 16 of Epoch: 31; Loss 1.465181 \n",
      "Step 17 of Epoch: 31; Loss 1.464675 \n",
      "Step 18 of Epoch: 31; Loss 1.465699 \n",
      "Step 19 of Epoch: 31; Loss 1.466958 \n",
      "Step 20 of Epoch: 31; Loss 1.466167 \n",
      "Step 21 of Epoch: 31; Loss 1.465935 \n",
      "Step 22 of Epoch: 31; Loss 1.466563 \n",
      "Step 23 of Epoch: 31; Loss 1.467138 \n",
      "Step 24 of Epoch: 31; Loss 1.467674 \n",
      "Step 25 of Epoch: 31; Loss 1.468798 \n",
      "Step 26 of Epoch: 31; Loss 1.468702 \n",
      "Step 27 of Epoch: 31; Loss 1.468709 \n",
      "Step 28 of Epoch: 31; Loss 1.468862 \n",
      "Step 29 of Epoch: 31; Loss 1.469316 \n",
      "Step 30 of Epoch: 31; Loss 1.470160 \n",
      "Step 31 of Epoch: 31; Loss 1.470038 \n",
      "Step 32 of Epoch: 31; Loss 1.470241 \n",
      "Step 33 of Epoch: 31; Loss 1.471208 \n",
      "Step 34 of Epoch: 31; Loss 1.471259 \n",
      "Step 35 of Epoch: 31; Loss 1.471749 \n",
      "Step 36 of Epoch: 31; Loss 1.471733 \n",
      "Step 37 of Epoch: 31; Loss 1.472061 \n",
      "Step 38 of Epoch: 31; Loss 1.472389 \n",
      "Step 39 of Epoch: 31; Loss 1.472318 \n",
      "Final Result for Epoch 31: Loss 1.472814; Val Acc 0.325932; Train Acc 0.415395\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 32; Loss 1.464677 \n",
      "Step 2 of Epoch: 32; Loss 1.456902 \n",
      "Step 3 of Epoch: 32; Loss 1.462139 \n",
      "Step 4 of Epoch: 32; Loss 1.460985 \n",
      "Step 5 of Epoch: 32; Loss 1.459144 \n",
      "Step 6 of Epoch: 32; Loss 1.454783 \n",
      "Step 7 of Epoch: 32; Loss 1.455689 \n",
      "Step 8 of Epoch: 32; Loss 1.458361 \n",
      "Step 9 of Epoch: 32; Loss 1.459172 \n",
      "Step 10 of Epoch: 32; Loss 1.460403 \n",
      "Step 11 of Epoch: 32; Loss 1.460065 \n",
      "Step 12 of Epoch: 32; Loss 1.462337 \n",
      "Step 13 of Epoch: 32; Loss 1.463172 \n",
      "Step 14 of Epoch: 32; Loss 1.465074 \n",
      "Step 15 of Epoch: 32; Loss 1.465705 \n",
      "Step 16 of Epoch: 32; Loss 1.466217 \n",
      "Step 17 of Epoch: 32; Loss 1.466584 \n",
      "Step 18 of Epoch: 32; Loss 1.467000 \n",
      "Step 19 of Epoch: 32; Loss 1.467437 \n",
      "Step 20 of Epoch: 32; Loss 1.467839 \n",
      "Step 21 of Epoch: 32; Loss 1.467572 \n",
      "Step 22 of Epoch: 32; Loss 1.467983 \n",
      "Step 23 of Epoch: 32; Loss 1.469273 \n",
      "Step 24 of Epoch: 32; Loss 1.468722 \n",
      "Step 25 of Epoch: 32; Loss 1.468518 \n",
      "Step 26 of Epoch: 32; Loss 1.468430 \n",
      "Step 27 of Epoch: 32; Loss 1.468571 \n",
      "Step 28 of Epoch: 32; Loss 1.468471 \n",
      "Step 29 of Epoch: 32; Loss 1.468375 \n",
      "Step 30 of Epoch: 32; Loss 1.468968 \n",
      "Step 31 of Epoch: 32; Loss 1.469035 \n",
      "Step 32 of Epoch: 32; Loss 1.469514 \n",
      "Step 33 of Epoch: 32; Loss 1.469486 \n",
      "Step 34 of Epoch: 32; Loss 1.469453 \n",
      "Step 35 of Epoch: 32; Loss 1.469870 \n",
      "Step 36 of Epoch: 32; Loss 1.470543 \n",
      "Step 37 of Epoch: 32; Loss 1.470283 \n",
      "Step 38 of Epoch: 32; Loss 1.470521 \n",
      "Step 39 of Epoch: 32; Loss 1.470419 \n",
      "Final Result for Epoch 32: Loss 1.470330; Val Acc 0.322325; Train Acc 0.416096\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 33; Loss 1.447545 \n",
      "Step 2 of Epoch: 33; Loss 1.448410 \n",
      "Step 3 of Epoch: 33; Loss 1.450763 \n",
      "Step 4 of Epoch: 33; Loss 1.452750 \n",
      "Step 5 of Epoch: 33; Loss 1.453646 \n",
      "Step 6 of Epoch: 33; Loss 1.452099 \n",
      "Step 7 of Epoch: 33; Loss 1.453237 \n",
      "Step 8 of Epoch: 33; Loss 1.454166 \n",
      "Step 9 of Epoch: 33; Loss 1.455819 \n",
      "Step 10 of Epoch: 33; Loss 1.456410 \n",
      "Step 11 of Epoch: 33; Loss 1.457538 \n",
      "Step 12 of Epoch: 33; Loss 1.456219 \n",
      "Step 13 of Epoch: 33; Loss 1.456682 \n",
      "Step 14 of Epoch: 33; Loss 1.456377 \n",
      "Step 15 of Epoch: 33; Loss 1.456251 \n",
      "Step 16 of Epoch: 33; Loss 1.457726 \n",
      "Step 17 of Epoch: 33; Loss 1.458872 \n",
      "Step 18 of Epoch: 33; Loss 1.460275 \n",
      "Step 19 of Epoch: 33; Loss 1.460996 \n",
      "Step 20 of Epoch: 33; Loss 1.462136 \n",
      "Step 21 of Epoch: 33; Loss 1.462138 \n",
      "Step 22 of Epoch: 33; Loss 1.462241 \n",
      "Step 23 of Epoch: 33; Loss 1.462329 \n",
      "Step 24 of Epoch: 33; Loss 1.462158 \n",
      "Step 25 of Epoch: 33; Loss 1.461576 \n",
      "Step 26 of Epoch: 33; Loss 1.461893 \n",
      "Step 27 of Epoch: 33; Loss 1.462731 \n",
      "Step 28 of Epoch: 33; Loss 1.462459 \n",
      "Step 29 of Epoch: 33; Loss 1.463173 \n",
      "Step 30 of Epoch: 33; Loss 1.463504 \n",
      "Step 31 of Epoch: 33; Loss 1.463907 \n",
      "Step 32 of Epoch: 33; Loss 1.464132 \n",
      "Step 33 of Epoch: 33; Loss 1.464640 \n",
      "Step 34 of Epoch: 33; Loss 1.465022 \n",
      "Step 35 of Epoch: 33; Loss 1.465921 \n",
      "Step 36 of Epoch: 33; Loss 1.466415 \n",
      "Step 37 of Epoch: 33; Loss 1.466879 \n",
      "Step 38 of Epoch: 33; Loss 1.467497 \n",
      "Step 39 of Epoch: 33; Loss 1.467930 \n",
      "Final Result for Epoch 33: Loss 1.468052; Val Acc 0.323006; Train Acc 0.417102\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 34; Loss 1.445456 \n",
      "Step 2 of Epoch: 34; Loss 1.437280 \n",
      "Step 3 of Epoch: 34; Loss 1.443510 \n",
      "Step 4 of Epoch: 34; Loss 1.448265 \n",
      "Step 5 of Epoch: 34; Loss 1.448320 \n",
      "Step 6 of Epoch: 34; Loss 1.450279 \n",
      "Step 7 of Epoch: 34; Loss 1.448403 \n",
      "Step 8 of Epoch: 34; Loss 1.452289 \n",
      "Step 9 of Epoch: 34; Loss 1.452685 \n",
      "Step 10 of Epoch: 34; Loss 1.452367 \n",
      "Step 11 of Epoch: 34; Loss 1.453539 \n",
      "Step 12 of Epoch: 34; Loss 1.452822 \n",
      "Step 13 of Epoch: 34; Loss 1.454961 \n",
      "Step 14 of Epoch: 34; Loss 1.454987 \n",
      "Step 15 of Epoch: 34; Loss 1.455440 \n",
      "Step 16 of Epoch: 34; Loss 1.455822 \n",
      "Step 17 of Epoch: 34; Loss 1.456406 \n",
      "Step 18 of Epoch: 34; Loss 1.457128 \n",
      "Step 19 of Epoch: 34; Loss 1.457460 \n",
      "Step 20 of Epoch: 34; Loss 1.457420 \n",
      "Step 21 of Epoch: 34; Loss 1.458155 \n",
      "Step 22 of Epoch: 34; Loss 1.458481 \n",
      "Step 23 of Epoch: 34; Loss 1.459021 \n",
      "Step 24 of Epoch: 34; Loss 1.459246 \n",
      "Step 25 of Epoch: 34; Loss 1.460257 \n",
      "Step 26 of Epoch: 34; Loss 1.460611 \n",
      "Step 27 of Epoch: 34; Loss 1.461730 \n",
      "Step 28 of Epoch: 34; Loss 1.462306 \n",
      "Step 29 of Epoch: 34; Loss 1.463069 \n",
      "Step 30 of Epoch: 34; Loss 1.463939 \n",
      "Step 31 of Epoch: 34; Loss 1.464404 \n",
      "Step 32 of Epoch: 34; Loss 1.464549 \n",
      "Step 33 of Epoch: 34; Loss 1.464555 \n",
      "Step 34 of Epoch: 34; Loss 1.464497 \n",
      "Step 35 of Epoch: 34; Loss 1.465167 \n",
      "Step 36 of Epoch: 34; Loss 1.465228 \n",
      "Step 37 of Epoch: 34; Loss 1.465344 \n",
      "Step 38 of Epoch: 34; Loss 1.465441 \n",
      "Step 39 of Epoch: 34; Loss 1.465909 \n",
      "Final Result for Epoch 34: Loss 1.465524; Val Acc 0.324890; Train Acc 0.416827\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 35; Loss 1.444724 \n",
      "Step 2 of Epoch: 35; Loss 1.449450 \n",
      "Step 3 of Epoch: 35; Loss 1.448629 \n",
      "Step 4 of Epoch: 35; Loss 1.449761 \n",
      "Step 5 of Epoch: 35; Loss 1.449872 \n",
      "Step 6 of Epoch: 35; Loss 1.452130 \n",
      "Step 7 of Epoch: 35; Loss 1.453512 \n",
      "Step 8 of Epoch: 35; Loss 1.453556 \n",
      "Step 9 of Epoch: 35; Loss 1.453192 \n",
      "Step 10 of Epoch: 35; Loss 1.451634 \n",
      "Step 11 of Epoch: 35; Loss 1.452359 \n",
      "Step 12 of Epoch: 35; Loss 1.453842 \n",
      "Step 13 of Epoch: 35; Loss 1.453786 \n",
      "Step 14 of Epoch: 35; Loss 1.454680 \n",
      "Step 15 of Epoch: 35; Loss 1.455650 \n",
      "Step 16 of Epoch: 35; Loss 1.455014 \n",
      "Step 17 of Epoch: 35; Loss 1.455101 \n",
      "Step 18 of Epoch: 35; Loss 1.456276 \n",
      "Step 19 of Epoch: 35; Loss 1.455580 \n",
      "Step 20 of Epoch: 35; Loss 1.457465 \n",
      "Step 21 of Epoch: 35; Loss 1.458105 \n",
      "Step 22 of Epoch: 35; Loss 1.458579 \n",
      "Step 23 of Epoch: 35; Loss 1.458747 \n",
      "Step 24 of Epoch: 35; Loss 1.459605 \n",
      "Step 25 of Epoch: 35; Loss 1.459515 \n",
      "Step 26 of Epoch: 35; Loss 1.459417 \n",
      "Step 27 of Epoch: 35; Loss 1.459378 \n",
      "Step 28 of Epoch: 35; Loss 1.459900 \n",
      "Step 29 of Epoch: 35; Loss 1.459643 \n",
      "Step 30 of Epoch: 35; Loss 1.459796 \n",
      "Step 31 of Epoch: 35; Loss 1.460200 \n",
      "Step 32 of Epoch: 35; Loss 1.460886 \n",
      "Step 33 of Epoch: 35; Loss 1.461303 \n",
      "Step 34 of Epoch: 35; Loss 1.461304 \n",
      "Step 35 of Epoch: 35; Loss 1.461423 \n",
      "Step 36 of Epoch: 35; Loss 1.461576 \n",
      "Step 37 of Epoch: 35; Loss 1.462114 \n",
      "Step 38 of Epoch: 35; Loss 1.462791 \n",
      "Step 39 of Epoch: 35; Loss 1.463053 \n",
      "Final Result for Epoch 35: Loss 1.463403; Val Acc 0.323768; Train Acc 0.419730\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 36; Loss 1.437382 \n",
      "Step 2 of Epoch: 36; Loss 1.438197 \n",
      "Step 3 of Epoch: 36; Loss 1.435307 \n",
      "Step 4 of Epoch: 36; Loss 1.445693 \n",
      "Step 5 of Epoch: 36; Loss 1.444188 \n",
      "Step 6 of Epoch: 36; Loss 1.441515 \n",
      "Step 7 of Epoch: 36; Loss 1.441839 \n",
      "Step 8 of Epoch: 36; Loss 1.445844 \n",
      "Step 9 of Epoch: 36; Loss 1.447554 \n",
      "Step 10 of Epoch: 36; Loss 1.449576 \n",
      "Step 11 of Epoch: 36; Loss 1.450232 \n",
      "Step 12 of Epoch: 36; Loss 1.450713 \n",
      "Step 13 of Epoch: 36; Loss 1.450376 \n",
      "Step 14 of Epoch: 36; Loss 1.450234 \n",
      "Step 15 of Epoch: 36; Loss 1.449812 \n",
      "Step 16 of Epoch: 36; Loss 1.450422 \n",
      "Step 17 of Epoch: 36; Loss 1.452114 \n",
      "Step 18 of Epoch: 36; Loss 1.453095 \n",
      "Step 19 of Epoch: 36; Loss 1.453769 \n",
      "Step 20 of Epoch: 36; Loss 1.453516 \n",
      "Step 21 of Epoch: 36; Loss 1.454544 \n",
      "Step 22 of Epoch: 36; Loss 1.453865 \n",
      "Step 23 of Epoch: 36; Loss 1.454446 \n",
      "Step 24 of Epoch: 36; Loss 1.454788 \n",
      "Step 25 of Epoch: 36; Loss 1.455314 \n",
      "Step 26 of Epoch: 36; Loss 1.455221 \n",
      "Step 27 of Epoch: 36; Loss 1.455859 \n",
      "Step 28 of Epoch: 36; Loss 1.456625 \n",
      "Step 29 of Epoch: 36; Loss 1.457250 \n",
      "Step 30 of Epoch: 36; Loss 1.457922 \n",
      "Step 31 of Epoch: 36; Loss 1.457689 \n",
      "Step 32 of Epoch: 36; Loss 1.457978 \n",
      "Step 33 of Epoch: 36; Loss 1.458073 \n",
      "Step 34 of Epoch: 36; Loss 1.458586 \n",
      "Step 35 of Epoch: 36; Loss 1.458551 \n",
      "Step 36 of Epoch: 36; Loss 1.459171 \n",
      "Step 37 of Epoch: 36; Loss 1.459678 \n",
      "Step 38 of Epoch: 36; Loss 1.460378 \n",
      "Step 39 of Epoch: 36; Loss 1.460816 \n",
      "Final Result for Epoch 36: Loss 1.461157; Val Acc 0.323727; Train Acc 0.419760\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 37; Loss 1.441433 \n",
      "Step 2 of Epoch: 37; Loss 1.448337 \n",
      "Step 3 of Epoch: 37; Loss 1.445121 \n",
      "Step 4 of Epoch: 37; Loss 1.444624 \n",
      "Step 5 of Epoch: 37; Loss 1.445408 \n",
      "Step 6 of Epoch: 37; Loss 1.446156 \n",
      "Step 7 of Epoch: 37; Loss 1.445639 \n",
      "Step 8 of Epoch: 37; Loss 1.446879 \n",
      "Step 9 of Epoch: 37; Loss 1.449079 \n",
      "Step 10 of Epoch: 37; Loss 1.447916 \n",
      "Step 11 of Epoch: 37; Loss 1.447452 \n",
      "Step 12 of Epoch: 37; Loss 1.448425 \n",
      "Step 13 of Epoch: 37; Loss 1.449738 \n",
      "Step 14 of Epoch: 37; Loss 1.450614 \n",
      "Step 15 of Epoch: 37; Loss 1.450087 \n",
      "Step 16 of Epoch: 37; Loss 1.451079 \n",
      "Step 17 of Epoch: 37; Loss 1.452047 \n",
      "Step 18 of Epoch: 37; Loss 1.452624 \n",
      "Step 19 of Epoch: 37; Loss 1.452582 \n",
      "Step 20 of Epoch: 37; Loss 1.453278 \n",
      "Step 21 of Epoch: 37; Loss 1.454230 \n",
      "Step 22 of Epoch: 37; Loss 1.454564 \n",
      "Step 23 of Epoch: 37; Loss 1.454380 \n",
      "Step 24 of Epoch: 37; Loss 1.454058 \n",
      "Step 25 of Epoch: 37; Loss 1.454733 \n",
      "Step 26 of Epoch: 37; Loss 1.454828 \n",
      "Step 27 of Epoch: 37; Loss 1.454818 \n",
      "Step 28 of Epoch: 37; Loss 1.454667 \n",
      "Step 29 of Epoch: 37; Loss 1.455444 \n",
      "Step 30 of Epoch: 37; Loss 1.455644 \n",
      "Step 31 of Epoch: 37; Loss 1.456230 \n",
      "Step 32 of Epoch: 37; Loss 1.456793 \n",
      "Step 33 of Epoch: 37; Loss 1.456680 \n",
      "Step 34 of Epoch: 37; Loss 1.457233 \n",
      "Step 35 of Epoch: 37; Loss 1.457566 \n",
      "Step 36 of Epoch: 37; Loss 1.457996 \n",
      "Step 37 of Epoch: 37; Loss 1.458476 \n",
      "Step 38 of Epoch: 37; Loss 1.459292 \n",
      "Step 39 of Epoch: 37; Loss 1.459153 \n",
      "Final Result for Epoch 37: Loss 1.459076; Val Acc 0.324168; Train Acc 0.419950\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 38; Loss 1.447072 \n",
      "Step 2 of Epoch: 38; Loss 1.446581 \n",
      "Step 3 of Epoch: 38; Loss 1.442226 \n",
      "Step 4 of Epoch: 38; Loss 1.440351 \n",
      "Step 5 of Epoch: 38; Loss 1.443028 \n",
      "Step 6 of Epoch: 38; Loss 1.442457 \n",
      "Step 7 of Epoch: 38; Loss 1.443687 \n",
      "Step 8 of Epoch: 38; Loss 1.444338 \n",
      "Step 9 of Epoch: 38; Loss 1.444981 \n",
      "Step 10 of Epoch: 38; Loss 1.444512 \n",
      "Step 11 of Epoch: 38; Loss 1.446219 \n",
      "Step 12 of Epoch: 38; Loss 1.447549 \n",
      "Step 13 of Epoch: 38; Loss 1.448965 \n",
      "Step 14 of Epoch: 38; Loss 1.449920 \n",
      "Step 15 of Epoch: 38; Loss 1.449328 \n",
      "Step 16 of Epoch: 38; Loss 1.448813 \n",
      "Step 17 of Epoch: 38; Loss 1.449409 \n",
      "Step 18 of Epoch: 38; Loss 1.450217 \n",
      "Step 19 of Epoch: 38; Loss 1.450467 \n",
      "Step 20 of Epoch: 38; Loss 1.451270 \n",
      "Step 21 of Epoch: 38; Loss 1.452496 \n",
      "Step 22 of Epoch: 38; Loss 1.453688 \n",
      "Step 23 of Epoch: 38; Loss 1.454248 \n",
      "Step 24 of Epoch: 38; Loss 1.454574 \n",
      "Step 25 of Epoch: 38; Loss 1.454794 \n",
      "Step 26 of Epoch: 38; Loss 1.455085 \n",
      "Step 27 of Epoch: 38; Loss 1.455216 \n",
      "Step 28 of Epoch: 38; Loss 1.455672 \n",
      "Step 29 of Epoch: 38; Loss 1.455396 \n",
      "Step 30 of Epoch: 38; Loss 1.456277 \n",
      "Step 31 of Epoch: 38; Loss 1.455945 \n",
      "Step 32 of Epoch: 38; Loss 1.455825 \n",
      "Step 33 of Epoch: 38; Loss 1.455849 \n",
      "Step 34 of Epoch: 38; Loss 1.455264 \n",
      "Step 35 of Epoch: 38; Loss 1.455362 \n",
      "Step 36 of Epoch: 38; Loss 1.455772 \n",
      "Step 37 of Epoch: 38; Loss 1.456245 \n",
      "Step 38 of Epoch: 38; Loss 1.456434 \n",
      "Step 39 of Epoch: 38; Loss 1.456463 \n",
      "Final Result for Epoch 38: Loss 1.456912; Val Acc 0.322164; Train Acc 0.420656\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 39; Loss 1.422501 \n",
      "Step 2 of Epoch: 39; Loss 1.430407 \n",
      "Step 3 of Epoch: 39; Loss 1.434890 \n",
      "Step 4 of Epoch: 39; Loss 1.435353 \n",
      "Step 5 of Epoch: 39; Loss 1.441305 \n",
      "Step 6 of Epoch: 39; Loss 1.439158 \n",
      "Step 7 of Epoch: 39; Loss 1.439795 \n",
      "Step 8 of Epoch: 39; Loss 1.439801 \n",
      "Step 9 of Epoch: 39; Loss 1.438858 \n",
      "Step 10 of Epoch: 39; Loss 1.442888 \n",
      "Step 11 of Epoch: 39; Loss 1.445362 \n",
      "Step 12 of Epoch: 39; Loss 1.445642 \n",
      "Step 13 of Epoch: 39; Loss 1.444931 \n",
      "Step 14 of Epoch: 39; Loss 1.446003 \n",
      "Step 15 of Epoch: 39; Loss 1.447573 \n",
      "Step 16 of Epoch: 39; Loss 1.448899 \n",
      "Step 17 of Epoch: 39; Loss 1.449314 \n",
      "Step 18 of Epoch: 39; Loss 1.449549 \n",
      "Step 19 of Epoch: 39; Loss 1.449139 \n",
      "Step 20 of Epoch: 39; Loss 1.449073 \n",
      "Step 21 of Epoch: 39; Loss 1.449562 \n",
      "Step 22 of Epoch: 39; Loss 1.450019 \n",
      "Step 23 of Epoch: 39; Loss 1.450393 \n",
      "Step 24 of Epoch: 39; Loss 1.449786 \n",
      "Step 25 of Epoch: 39; Loss 1.450126 \n",
      "Step 26 of Epoch: 39; Loss 1.450814 \n",
      "Step 27 of Epoch: 39; Loss 1.451450 \n",
      "Step 28 of Epoch: 39; Loss 1.452114 \n",
      "Step 29 of Epoch: 39; Loss 1.451912 \n",
      "Step 30 of Epoch: 39; Loss 1.452308 \n",
      "Step 31 of Epoch: 39; Loss 1.452434 \n",
      "Step 32 of Epoch: 39; Loss 1.452610 \n",
      "Step 33 of Epoch: 39; Loss 1.452872 \n",
      "Step 34 of Epoch: 39; Loss 1.453067 \n",
      "Step 35 of Epoch: 39; Loss 1.453388 \n",
      "Step 36 of Epoch: 39; Loss 1.454074 \n",
      "Step 37 of Epoch: 39; Loss 1.454364 \n",
      "Step 38 of Epoch: 39; Loss 1.454786 \n",
      "Step 39 of Epoch: 39; Loss 1.455046 \n",
      "Final Result for Epoch 39: Loss 1.455423; Val Acc 0.321884; Train Acc 0.421381\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 40; Loss 1.430267 \n",
      "Step 2 of Epoch: 40; Loss 1.429777 \n",
      "Step 3 of Epoch: 40; Loss 1.433697 \n",
      "Step 4 of Epoch: 40; Loss 1.434212 \n",
      "Step 5 of Epoch: 40; Loss 1.433917 \n",
      "Step 6 of Epoch: 40; Loss 1.437380 \n",
      "Step 7 of Epoch: 40; Loss 1.440628 \n",
      "Step 8 of Epoch: 40; Loss 1.441637 \n",
      "Step 9 of Epoch: 40; Loss 1.442019 \n",
      "Step 10 of Epoch: 40; Loss 1.443462 \n",
      "Step 11 of Epoch: 40; Loss 1.443811 \n",
      "Step 12 of Epoch: 40; Loss 1.446141 \n",
      "Step 13 of Epoch: 40; Loss 1.445837 \n",
      "Step 14 of Epoch: 40; Loss 1.446790 \n",
      "Step 15 of Epoch: 40; Loss 1.448352 \n",
      "Step 16 of Epoch: 40; Loss 1.448028 \n",
      "Step 17 of Epoch: 40; Loss 1.447869 \n",
      "Step 18 of Epoch: 40; Loss 1.447728 \n",
      "Step 19 of Epoch: 40; Loss 1.447919 \n",
      "Step 20 of Epoch: 40; Loss 1.447894 \n",
      "Step 21 of Epoch: 40; Loss 1.448612 \n",
      "Step 22 of Epoch: 40; Loss 1.447626 \n",
      "Step 23 of Epoch: 40; Loss 1.448157 \n",
      "Step 24 of Epoch: 40; Loss 1.448729 \n",
      "Step 25 of Epoch: 40; Loss 1.449532 \n",
      "Step 26 of Epoch: 40; Loss 1.449518 \n",
      "Step 27 of Epoch: 40; Loss 1.450281 \n",
      "Step 28 of Epoch: 40; Loss 1.450800 \n",
      "Step 29 of Epoch: 40; Loss 1.451142 \n",
      "Step 30 of Epoch: 40; Loss 1.452112 \n",
      "Step 31 of Epoch: 40; Loss 1.452163 \n",
      "Step 32 of Epoch: 40; Loss 1.452177 \n",
      "Step 33 of Epoch: 40; Loss 1.452477 \n",
      "Step 34 of Epoch: 40; Loss 1.452924 \n",
      "Step 35 of Epoch: 40; Loss 1.453474 \n",
      "Step 36 of Epoch: 40; Loss 1.453666 \n",
      "Step 37 of Epoch: 40; Loss 1.453782 \n",
      "Step 38 of Epoch: 40; Loss 1.454185 \n",
      "Step 39 of Epoch: 40; Loss 1.453929 \n",
      "Final Result for Epoch 40: Loss 1.453959; Val Acc 0.322926; Train Acc 0.421311\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 41; Loss 1.435317 \n",
      "Step 2 of Epoch: 41; Loss 1.434276 \n",
      "Step 3 of Epoch: 41; Loss 1.436263 \n",
      "Step 4 of Epoch: 41; Loss 1.439032 \n",
      "Step 5 of Epoch: 41; Loss 1.438098 \n",
      "Step 6 of Epoch: 41; Loss 1.435685 \n",
      "Step 7 of Epoch: 41; Loss 1.436955 \n",
      "Step 8 of Epoch: 41; Loss 1.438292 \n",
      "Step 9 of Epoch: 41; Loss 1.438251 \n",
      "Step 10 of Epoch: 41; Loss 1.439554 \n",
      "Step 11 of Epoch: 41; Loss 1.439117 \n",
      "Step 12 of Epoch: 41; Loss 1.439367 \n",
      "Step 13 of Epoch: 41; Loss 1.441728 \n",
      "Step 14 of Epoch: 41; Loss 1.442986 \n",
      "Step 15 of Epoch: 41; Loss 1.444114 \n",
      "Step 16 of Epoch: 41; Loss 1.444048 \n",
      "Step 17 of Epoch: 41; Loss 1.442910 \n",
      "Step 18 of Epoch: 41; Loss 1.443114 \n",
      "Step 19 of Epoch: 41; Loss 1.444269 \n",
      "Step 20 of Epoch: 41; Loss 1.444314 \n",
      "Step 21 of Epoch: 41; Loss 1.445117 \n",
      "Step 22 of Epoch: 41; Loss 1.445460 \n",
      "Step 23 of Epoch: 41; Loss 1.445765 \n",
      "Step 24 of Epoch: 41; Loss 1.446557 \n",
      "Step 25 of Epoch: 41; Loss 1.447412 \n",
      "Step 26 of Epoch: 41; Loss 1.447893 \n",
      "Step 27 of Epoch: 41; Loss 1.448000 \n",
      "Step 28 of Epoch: 41; Loss 1.448653 \n",
      "Step 29 of Epoch: 41; Loss 1.448707 \n",
      "Step 30 of Epoch: 41; Loss 1.449291 \n",
      "Step 31 of Epoch: 41; Loss 1.449137 \n",
      "Step 32 of Epoch: 41; Loss 1.449453 \n",
      "Step 33 of Epoch: 41; Loss 1.449941 \n",
      "Step 34 of Epoch: 41; Loss 1.449949 \n",
      "Step 35 of Epoch: 41; Loss 1.450206 \n",
      "Step 36 of Epoch: 41; Loss 1.450521 \n",
      "Step 37 of Epoch: 41; Loss 1.450814 \n",
      "Step 38 of Epoch: 41; Loss 1.451240 \n",
      "Step 39 of Epoch: 41; Loss 1.451329 \n",
      "Final Result for Epoch 41: Loss 1.451304; Val Acc 0.320882; Train Acc 0.423288\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 42; Loss 1.441552 \n",
      "Step 2 of Epoch: 42; Loss 1.433884 \n",
      "Step 3 of Epoch: 42; Loss 1.434128 \n",
      "Step 4 of Epoch: 42; Loss 1.433979 \n",
      "Step 5 of Epoch: 42; Loss 1.434799 \n",
      "Step 6 of Epoch: 42; Loss 1.437778 \n",
      "Step 7 of Epoch: 42; Loss 1.437075 \n",
      "Step 8 of Epoch: 42; Loss 1.438512 \n",
      "Step 9 of Epoch: 42; Loss 1.438568 \n",
      "Step 10 of Epoch: 42; Loss 1.439065 \n",
      "Step 11 of Epoch: 42; Loss 1.439629 \n",
      "Step 12 of Epoch: 42; Loss 1.441907 \n",
      "Step 13 of Epoch: 42; Loss 1.442111 \n",
      "Step 14 of Epoch: 42; Loss 1.442127 \n",
      "Step 15 of Epoch: 42; Loss 1.441716 \n",
      "Step 16 of Epoch: 42; Loss 1.442325 \n",
      "Step 17 of Epoch: 42; Loss 1.442805 \n",
      "Step 18 of Epoch: 42; Loss 1.444126 \n",
      "Step 19 of Epoch: 42; Loss 1.443890 \n",
      "Step 20 of Epoch: 42; Loss 1.444368 \n",
      "Step 21 of Epoch: 42; Loss 1.444980 \n",
      "Step 22 of Epoch: 42; Loss 1.445176 \n",
      "Step 23 of Epoch: 42; Loss 1.445340 \n",
      "Step 24 of Epoch: 42; Loss 1.445276 \n",
      "Step 25 of Epoch: 42; Loss 1.445491 \n",
      "Step 26 of Epoch: 42; Loss 1.446317 \n",
      "Step 27 of Epoch: 42; Loss 1.447055 \n",
      "Step 28 of Epoch: 42; Loss 1.447569 \n",
      "Step 29 of Epoch: 42; Loss 1.448115 \n",
      "Step 30 of Epoch: 42; Loss 1.448431 \n",
      "Step 31 of Epoch: 42; Loss 1.448007 \n",
      "Step 32 of Epoch: 42; Loss 1.448305 \n",
      "Step 33 of Epoch: 42; Loss 1.448324 \n",
      "Step 34 of Epoch: 42; Loss 1.448687 \n",
      "Step 35 of Epoch: 42; Loss 1.449053 \n",
      "Step 36 of Epoch: 42; Loss 1.449152 \n",
      "Step 37 of Epoch: 42; Loss 1.449876 \n",
      "Step 38 of Epoch: 42; Loss 1.449662 \n",
      "Step 39 of Epoch: 42; Loss 1.449978 \n",
      "Final Result for Epoch 42: Loss 1.450160; Val Acc 0.319840; Train Acc 0.424304\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 43; Loss 1.435670 \n",
      "Step 2 of Epoch: 43; Loss 1.432418 \n",
      "Step 3 of Epoch: 43; Loss 1.434305 \n",
      "Step 4 of Epoch: 43; Loss 1.436203 \n",
      "Step 5 of Epoch: 43; Loss 1.437566 \n",
      "Step 6 of Epoch: 43; Loss 1.437729 \n",
      "Step 7 of Epoch: 43; Loss 1.439631 \n",
      "Step 8 of Epoch: 43; Loss 1.440731 \n",
      "Step 9 of Epoch: 43; Loss 1.440135 \n",
      "Step 10 of Epoch: 43; Loss 1.440812 \n",
      "Step 11 of Epoch: 43; Loss 1.441795 \n",
      "Step 12 of Epoch: 43; Loss 1.440963 \n",
      "Step 13 of Epoch: 43; Loss 1.440329 \n",
      "Step 14 of Epoch: 43; Loss 1.441045 \n",
      "Step 15 of Epoch: 43; Loss 1.441423 \n",
      "Step 16 of Epoch: 43; Loss 1.442071 \n",
      "Step 17 of Epoch: 43; Loss 1.442290 \n",
      "Step 18 of Epoch: 43; Loss 1.442242 \n",
      "Step 19 of Epoch: 43; Loss 1.443499 \n",
      "Step 20 of Epoch: 43; Loss 1.442986 \n",
      "Step 21 of Epoch: 43; Loss 1.443403 \n",
      "Step 22 of Epoch: 43; Loss 1.444370 \n",
      "Step 23 of Epoch: 43; Loss 1.444681 \n",
      "Step 24 of Epoch: 43; Loss 1.444754 \n",
      "Step 25 of Epoch: 43; Loss 1.444718 \n",
      "Step 26 of Epoch: 43; Loss 1.444597 \n",
      "Step 27 of Epoch: 43; Loss 1.444692 \n",
      "Step 28 of Epoch: 43; Loss 1.445325 \n",
      "Step 29 of Epoch: 43; Loss 1.445507 \n",
      "Step 30 of Epoch: 43; Loss 1.445750 \n",
      "Step 31 of Epoch: 43; Loss 1.446208 \n",
      "Step 32 of Epoch: 43; Loss 1.446822 \n",
      "Step 33 of Epoch: 43; Loss 1.446576 \n",
      "Step 34 of Epoch: 43; Loss 1.447110 \n",
      "Step 35 of Epoch: 43; Loss 1.447075 \n",
      "Step 36 of Epoch: 43; Loss 1.447774 \n",
      "Step 37 of Epoch: 43; Loss 1.447960 \n",
      "Step 38 of Epoch: 43; Loss 1.447796 \n",
      "Step 39 of Epoch: 43; Loss 1.447735 \n",
      "Final Result for Epoch 43: Loss 1.447844; Val Acc 0.320922; Train Acc 0.424479\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 44; Loss 1.430558 \n",
      "Step 2 of Epoch: 44; Loss 1.425714 \n",
      "Step 3 of Epoch: 44; Loss 1.428154 \n",
      "Step 4 of Epoch: 44; Loss 1.426469 \n",
      "Step 5 of Epoch: 44; Loss 1.432788 \n",
      "Step 6 of Epoch: 44; Loss 1.434065 \n",
      "Step 7 of Epoch: 44; Loss 1.436215 \n",
      "Step 8 of Epoch: 44; Loss 1.437987 \n",
      "Step 9 of Epoch: 44; Loss 1.439438 \n",
      "Step 10 of Epoch: 44; Loss 1.437829 \n",
      "Step 11 of Epoch: 44; Loss 1.440476 \n",
      "Step 12 of Epoch: 44; Loss 1.441835 \n",
      "Step 13 of Epoch: 44; Loss 1.441147 \n",
      "Step 14 of Epoch: 44; Loss 1.441685 \n",
      "Step 15 of Epoch: 44; Loss 1.441463 \n",
      "Step 16 of Epoch: 44; Loss 1.440718 \n",
      "Step 17 of Epoch: 44; Loss 1.440510 \n",
      "Step 18 of Epoch: 44; Loss 1.441006 \n",
      "Step 19 of Epoch: 44; Loss 1.441679 \n",
      "Step 20 of Epoch: 44; Loss 1.441474 \n",
      "Step 21 of Epoch: 44; Loss 1.441590 \n",
      "Step 22 of Epoch: 44; Loss 1.441713 \n",
      "Step 23 of Epoch: 44; Loss 1.441790 \n",
      "Step 24 of Epoch: 44; Loss 1.441479 \n",
      "Step 25 of Epoch: 44; Loss 1.441527 \n",
      "Step 26 of Epoch: 44; Loss 1.442139 \n",
      "Step 27 of Epoch: 44; Loss 1.443479 \n",
      "Step 28 of Epoch: 44; Loss 1.443800 \n",
      "Step 29 of Epoch: 44; Loss 1.444719 \n",
      "Step 30 of Epoch: 44; Loss 1.444704 \n",
      "Step 31 of Epoch: 44; Loss 1.444401 \n",
      "Step 32 of Epoch: 44; Loss 1.444851 \n",
      "Step 33 of Epoch: 44; Loss 1.444565 \n",
      "Step 34 of Epoch: 44; Loss 1.444892 \n",
      "Step 35 of Epoch: 44; Loss 1.445249 \n",
      "Step 36 of Epoch: 44; Loss 1.445617 \n",
      "Step 37 of Epoch: 44; Loss 1.445607 \n",
      "Step 38 of Epoch: 44; Loss 1.446134 \n",
      "Step 39 of Epoch: 44; Loss 1.446817 \n",
      "Final Result for Epoch 44: Loss 1.446811; Val Acc 0.319279; Train Acc 0.425666\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 45; Loss 1.430822 \n",
      "Step 2 of Epoch: 45; Loss 1.426300 \n",
      "Step 3 of Epoch: 45; Loss 1.429257 \n",
      "Step 4 of Epoch: 45; Loss 1.429599 \n",
      "Step 5 of Epoch: 45; Loss 1.431727 \n",
      "Step 6 of Epoch: 45; Loss 1.430812 \n",
      "Step 7 of Epoch: 45; Loss 1.434506 \n",
      "Step 8 of Epoch: 45; Loss 1.434253 \n",
      "Step 9 of Epoch: 45; Loss 1.435817 \n",
      "Step 10 of Epoch: 45; Loss 1.435530 \n",
      "Step 11 of Epoch: 45; Loss 1.434990 \n",
      "Step 12 of Epoch: 45; Loss 1.435150 \n",
      "Step 13 of Epoch: 45; Loss 1.436050 \n",
      "Step 14 of Epoch: 45; Loss 1.435981 \n",
      "Step 15 of Epoch: 45; Loss 1.436187 \n",
      "Step 16 of Epoch: 45; Loss 1.436342 \n",
      "Step 17 of Epoch: 45; Loss 1.436820 \n",
      "Step 18 of Epoch: 45; Loss 1.437153 \n",
      "Step 19 of Epoch: 45; Loss 1.437122 \n",
      "Step 20 of Epoch: 45; Loss 1.436731 \n",
      "Step 21 of Epoch: 45; Loss 1.437241 \n",
      "Step 22 of Epoch: 45; Loss 1.437478 \n",
      "Step 23 of Epoch: 45; Loss 1.437691 \n",
      "Step 24 of Epoch: 45; Loss 1.438773 \n",
      "Step 25 of Epoch: 45; Loss 1.439034 \n",
      "Step 26 of Epoch: 45; Loss 1.439027 \n",
      "Step 27 of Epoch: 45; Loss 1.439508 \n",
      "Step 28 of Epoch: 45; Loss 1.439628 \n",
      "Step 29 of Epoch: 45; Loss 1.440305 \n",
      "Step 30 of Epoch: 45; Loss 1.440709 \n",
      "Step 31 of Epoch: 45; Loss 1.440409 \n",
      "Step 32 of Epoch: 45; Loss 1.441361 \n",
      "Step 33 of Epoch: 45; Loss 1.442118 \n",
      "Step 34 of Epoch: 45; Loss 1.442091 \n",
      "Step 35 of Epoch: 45; Loss 1.442751 \n",
      "Step 36 of Epoch: 45; Loss 1.443444 \n",
      "Step 37 of Epoch: 45; Loss 1.443155 \n",
      "Step 38 of Epoch: 45; Loss 1.443828 \n",
      "Step 39 of Epoch: 45; Loss 1.444504 \n",
      "Final Result for Epoch 45: Loss 1.444891; Val Acc 0.322445; Train Acc 0.426847\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 46; Loss 1.423864 \n",
      "Step 2 of Epoch: 46; Loss 1.424731 \n",
      "Step 3 of Epoch: 46; Loss 1.433972 \n",
      "Step 4 of Epoch: 46; Loss 1.432453 \n",
      "Step 5 of Epoch: 46; Loss 1.434970 \n",
      "Step 6 of Epoch: 46; Loss 1.433667 \n",
      "Step 7 of Epoch: 46; Loss 1.434966 \n",
      "Step 8 of Epoch: 46; Loss 1.435218 \n",
      "Step 9 of Epoch: 46; Loss 1.437478 \n",
      "Step 10 of Epoch: 46; Loss 1.439006 \n",
      "Step 11 of Epoch: 46; Loss 1.439500 \n",
      "Step 12 of Epoch: 46; Loss 1.439804 \n",
      "Step 13 of Epoch: 46; Loss 1.440047 \n",
      "Step 14 of Epoch: 46; Loss 1.439016 \n",
      "Step 15 of Epoch: 46; Loss 1.438827 \n",
      "Step 16 of Epoch: 46; Loss 1.438252 \n",
      "Step 17 of Epoch: 46; Loss 1.438328 \n",
      "Step 18 of Epoch: 46; Loss 1.438225 \n",
      "Step 19 of Epoch: 46; Loss 1.438576 \n",
      "Step 20 of Epoch: 46; Loss 1.438451 \n",
      "Step 21 of Epoch: 46; Loss 1.438483 \n",
      "Step 22 of Epoch: 46; Loss 1.439029 \n",
      "Step 23 of Epoch: 46; Loss 1.439810 \n",
      "Step 24 of Epoch: 46; Loss 1.439492 \n",
      "Step 25 of Epoch: 46; Loss 1.440215 \n",
      "Step 26 of Epoch: 46; Loss 1.440202 \n",
      "Step 27 of Epoch: 46; Loss 1.440762 \n",
      "Step 28 of Epoch: 46; Loss 1.440573 \n",
      "Step 29 of Epoch: 46; Loss 1.441017 \n",
      "Step 30 of Epoch: 46; Loss 1.441532 \n",
      "Step 31 of Epoch: 46; Loss 1.441831 \n",
      "Step 32 of Epoch: 46; Loss 1.442226 \n",
      "Step 33 of Epoch: 46; Loss 1.442661 \n",
      "Step 34 of Epoch: 46; Loss 1.442978 \n",
      "Step 35 of Epoch: 46; Loss 1.443068 \n",
      "Step 36 of Epoch: 46; Loss 1.443233 \n",
      "Step 37 of Epoch: 46; Loss 1.443691 \n",
      "Step 38 of Epoch: 46; Loss 1.443718 \n",
      "Step 39 of Epoch: 46; Loss 1.443838 \n",
      "Final Result for Epoch 46: Loss 1.443897; Val Acc 0.322685; Train Acc 0.426311\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 47; Loss 1.425937 \n",
      "Step 2 of Epoch: 47; Loss 1.418025 \n",
      "Step 3 of Epoch: 47; Loss 1.418156 \n",
      "Step 4 of Epoch: 47; Loss 1.419575 \n",
      "Step 5 of Epoch: 47; Loss 1.420552 \n",
      "Step 6 of Epoch: 47; Loss 1.423567 \n",
      "Step 7 of Epoch: 47; Loss 1.425082 \n",
      "Step 8 of Epoch: 47; Loss 1.426747 \n",
      "Step 9 of Epoch: 47; Loss 1.429364 \n",
      "Step 10 of Epoch: 47; Loss 1.429250 \n",
      "Step 11 of Epoch: 47; Loss 1.430476 \n",
      "Step 12 of Epoch: 47; Loss 1.430631 \n",
      "Step 13 of Epoch: 47; Loss 1.430902 \n",
      "Step 14 of Epoch: 47; Loss 1.431787 \n",
      "Step 15 of Epoch: 47; Loss 1.432950 \n",
      "Step 16 of Epoch: 47; Loss 1.432461 \n",
      "Step 17 of Epoch: 47; Loss 1.432613 \n",
      "Step 18 of Epoch: 47; Loss 1.432578 \n",
      "Step 19 of Epoch: 47; Loss 1.433418 \n",
      "Step 20 of Epoch: 47; Loss 1.433307 \n",
      "Step 21 of Epoch: 47; Loss 1.434017 \n",
      "Step 22 of Epoch: 47; Loss 1.434938 \n",
      "Step 23 of Epoch: 47; Loss 1.435625 \n",
      "Step 24 of Epoch: 47; Loss 1.435939 \n",
      "Step 25 of Epoch: 47; Loss 1.436290 \n",
      "Step 26 of Epoch: 47; Loss 1.436585 \n",
      "Step 27 of Epoch: 47; Loss 1.436872 \n",
      "Step 28 of Epoch: 47; Loss 1.437202 \n",
      "Step 29 of Epoch: 47; Loss 1.437722 \n",
      "Step 30 of Epoch: 47; Loss 1.438129 \n",
      "Step 31 of Epoch: 47; Loss 1.439182 \n",
      "Step 32 of Epoch: 47; Loss 1.439603 \n",
      "Step 33 of Epoch: 47; Loss 1.439506 \n",
      "Step 34 of Epoch: 47; Loss 1.440059 \n",
      "Step 35 of Epoch: 47; Loss 1.440276 \n",
      "Step 36 of Epoch: 47; Loss 1.441006 \n",
      "Step 37 of Epoch: 47; Loss 1.441530 \n",
      "Step 38 of Epoch: 47; Loss 1.441494 \n",
      "Step 39 of Epoch: 47; Loss 1.441808 \n",
      "Final Result for Epoch 47: Loss 1.442185; Val Acc 0.320240; Train Acc 0.425751\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 48; Loss 1.437281 \n",
      "Step 2 of Epoch: 48; Loss 1.421093 \n",
      "Step 3 of Epoch: 48; Loss 1.420726 \n",
      "Step 4 of Epoch: 48; Loss 1.420144 \n",
      "Step 5 of Epoch: 48; Loss 1.424149 \n",
      "Step 6 of Epoch: 48; Loss 1.425129 \n",
      "Step 7 of Epoch: 48; Loss 1.429453 \n",
      "Step 8 of Epoch: 48; Loss 1.426138 \n",
      "Step 9 of Epoch: 48; Loss 1.428602 \n",
      "Step 10 of Epoch: 48; Loss 1.429860 \n",
      "Step 11 of Epoch: 48; Loss 1.430288 \n",
      "Step 12 of Epoch: 48; Loss 1.430681 \n",
      "Step 13 of Epoch: 48; Loss 1.431464 \n",
      "Step 14 of Epoch: 48; Loss 1.431257 \n",
      "Step 15 of Epoch: 48; Loss 1.431122 \n",
      "Step 16 of Epoch: 48; Loss 1.431556 \n",
      "Step 17 of Epoch: 48; Loss 1.433164 \n",
      "Step 18 of Epoch: 48; Loss 1.433198 \n",
      "Step 19 of Epoch: 48; Loss 1.434591 \n",
      "Step 20 of Epoch: 48; Loss 1.433943 \n",
      "Step 21 of Epoch: 48; Loss 1.434662 \n",
      "Step 22 of Epoch: 48; Loss 1.435313 \n",
      "Step 23 of Epoch: 48; Loss 1.435375 \n",
      "Step 24 of Epoch: 48; Loss 1.435331 \n",
      "Step 25 of Epoch: 48; Loss 1.436193 \n",
      "Step 26 of Epoch: 48; Loss 1.436598 \n",
      "Step 27 of Epoch: 48; Loss 1.436953 \n",
      "Step 28 of Epoch: 48; Loss 1.437843 \n",
      "Step 29 of Epoch: 48; Loss 1.438110 \n",
      "Step 30 of Epoch: 48; Loss 1.438499 \n",
      "Step 31 of Epoch: 48; Loss 1.439287 \n",
      "Step 32 of Epoch: 48; Loss 1.439317 \n",
      "Step 33 of Epoch: 48; Loss 1.439493 \n",
      "Step 34 of Epoch: 48; Loss 1.439823 \n",
      "Step 35 of Epoch: 48; Loss 1.439933 \n",
      "Step 36 of Epoch: 48; Loss 1.439885 \n",
      "Step 37 of Epoch: 48; Loss 1.440227 \n",
      "Step 38 of Epoch: 48; Loss 1.440293 \n",
      "Step 39 of Epoch: 48; Loss 1.440494 \n",
      "Final Result for Epoch 48: Loss 1.440696; Val Acc 0.319599; Train Acc 0.427047\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 49; Loss 1.419968 \n",
      "Step 2 of Epoch: 49; Loss 1.428427 \n",
      "Step 3 of Epoch: 49; Loss 1.424819 \n",
      "Step 4 of Epoch: 49; Loss 1.424157 \n",
      "Step 5 of Epoch: 49; Loss 1.425345 \n",
      "Step 6 of Epoch: 49; Loss 1.425743 \n",
      "Step 7 of Epoch: 49; Loss 1.425296 \n",
      "Step 8 of Epoch: 49; Loss 1.426394 \n",
      "Step 9 of Epoch: 49; Loss 1.426842 \n",
      "Step 10 of Epoch: 49; Loss 1.427488 \n",
      "Step 11 of Epoch: 49; Loss 1.428877 \n",
      "Step 12 of Epoch: 49; Loss 1.429054 \n",
      "Step 13 of Epoch: 49; Loss 1.429184 \n",
      "Step 14 of Epoch: 49; Loss 1.430356 \n",
      "Step 15 of Epoch: 49; Loss 1.430823 \n",
      "Step 16 of Epoch: 49; Loss 1.432162 \n",
      "Step 17 of Epoch: 49; Loss 1.433192 \n",
      "Step 18 of Epoch: 49; Loss 1.433116 \n",
      "Step 19 of Epoch: 49; Loss 1.433772 \n",
      "Step 20 of Epoch: 49; Loss 1.433313 \n",
      "Step 21 of Epoch: 49; Loss 1.434531 \n",
      "Step 22 of Epoch: 49; Loss 1.434436 \n",
      "Step 23 of Epoch: 49; Loss 1.433872 \n",
      "Step 24 of Epoch: 49; Loss 1.434253 \n",
      "Step 25 of Epoch: 49; Loss 1.434993 \n",
      "Step 26 of Epoch: 49; Loss 1.435991 \n",
      "Step 27 of Epoch: 49; Loss 1.436263 \n",
      "Step 28 of Epoch: 49; Loss 1.436587 \n",
      "Step 29 of Epoch: 49; Loss 1.436514 \n",
      "Step 30 of Epoch: 49; Loss 1.436611 \n",
      "Step 31 of Epoch: 49; Loss 1.437452 \n",
      "Step 32 of Epoch: 49; Loss 1.437967 \n",
      "Step 33 of Epoch: 49; Loss 1.437807 \n",
      "Step 34 of Epoch: 49; Loss 1.438560 \n",
      "Step 35 of Epoch: 49; Loss 1.438752 \n",
      "Step 36 of Epoch: 49; Loss 1.439218 \n",
      "Step 37 of Epoch: 49; Loss 1.439381 \n",
      "Step 38 of Epoch: 49; Loss 1.439595 \n",
      "Step 39 of Epoch: 49; Loss 1.439545 \n",
      "Final Result for Epoch 49: Loss 1.439975; Val Acc 0.320641; Train Acc 0.429069\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 50; Loss 1.419069 \n",
      "Step 2 of Epoch: 50; Loss 1.419936 \n",
      "Step 3 of Epoch: 50; Loss 1.423732 \n",
      "Step 4 of Epoch: 50; Loss 1.425516 \n",
      "Step 5 of Epoch: 50; Loss 1.426376 \n",
      "Step 6 of Epoch: 50; Loss 1.428496 \n",
      "Step 7 of Epoch: 50; Loss 1.427698 \n",
      "Step 8 of Epoch: 50; Loss 1.427728 \n",
      "Step 9 of Epoch: 50; Loss 1.427679 \n",
      "Step 10 of Epoch: 50; Loss 1.426856 \n",
      "Step 11 of Epoch: 50; Loss 1.428186 \n",
      "Step 12 of Epoch: 50; Loss 1.429426 \n",
      "Step 13 of Epoch: 50; Loss 1.429231 \n",
      "Step 14 of Epoch: 50; Loss 1.430342 \n",
      "Step 15 of Epoch: 50; Loss 1.430863 \n",
      "Step 16 of Epoch: 50; Loss 1.432035 \n",
      "Step 17 of Epoch: 50; Loss 1.432394 \n",
      "Step 18 of Epoch: 50; Loss 1.433232 \n",
      "Step 19 of Epoch: 50; Loss 1.433264 \n",
      "Step 20 of Epoch: 50; Loss 1.433154 \n",
      "Step 21 of Epoch: 50; Loss 1.434562 \n",
      "Step 22 of Epoch: 50; Loss 1.435254 \n",
      "Step 23 of Epoch: 50; Loss 1.436100 \n",
      "Step 24 of Epoch: 50; Loss 1.436881 \n",
      "Step 25 of Epoch: 50; Loss 1.436771 \n",
      "Step 26 of Epoch: 50; Loss 1.437089 \n",
      "Step 27 of Epoch: 50; Loss 1.437078 \n",
      "Step 28 of Epoch: 50; Loss 1.437798 \n",
      "Step 29 of Epoch: 50; Loss 1.437330 \n",
      "Step 30 of Epoch: 50; Loss 1.437395 \n",
      "Step 31 of Epoch: 50; Loss 1.437588 \n",
      "Step 32 of Epoch: 50; Loss 1.437681 \n",
      "Step 33 of Epoch: 50; Loss 1.437704 \n",
      "Step 34 of Epoch: 50; Loss 1.437926 \n",
      "Step 35 of Epoch: 50; Loss 1.437855 \n",
      "Step 36 of Epoch: 50; Loss 1.437624 \n",
      "Step 37 of Epoch: 50; Loss 1.437468 \n",
      "Step 38 of Epoch: 50; Loss 1.437627 \n",
      "Step 39 of Epoch: 50; Loss 1.438456 \n",
      "Final Result for Epoch 50: Loss 1.439020; Val Acc 0.315872; Train Acc 0.426777\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 51; Loss 1.418902 \n",
      "Step 2 of Epoch: 51; Loss 1.426118 \n",
      "Step 3 of Epoch: 51; Loss 1.421216 \n",
      "Step 4 of Epoch: 51; Loss 1.423863 \n",
      "Step 5 of Epoch: 51; Loss 1.426829 \n",
      "Step 6 of Epoch: 51; Loss 1.426874 \n",
      "Step 7 of Epoch: 51; Loss 1.428596 \n",
      "Step 8 of Epoch: 51; Loss 1.429368 \n",
      "Step 9 of Epoch: 51; Loss 1.427546 \n",
      "Step 10 of Epoch: 51; Loss 1.428951 \n",
      "Step 11 of Epoch: 51; Loss 1.429955 \n",
      "Step 12 of Epoch: 51; Loss 1.430469 \n",
      "Step 13 of Epoch: 51; Loss 1.430383 \n",
      "Step 14 of Epoch: 51; Loss 1.429237 \n",
      "Step 15 of Epoch: 51; Loss 1.431051 \n",
      "Step 16 of Epoch: 51; Loss 1.431156 \n",
      "Step 17 of Epoch: 51; Loss 1.431240 \n",
      "Step 18 of Epoch: 51; Loss 1.431019 \n",
      "Step 19 of Epoch: 51; Loss 1.430140 \n",
      "Step 20 of Epoch: 51; Loss 1.430929 \n",
      "Step 21 of Epoch: 51; Loss 1.431942 \n",
      "Step 22 of Epoch: 51; Loss 1.432304 \n",
      "Step 23 of Epoch: 51; Loss 1.432828 \n",
      "Step 24 of Epoch: 51; Loss 1.432052 \n",
      "Step 25 of Epoch: 51; Loss 1.432282 \n",
      "Step 26 of Epoch: 51; Loss 1.432983 \n",
      "Step 27 of Epoch: 51; Loss 1.432809 \n",
      "Step 28 of Epoch: 51; Loss 1.433606 \n",
      "Step 29 of Epoch: 51; Loss 1.433758 \n",
      "Step 30 of Epoch: 51; Loss 1.433990 \n",
      "Step 31 of Epoch: 51; Loss 1.433792 \n",
      "Step 32 of Epoch: 51; Loss 1.434616 \n",
      "Step 33 of Epoch: 51; Loss 1.435094 \n",
      "Step 34 of Epoch: 51; Loss 1.435729 \n",
      "Step 35 of Epoch: 51; Loss 1.435900 \n",
      "Step 36 of Epoch: 51; Loss 1.436081 \n",
      "Step 37 of Epoch: 51; Loss 1.436459 \n",
      "Step 38 of Epoch: 51; Loss 1.436945 \n",
      "Step 39 of Epoch: 51; Loss 1.437206 \n",
      "Final Result for Epoch 51: Loss 1.437566; Val Acc 0.316673; Train Acc 0.431046\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 52; Loss 1.418125 \n",
      "Step 2 of Epoch: 52; Loss 1.418615 \n",
      "Step 3 of Epoch: 52; Loss 1.416306 \n",
      "Step 4 of Epoch: 52; Loss 1.414320 \n",
      "Step 5 of Epoch: 52; Loss 1.418859 \n",
      "Step 6 of Epoch: 52; Loss 1.417595 \n",
      "Step 7 of Epoch: 52; Loss 1.418461 \n",
      "Step 8 of Epoch: 52; Loss 1.418549 \n",
      "Step 9 of Epoch: 52; Loss 1.420676 \n",
      "Step 10 of Epoch: 52; Loss 1.422288 \n",
      "Step 11 of Epoch: 52; Loss 1.425258 \n",
      "Step 12 of Epoch: 52; Loss 1.426175 \n",
      "Step 13 of Epoch: 52; Loss 1.427442 \n",
      "Step 14 of Epoch: 52; Loss 1.428025 \n",
      "Step 15 of Epoch: 52; Loss 1.427536 \n",
      "Step 16 of Epoch: 52; Loss 1.427938 \n",
      "Step 17 of Epoch: 52; Loss 1.428815 \n",
      "Step 18 of Epoch: 52; Loss 1.429790 \n",
      "Step 19 of Epoch: 52; Loss 1.429509 \n",
      "Step 20 of Epoch: 52; Loss 1.430293 \n",
      "Step 21 of Epoch: 52; Loss 1.430691 \n",
      "Step 22 of Epoch: 52; Loss 1.430490 \n",
      "Step 23 of Epoch: 52; Loss 1.431028 \n",
      "Step 24 of Epoch: 52; Loss 1.431550 \n",
      "Step 25 of Epoch: 52; Loss 1.431258 \n",
      "Step 26 of Epoch: 52; Loss 1.432272 \n",
      "Step 27 of Epoch: 52; Loss 1.432409 \n",
      "Step 28 of Epoch: 52; Loss 1.432614 \n",
      "Step 29 of Epoch: 52; Loss 1.433125 \n",
      "Step 30 of Epoch: 52; Loss 1.433280 \n",
      "Step 31 of Epoch: 52; Loss 1.433605 \n",
      "Step 32 of Epoch: 52; Loss 1.433967 \n",
      "Step 33 of Epoch: 52; Loss 1.433154 \n",
      "Step 34 of Epoch: 52; Loss 1.433829 \n",
      "Step 35 of Epoch: 52; Loss 1.433509 \n",
      "Step 36 of Epoch: 52; Loss 1.433666 \n",
      "Step 37 of Epoch: 52; Loss 1.434715 \n",
      "Step 38 of Epoch: 52; Loss 1.435054 \n",
      "Step 39 of Epoch: 52; Loss 1.435850 \n",
      "Final Result for Epoch 52: Loss 1.436285; Val Acc 0.317876; Train Acc 0.427277\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 53; Loss 1.406471 \n",
      "Step 2 of Epoch: 53; Loss 1.412427 \n",
      "Step 3 of Epoch: 53; Loss 1.412142 \n",
      "Step 4 of Epoch: 53; Loss 1.413449 \n",
      "Step 5 of Epoch: 53; Loss 1.420558 \n",
      "Step 6 of Epoch: 53; Loss 1.421303 \n",
      "Step 7 of Epoch: 53; Loss 1.418864 \n",
      "Step 8 of Epoch: 53; Loss 1.418305 \n",
      "Step 9 of Epoch: 53; Loss 1.418252 \n",
      "Step 10 of Epoch: 53; Loss 1.419377 \n",
      "Step 11 of Epoch: 53; Loss 1.421466 \n",
      "Step 12 of Epoch: 53; Loss 1.421392 \n",
      "Step 13 of Epoch: 53; Loss 1.421660 \n",
      "Step 14 of Epoch: 53; Loss 1.423638 \n",
      "Step 15 of Epoch: 53; Loss 1.424063 \n",
      "Step 16 of Epoch: 53; Loss 1.425562 \n",
      "Step 17 of Epoch: 53; Loss 1.426055 \n",
      "Step 18 of Epoch: 53; Loss 1.426775 \n",
      "Step 19 of Epoch: 53; Loss 1.426901 \n",
      "Step 20 of Epoch: 53; Loss 1.428625 \n",
      "Step 21 of Epoch: 53; Loss 1.429020 \n",
      "Step 22 of Epoch: 53; Loss 1.430577 \n",
      "Step 23 of Epoch: 53; Loss 1.430387 \n",
      "Step 24 of Epoch: 53; Loss 1.430341 \n",
      "Step 25 of Epoch: 53; Loss 1.430887 \n",
      "Step 26 of Epoch: 53; Loss 1.430904 \n",
      "Step 27 of Epoch: 53; Loss 1.431415 \n",
      "Step 28 of Epoch: 53; Loss 1.432021 \n",
      "Step 29 of Epoch: 53; Loss 1.432688 \n",
      "Step 30 of Epoch: 53; Loss 1.433033 \n",
      "Step 31 of Epoch: 53; Loss 1.433344 \n",
      "Step 32 of Epoch: 53; Loss 1.433913 \n",
      "Step 33 of Epoch: 53; Loss 1.433555 \n",
      "Step 34 of Epoch: 53; Loss 1.434324 \n",
      "Step 35 of Epoch: 53; Loss 1.434944 \n",
      "Step 36 of Epoch: 53; Loss 1.435207 \n",
      "Step 37 of Epoch: 53; Loss 1.435475 \n",
      "Step 38 of Epoch: 53; Loss 1.435570 \n",
      "Step 39 of Epoch: 53; Loss 1.435402 \n",
      "Final Result for Epoch 53: Loss 1.435681; Val Acc 0.319479; Train Acc 0.429494\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 54; Loss 1.425073 \n",
      "Step 2 of Epoch: 54; Loss 1.428594 \n",
      "Step 3 of Epoch: 54; Loss 1.422442 \n",
      "Step 4 of Epoch: 54; Loss 1.423845 \n",
      "Step 5 of Epoch: 54; Loss 1.422985 \n",
      "Step 6 of Epoch: 54; Loss 1.423567 \n",
      "Step 7 of Epoch: 54; Loss 1.425383 \n",
      "Step 8 of Epoch: 54; Loss 1.422679 \n",
      "Step 9 of Epoch: 54; Loss 1.425141 \n",
      "Step 10 of Epoch: 54; Loss 1.424948 \n",
      "Step 11 of Epoch: 54; Loss 1.424581 \n",
      "Step 12 of Epoch: 54; Loss 1.423820 \n",
      "Step 13 of Epoch: 54; Loss 1.424945 \n",
      "Step 14 of Epoch: 54; Loss 1.425119 \n",
      "Step 15 of Epoch: 54; Loss 1.425127 \n",
      "Step 16 of Epoch: 54; Loss 1.424883 \n",
      "Step 17 of Epoch: 54; Loss 1.424269 \n",
      "Step 18 of Epoch: 54; Loss 1.424931 \n",
      "Step 19 of Epoch: 54; Loss 1.425820 \n",
      "Step 20 of Epoch: 54; Loss 1.425710 \n",
      "Step 21 of Epoch: 54; Loss 1.424991 \n",
      "Step 22 of Epoch: 54; Loss 1.425871 \n",
      "Step 23 of Epoch: 54; Loss 1.426123 \n",
      "Step 24 of Epoch: 54; Loss 1.426406 \n",
      "Step 25 of Epoch: 54; Loss 1.427199 \n",
      "Step 26 of Epoch: 54; Loss 1.428107 \n",
      "Step 27 of Epoch: 54; Loss 1.428877 \n",
      "Step 28 of Epoch: 54; Loss 1.428745 \n",
      "Step 29 of Epoch: 54; Loss 1.429061 \n",
      "Step 30 of Epoch: 54; Loss 1.429021 \n",
      "Step 31 of Epoch: 54; Loss 1.429170 \n",
      "Step 32 of Epoch: 54; Loss 1.429374 \n",
      "Step 33 of Epoch: 54; Loss 1.430132 \n",
      "Step 34 of Epoch: 54; Loss 1.430995 \n",
      "Step 35 of Epoch: 54; Loss 1.431751 \n",
      "Step 36 of Epoch: 54; Loss 1.431620 \n",
      "Step 37 of Epoch: 54; Loss 1.432156 \n",
      "Step 38 of Epoch: 54; Loss 1.432863 \n",
      "Step 39 of Epoch: 54; Loss 1.433181 \n",
      "Final Result for Epoch 54: Loss 1.433810; Val Acc 0.319399; Train Acc 0.430706\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 55; Loss 1.398713 \n",
      "Step 2 of Epoch: 55; Loss 1.405900 \n",
      "Step 3 of Epoch: 55; Loss 1.411893 \n",
      "Step 4 of Epoch: 55; Loss 1.415104 \n",
      "Step 5 of Epoch: 55; Loss 1.419702 \n",
      "Step 6 of Epoch: 55; Loss 1.420075 \n",
      "Step 7 of Epoch: 55; Loss 1.421717 \n",
      "Step 8 of Epoch: 55; Loss 1.421026 \n",
      "Step 9 of Epoch: 55; Loss 1.420746 \n",
      "Step 10 of Epoch: 55; Loss 1.421715 \n",
      "Step 11 of Epoch: 55; Loss 1.422468 \n",
      "Step 12 of Epoch: 55; Loss 1.421204 \n",
      "Step 13 of Epoch: 55; Loss 1.421958 \n",
      "Step 14 of Epoch: 55; Loss 1.423318 \n",
      "Step 15 of Epoch: 55; Loss 1.425319 \n",
      "Step 16 of Epoch: 55; Loss 1.426610 \n",
      "Step 17 of Epoch: 55; Loss 1.426504 \n",
      "Step 18 of Epoch: 55; Loss 1.426190 \n",
      "Step 19 of Epoch: 55; Loss 1.427042 \n",
      "Step 20 of Epoch: 55; Loss 1.426521 \n",
      "Step 21 of Epoch: 55; Loss 1.426657 \n",
      "Step 22 of Epoch: 55; Loss 1.426484 \n",
      "Step 23 of Epoch: 55; Loss 1.427269 \n",
      "Step 24 of Epoch: 55; Loss 1.428072 \n",
      "Step 25 of Epoch: 55; Loss 1.428008 \n",
      "Step 26 of Epoch: 55; Loss 1.428975 \n",
      "Step 27 of Epoch: 55; Loss 1.429330 \n",
      "Step 28 of Epoch: 55; Loss 1.429703 \n",
      "Step 29 of Epoch: 55; Loss 1.429507 \n",
      "Step 30 of Epoch: 55; Loss 1.429626 \n",
      "Step 31 of Epoch: 55; Loss 1.429900 \n",
      "Step 32 of Epoch: 55; Loss 1.430645 \n",
      "Step 33 of Epoch: 55; Loss 1.431165 \n",
      "Step 34 of Epoch: 55; Loss 1.431948 \n",
      "Step 35 of Epoch: 55; Loss 1.432201 \n",
      "Step 36 of Epoch: 55; Loss 1.432504 \n",
      "Step 37 of Epoch: 55; Loss 1.432178 \n",
      "Step 38 of Epoch: 55; Loss 1.432337 \n",
      "Step 39 of Epoch: 55; Loss 1.432433 \n",
      "Final Result for Epoch 55: Loss 1.433177; Val Acc 0.320561; Train Acc 0.430781\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 56; Loss 1.412097 \n",
      "Step 2 of Epoch: 56; Loss 1.405619 \n",
      "Step 3 of Epoch: 56; Loss 1.406828 \n",
      "Step 4 of Epoch: 56; Loss 1.407981 \n",
      "Step 5 of Epoch: 56; Loss 1.408846 \n",
      "Step 6 of Epoch: 56; Loss 1.413231 \n",
      "Step 7 of Epoch: 56; Loss 1.413768 \n",
      "Step 8 of Epoch: 56; Loss 1.414650 \n",
      "Step 9 of Epoch: 56; Loss 1.415223 \n",
      "Step 10 of Epoch: 56; Loss 1.417297 \n",
      "Step 11 of Epoch: 56; Loss 1.418982 \n",
      "Step 12 of Epoch: 56; Loss 1.418792 \n",
      "Step 13 of Epoch: 56; Loss 1.418623 \n",
      "Step 14 of Epoch: 56; Loss 1.420253 \n",
      "Step 15 of Epoch: 56; Loss 1.420711 \n",
      "Step 16 of Epoch: 56; Loss 1.420806 \n",
      "Step 17 of Epoch: 56; Loss 1.420811 \n",
      "Step 18 of Epoch: 56; Loss 1.421201 \n",
      "Step 19 of Epoch: 56; Loss 1.421647 \n",
      "Step 20 of Epoch: 56; Loss 1.421934 \n",
      "Step 21 of Epoch: 56; Loss 1.423647 \n",
      "Step 22 of Epoch: 56; Loss 1.424583 \n",
      "Step 23 of Epoch: 56; Loss 1.425572 \n",
      "Step 24 of Epoch: 56; Loss 1.426427 \n",
      "Step 25 of Epoch: 56; Loss 1.427040 \n",
      "Step 26 of Epoch: 56; Loss 1.428323 \n",
      "Step 27 of Epoch: 56; Loss 1.428647 \n",
      "Step 28 of Epoch: 56; Loss 1.428656 \n",
      "Step 29 of Epoch: 56; Loss 1.428436 \n",
      "Step 30 of Epoch: 56; Loss 1.428092 \n",
      "Step 31 of Epoch: 56; Loss 1.428406 \n",
      "Step 32 of Epoch: 56; Loss 1.428924 \n",
      "Step 33 of Epoch: 56; Loss 1.429103 \n",
      "Step 34 of Epoch: 56; Loss 1.429667 \n",
      "Step 35 of Epoch: 56; Loss 1.430322 \n",
      "Step 36 of Epoch: 56; Loss 1.431161 \n",
      "Step 37 of Epoch: 56; Loss 1.431507 \n",
      "Step 38 of Epoch: 56; Loss 1.431834 \n",
      "Step 39 of Epoch: 56; Loss 1.432444 \n",
      "Final Result for Epoch 56: Loss 1.432551; Val Acc 0.319238; Train Acc 0.431436\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 57; Loss 1.421427 \n",
      "Step 2 of Epoch: 57; Loss 1.413587 \n",
      "Step 3 of Epoch: 57; Loss 1.410761 \n",
      "Step 4 of Epoch: 57; Loss 1.410792 \n",
      "Step 5 of Epoch: 57; Loss 1.415884 \n",
      "Step 6 of Epoch: 57; Loss 1.418981 \n",
      "Step 7 of Epoch: 57; Loss 1.420407 \n",
      "Step 8 of Epoch: 57; Loss 1.419616 \n",
      "Step 9 of Epoch: 57; Loss 1.421266 \n",
      "Step 10 of Epoch: 57; Loss 1.422333 \n",
      "Step 11 of Epoch: 57; Loss 1.423626 \n",
      "Step 12 of Epoch: 57; Loss 1.422826 \n",
      "Step 13 of Epoch: 57; Loss 1.422779 \n",
      "Step 14 of Epoch: 57; Loss 1.424461 \n",
      "Step 15 of Epoch: 57; Loss 1.422925 \n",
      "Step 16 of Epoch: 57; Loss 1.424081 \n",
      "Step 17 of Epoch: 57; Loss 1.425215 \n",
      "Step 18 of Epoch: 57; Loss 1.425545 \n",
      "Step 19 of Epoch: 57; Loss 1.425312 \n",
      "Step 20 of Epoch: 57; Loss 1.426405 \n",
      "Step 21 of Epoch: 57; Loss 1.425966 \n",
      "Step 22 of Epoch: 57; Loss 1.426137 \n",
      "Step 23 of Epoch: 57; Loss 1.426267 \n",
      "Step 24 of Epoch: 57; Loss 1.426807 \n",
      "Step 25 of Epoch: 57; Loss 1.426432 \n",
      "Step 26 of Epoch: 57; Loss 1.426972 \n",
      "Step 27 of Epoch: 57; Loss 1.426770 \n",
      "Step 28 of Epoch: 57; Loss 1.426741 \n",
      "Step 29 of Epoch: 57; Loss 1.426999 \n",
      "Step 30 of Epoch: 57; Loss 1.427776 \n",
      "Step 31 of Epoch: 57; Loss 1.427667 \n",
      "Step 32 of Epoch: 57; Loss 1.428090 \n",
      "Step 33 of Epoch: 57; Loss 1.428753 \n",
      "Step 34 of Epoch: 57; Loss 1.429501 \n",
      "Step 35 of Epoch: 57; Loss 1.429944 \n",
      "Step 36 of Epoch: 57; Loss 1.430335 \n",
      "Step 37 of Epoch: 57; Loss 1.430948 \n",
      "Step 38 of Epoch: 57; Loss 1.430571 \n",
      "Step 39 of Epoch: 57; Loss 1.431150 \n",
      "Final Result for Epoch 57: Loss 1.431505; Val Acc 0.320321; Train Acc 0.429930\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 58; Loss 1.419744 \n",
      "Step 2 of Epoch: 58; Loss 1.424083 \n",
      "Step 3 of Epoch: 58; Loss 1.421757 \n",
      "Step 4 of Epoch: 58; Loss 1.420089 \n",
      "Step 5 of Epoch: 58; Loss 1.419425 \n",
      "Step 6 of Epoch: 58; Loss 1.423920 \n",
      "Step 7 of Epoch: 58; Loss 1.424783 \n",
      "Step 8 of Epoch: 58; Loss 1.426274 \n",
      "Step 9 of Epoch: 58; Loss 1.425757 \n",
      "Step 10 of Epoch: 58; Loss 1.427091 \n",
      "Step 11 of Epoch: 58; Loss 1.425748 \n",
      "Step 12 of Epoch: 58; Loss 1.426265 \n",
      "Step 13 of Epoch: 58; Loss 1.425644 \n",
      "Step 14 of Epoch: 58; Loss 1.425295 \n",
      "Step 15 of Epoch: 58; Loss 1.424613 \n",
      "Step 16 of Epoch: 58; Loss 1.424786 \n",
      "Step 17 of Epoch: 58; Loss 1.424642 \n",
      "Step 18 of Epoch: 58; Loss 1.424060 \n",
      "Step 19 of Epoch: 58; Loss 1.423862 \n",
      "Step 20 of Epoch: 58; Loss 1.424845 \n",
      "Step 21 of Epoch: 58; Loss 1.426017 \n",
      "Step 22 of Epoch: 58; Loss 1.426462 \n",
      "Step 23 of Epoch: 58; Loss 1.426381 \n",
      "Step 24 of Epoch: 58; Loss 1.426064 \n",
      "Step 25 of Epoch: 58; Loss 1.427035 \n",
      "Step 26 of Epoch: 58; Loss 1.427361 \n",
      "Step 27 of Epoch: 58; Loss 1.427575 \n",
      "Step 28 of Epoch: 58; Loss 1.428118 \n",
      "Step 29 of Epoch: 58; Loss 1.428845 \n",
      "Step 30 of Epoch: 58; Loss 1.428869 \n",
      "Step 31 of Epoch: 58; Loss 1.428981 \n",
      "Step 32 of Epoch: 58; Loss 1.429088 \n",
      "Step 33 of Epoch: 58; Loss 1.429063 \n",
      "Step 34 of Epoch: 58; Loss 1.429240 \n",
      "Step 35 of Epoch: 58; Loss 1.429595 \n",
      "Step 36 of Epoch: 58; Loss 1.429833 \n",
      "Step 37 of Epoch: 58; Loss 1.430147 \n",
      "Step 38 of Epoch: 58; Loss 1.430062 \n",
      "Step 39 of Epoch: 58; Loss 1.429679 \n",
      "Final Result for Epoch 58: Loss 1.430010; Val Acc 0.318677; Train Acc 0.432658\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 59; Loss 1.410491 \n",
      "Step 2 of Epoch: 59; Loss 1.413287 \n",
      "Step 3 of Epoch: 59; Loss 1.408810 \n",
      "Step 4 of Epoch: 59; Loss 1.410040 \n",
      "Step 5 of Epoch: 59; Loss 1.413978 \n",
      "Step 6 of Epoch: 59; Loss 1.416224 \n",
      "Step 7 of Epoch: 59; Loss 1.416401 \n",
      "Step 8 of Epoch: 59; Loss 1.417164 \n",
      "Step 9 of Epoch: 59; Loss 1.417605 \n",
      "Step 10 of Epoch: 59; Loss 1.417368 \n",
      "Step 11 of Epoch: 59; Loss 1.417653 \n",
      "Step 12 of Epoch: 59; Loss 1.417372 \n",
      "Step 13 of Epoch: 59; Loss 1.418946 \n",
      "Step 14 of Epoch: 59; Loss 1.419306 \n",
      "Step 15 of Epoch: 59; Loss 1.418996 \n",
      "Step 16 of Epoch: 59; Loss 1.420006 \n",
      "Step 17 of Epoch: 59; Loss 1.419056 \n",
      "Step 18 of Epoch: 59; Loss 1.419695 \n",
      "Step 19 of Epoch: 59; Loss 1.420990 \n",
      "Step 20 of Epoch: 59; Loss 1.422090 \n",
      "Step 21 of Epoch: 59; Loss 1.423061 \n",
      "Step 22 of Epoch: 59; Loss 1.423749 \n",
      "Step 23 of Epoch: 59; Loss 1.423829 \n",
      "Step 24 of Epoch: 59; Loss 1.424709 \n",
      "Step 25 of Epoch: 59; Loss 1.424717 \n",
      "Step 26 of Epoch: 59; Loss 1.425658 \n",
      "Step 27 of Epoch: 59; Loss 1.425892 \n",
      "Step 28 of Epoch: 59; Loss 1.425234 \n",
      "Step 29 of Epoch: 59; Loss 1.425239 \n",
      "Step 30 of Epoch: 59; Loss 1.425468 \n",
      "Step 31 of Epoch: 59; Loss 1.425678 \n",
      "Step 32 of Epoch: 59; Loss 1.425938 \n",
      "Step 33 of Epoch: 59; Loss 1.426279 \n",
      "Step 34 of Epoch: 59; Loss 1.427306 \n",
      "Step 35 of Epoch: 59; Loss 1.426963 \n",
      "Step 36 of Epoch: 59; Loss 1.427452 \n",
      "Step 37 of Epoch: 59; Loss 1.428367 \n",
      "Step 38 of Epoch: 59; Loss 1.428879 \n",
      "Step 39 of Epoch: 59; Loss 1.429196 \n",
      "Final Result for Epoch 59: Loss 1.429711; Val Acc 0.320762; Train Acc 0.433854\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 60; Loss 1.415923 \n",
      "Step 2 of Epoch: 60; Loss 1.413097 \n",
      "Step 3 of Epoch: 60; Loss 1.409608 \n",
      "Step 4 of Epoch: 60; Loss 1.414860 \n",
      "Step 5 of Epoch: 60; Loss 1.416090 \n",
      "Step 6 of Epoch: 60; Loss 1.411952 \n",
      "Step 7 of Epoch: 60; Loss 1.412718 \n",
      "Step 8 of Epoch: 60; Loss 1.414608 \n",
      "Step 9 of Epoch: 60; Loss 1.415901 \n",
      "Step 10 of Epoch: 60; Loss 1.415471 \n",
      "Step 11 of Epoch: 60; Loss 1.418335 \n",
      "Step 12 of Epoch: 60; Loss 1.417894 \n",
      "Step 13 of Epoch: 60; Loss 1.419206 \n",
      "Step 14 of Epoch: 60; Loss 1.419780 \n",
      "Step 15 of Epoch: 60; Loss 1.420504 \n",
      "Step 16 of Epoch: 60; Loss 1.420515 \n",
      "Step 17 of Epoch: 60; Loss 1.420667 \n",
      "Step 18 of Epoch: 60; Loss 1.420509 \n",
      "Step 19 of Epoch: 60; Loss 1.421360 \n",
      "Step 20 of Epoch: 60; Loss 1.422018 \n",
      "Step 21 of Epoch: 60; Loss 1.422897 \n",
      "Step 22 of Epoch: 60; Loss 1.423006 \n",
      "Step 23 of Epoch: 60; Loss 1.423520 \n",
      "Step 24 of Epoch: 60; Loss 1.424242 \n",
      "Step 25 of Epoch: 60; Loss 1.424417 \n",
      "Step 26 of Epoch: 60; Loss 1.424647 \n",
      "Step 27 of Epoch: 60; Loss 1.424708 \n",
      "Step 28 of Epoch: 60; Loss 1.424417 \n",
      "Step 29 of Epoch: 60; Loss 1.425005 \n",
      "Step 30 of Epoch: 60; Loss 1.425705 \n",
      "Step 31 of Epoch: 60; Loss 1.426007 \n",
      "Step 32 of Epoch: 60; Loss 1.426463 \n",
      "Step 33 of Epoch: 60; Loss 1.426342 \n",
      "Step 34 of Epoch: 60; Loss 1.426359 \n",
      "Step 35 of Epoch: 60; Loss 1.427131 \n",
      "Step 36 of Epoch: 60; Loss 1.427492 \n",
      "Step 37 of Epoch: 60; Loss 1.427653 \n",
      "Step 38 of Epoch: 60; Loss 1.427850 \n",
      "Step 39 of Epoch: 60; Loss 1.427875 \n",
      "Final Result for Epoch 60: Loss 1.428187; Val Acc 0.316593; Train Acc 0.433163\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 61; Loss 1.416077 \n",
      "Step 2 of Epoch: 61; Loss 1.420528 \n",
      "Step 3 of Epoch: 61; Loss 1.415088 \n",
      "Step 4 of Epoch: 61; Loss 1.416904 \n",
      "Step 5 of Epoch: 61; Loss 1.416122 \n",
      "Step 6 of Epoch: 61; Loss 1.416140 \n",
      "Step 7 of Epoch: 61; Loss 1.417798 \n",
      "Step 8 of Epoch: 61; Loss 1.420475 \n",
      "Step 9 of Epoch: 61; Loss 1.420550 \n",
      "Step 10 of Epoch: 61; Loss 1.420515 \n",
      "Step 11 of Epoch: 61; Loss 1.421438 \n",
      "Step 12 of Epoch: 61; Loss 1.422525 \n",
      "Step 13 of Epoch: 61; Loss 1.423148 \n",
      "Step 14 of Epoch: 61; Loss 1.424170 \n",
      "Step 15 of Epoch: 61; Loss 1.423975 \n",
      "Step 16 of Epoch: 61; Loss 1.423081 \n",
      "Step 17 of Epoch: 61; Loss 1.423399 \n",
      "Step 18 of Epoch: 61; Loss 1.423165 \n",
      "Step 19 of Epoch: 61; Loss 1.423840 \n",
      "Step 20 of Epoch: 61; Loss 1.424711 \n",
      "Step 21 of Epoch: 61; Loss 1.424194 \n",
      "Step 22 of Epoch: 61; Loss 1.423473 \n",
      "Step 23 of Epoch: 61; Loss 1.423181 \n",
      "Step 24 of Epoch: 61; Loss 1.423779 \n",
      "Step 25 of Epoch: 61; Loss 1.424974 \n",
      "Step 26 of Epoch: 61; Loss 1.425544 \n",
      "Step 27 of Epoch: 61; Loss 1.425896 \n",
      "Step 28 of Epoch: 61; Loss 1.425531 \n",
      "Step 29 of Epoch: 61; Loss 1.426373 \n",
      "Step 30 of Epoch: 61; Loss 1.427421 \n",
      "Step 31 of Epoch: 61; Loss 1.427355 \n",
      "Step 32 of Epoch: 61; Loss 1.427565 \n",
      "Step 33 of Epoch: 61; Loss 1.427529 \n",
      "Step 34 of Epoch: 61; Loss 1.427676 \n",
      "Step 35 of Epoch: 61; Loss 1.427720 \n",
      "Step 36 of Epoch: 61; Loss 1.427511 \n",
      "Step 37 of Epoch: 61; Loss 1.427471 \n",
      "Step 38 of Epoch: 61; Loss 1.427895 \n",
      "Step 39 of Epoch: 61; Loss 1.428522 \n",
      "Final Result for Epoch 61: Loss 1.428432; Val Acc 0.318878; Train Acc 0.431021\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 62; Loss 1.409795 \n",
      "Step 2 of Epoch: 62; Loss 1.413004 \n",
      "Step 3 of Epoch: 62; Loss 1.410597 \n",
      "Step 4 of Epoch: 62; Loss 1.415179 \n",
      "Step 5 of Epoch: 62; Loss 1.412957 \n",
      "Step 6 of Epoch: 62; Loss 1.415587 \n",
      "Step 7 of Epoch: 62; Loss 1.418055 \n",
      "Step 8 of Epoch: 62; Loss 1.417502 \n",
      "Step 9 of Epoch: 62; Loss 1.418227 \n",
      "Step 10 of Epoch: 62; Loss 1.418189 \n",
      "Step 11 of Epoch: 62; Loss 1.418749 \n",
      "Step 12 of Epoch: 62; Loss 1.419860 \n",
      "Step 13 of Epoch: 62; Loss 1.419733 \n",
      "Step 14 of Epoch: 62; Loss 1.420660 \n",
      "Step 15 of Epoch: 62; Loss 1.421211 \n",
      "Step 16 of Epoch: 62; Loss 1.421795 \n",
      "Step 17 of Epoch: 62; Loss 1.421271 \n",
      "Step 18 of Epoch: 62; Loss 1.421875 \n",
      "Step 19 of Epoch: 62; Loss 1.421093 \n",
      "Step 20 of Epoch: 62; Loss 1.421547 \n",
      "Step 21 of Epoch: 62; Loss 1.420403 \n",
      "Step 22 of Epoch: 62; Loss 1.420853 \n",
      "Step 23 of Epoch: 62; Loss 1.420921 \n",
      "Step 24 of Epoch: 62; Loss 1.421527 \n",
      "Step 25 of Epoch: 62; Loss 1.422570 \n",
      "Step 26 of Epoch: 62; Loss 1.422812 \n",
      "Step 27 of Epoch: 62; Loss 1.423614 \n",
      "Step 28 of Epoch: 62; Loss 1.423763 \n",
      "Step 29 of Epoch: 62; Loss 1.424112 \n",
      "Step 30 of Epoch: 62; Loss 1.424057 \n",
      "Step 31 of Epoch: 62; Loss 1.424136 \n",
      "Step 32 of Epoch: 62; Loss 1.424565 \n",
      "Step 33 of Epoch: 62; Loss 1.424380 \n",
      "Step 34 of Epoch: 62; Loss 1.424520 \n",
      "Step 35 of Epoch: 62; Loss 1.424470 \n",
      "Step 36 of Epoch: 62; Loss 1.425305 \n",
      "Step 37 of Epoch: 62; Loss 1.425612 \n",
      "Step 38 of Epoch: 62; Loss 1.425930 \n",
      "Step 39 of Epoch: 62; Loss 1.426832 \n",
      "Final Result for Epoch 62: Loss 1.427244; Val Acc 0.319479; Train Acc 0.432528\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 63; Loss 1.417696 \n",
      "Step 2 of Epoch: 63; Loss 1.415217 \n",
      "Step 3 of Epoch: 63; Loss 1.410094 \n",
      "Step 4 of Epoch: 63; Loss 1.406670 \n",
      "Step 5 of Epoch: 63; Loss 1.409187 \n",
      "Step 6 of Epoch: 63; Loss 1.410288 \n",
      "Step 7 of Epoch: 63; Loss 1.411579 \n",
      "Step 8 of Epoch: 63; Loss 1.412655 \n",
      "Step 9 of Epoch: 63; Loss 1.413633 \n",
      "Step 10 of Epoch: 63; Loss 1.412730 \n",
      "Step 11 of Epoch: 63; Loss 1.415073 \n",
      "Step 12 of Epoch: 63; Loss 1.414908 \n",
      "Step 13 of Epoch: 63; Loss 1.415338 \n",
      "Step 14 of Epoch: 63; Loss 1.416165 \n",
      "Step 15 of Epoch: 63; Loss 1.416755 \n",
      "Step 16 of Epoch: 63; Loss 1.416560 \n",
      "Step 17 of Epoch: 63; Loss 1.417349 \n",
      "Step 18 of Epoch: 63; Loss 1.417740 \n",
      "Step 19 of Epoch: 63; Loss 1.418221 \n",
      "Step 20 of Epoch: 63; Loss 1.418743 \n",
      "Step 21 of Epoch: 63; Loss 1.418939 \n",
      "Step 22 of Epoch: 63; Loss 1.419355 \n",
      "Step 23 of Epoch: 63; Loss 1.420861 \n",
      "Step 24 of Epoch: 63; Loss 1.421661 \n",
      "Step 25 of Epoch: 63; Loss 1.421823 \n",
      "Step 26 of Epoch: 63; Loss 1.422444 \n",
      "Step 27 of Epoch: 63; Loss 1.422663 \n",
      "Step 28 of Epoch: 63; Loss 1.422675 \n",
      "Step 29 of Epoch: 63; Loss 1.423674 \n",
      "Step 30 of Epoch: 63; Loss 1.423712 \n",
      "Step 31 of Epoch: 63; Loss 1.424275 \n",
      "Step 32 of Epoch: 63; Loss 1.424669 \n",
      "Step 33 of Epoch: 63; Loss 1.424986 \n",
      "Step 34 of Epoch: 63; Loss 1.425522 \n",
      "Step 35 of Epoch: 63; Loss 1.426212 \n",
      "Step 36 of Epoch: 63; Loss 1.426143 \n",
      "Step 37 of Epoch: 63; Loss 1.426422 \n",
      "Step 38 of Epoch: 63; Loss 1.426395 \n",
      "Step 39 of Epoch: 63; Loss 1.426338 \n",
      "Final Result for Epoch 63: Loss 1.426540; Val Acc 0.319920; Train Acc 0.433764\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 64; Loss 1.383462 \n",
      "Step 2 of Epoch: 64; Loss 1.403145 \n",
      "Step 3 of Epoch: 64; Loss 1.398835 \n",
      "Step 4 of Epoch: 64; Loss 1.400679 \n",
      "Step 5 of Epoch: 64; Loss 1.403664 \n",
      "Step 6 of Epoch: 64; Loss 1.406823 \n",
      "Step 7 of Epoch: 64; Loss 1.408113 \n",
      "Step 8 of Epoch: 64; Loss 1.408537 \n",
      "Step 9 of Epoch: 64; Loss 1.409015 \n",
      "Step 10 of Epoch: 64; Loss 1.410373 \n",
      "Step 11 of Epoch: 64; Loss 1.412130 \n",
      "Step 12 of Epoch: 64; Loss 1.413050 \n",
      "Step 13 of Epoch: 64; Loss 1.414508 \n",
      "Step 14 of Epoch: 64; Loss 1.414373 \n",
      "Step 15 of Epoch: 64; Loss 1.415195 \n",
      "Step 16 of Epoch: 64; Loss 1.414677 \n",
      "Step 17 of Epoch: 64; Loss 1.414435 \n",
      "Step 18 of Epoch: 64; Loss 1.414716 \n",
      "Step 19 of Epoch: 64; Loss 1.415233 \n",
      "Step 20 of Epoch: 64; Loss 1.415743 \n",
      "Step 21 of Epoch: 64; Loss 1.416655 \n",
      "Step 22 of Epoch: 64; Loss 1.416664 \n",
      "Step 23 of Epoch: 64; Loss 1.417525 \n",
      "Step 24 of Epoch: 64; Loss 1.417751 \n",
      "Step 25 of Epoch: 64; Loss 1.418179 \n",
      "Step 26 of Epoch: 64; Loss 1.418263 \n",
      "Step 27 of Epoch: 64; Loss 1.419001 \n",
      "Step 28 of Epoch: 64; Loss 1.419611 \n",
      "Step 29 of Epoch: 64; Loss 1.420889 \n",
      "Step 30 of Epoch: 64; Loss 1.421122 \n",
      "Step 31 of Epoch: 64; Loss 1.422157 \n",
      "Step 32 of Epoch: 64; Loss 1.422159 \n",
      "Step 33 of Epoch: 64; Loss 1.422711 \n",
      "Step 34 of Epoch: 64; Loss 1.423383 \n",
      "Step 35 of Epoch: 64; Loss 1.423807 \n",
      "Step 36 of Epoch: 64; Loss 1.424336 \n",
      "Step 37 of Epoch: 64; Loss 1.425307 \n",
      "Step 38 of Epoch: 64; Loss 1.425390 \n",
      "Step 39 of Epoch: 64; Loss 1.425616 \n",
      "Final Result for Epoch 64: Loss 1.425672; Val Acc 0.317876; Train Acc 0.434489\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 65; Loss 1.402677 \n",
      "Step 2 of Epoch: 65; Loss 1.392862 \n",
      "Step 3 of Epoch: 65; Loss 1.402159 \n",
      "Step 4 of Epoch: 65; Loss 1.401673 \n",
      "Step 5 of Epoch: 65; Loss 1.405930 \n",
      "Step 6 of Epoch: 65; Loss 1.408226 \n",
      "Step 7 of Epoch: 65; Loss 1.408090 \n",
      "Step 8 of Epoch: 65; Loss 1.408586 \n",
      "Step 9 of Epoch: 65; Loss 1.413149 \n",
      "Step 10 of Epoch: 65; Loss 1.413035 \n",
      "Step 11 of Epoch: 65; Loss 1.413766 \n",
      "Step 12 of Epoch: 65; Loss 1.415175 \n",
      "Step 13 of Epoch: 65; Loss 1.415170 \n",
      "Step 14 of Epoch: 65; Loss 1.415901 \n",
      "Step 15 of Epoch: 65; Loss 1.417151 \n",
      "Step 16 of Epoch: 65; Loss 1.417581 \n",
      "Step 17 of Epoch: 65; Loss 1.417954 \n",
      "Step 18 of Epoch: 65; Loss 1.418679 \n",
      "Step 19 of Epoch: 65; Loss 1.419373 \n",
      "Step 20 of Epoch: 65; Loss 1.420583 \n",
      "Step 21 of Epoch: 65; Loss 1.420175 \n",
      "Step 22 of Epoch: 65; Loss 1.419889 \n",
      "Step 23 of Epoch: 65; Loss 1.420153 \n",
      "Step 24 of Epoch: 65; Loss 1.420713 \n",
      "Step 25 of Epoch: 65; Loss 1.420540 \n",
      "Step 26 of Epoch: 65; Loss 1.420832 \n",
      "Step 27 of Epoch: 65; Loss 1.420877 \n",
      "Step 28 of Epoch: 65; Loss 1.421119 \n",
      "Step 29 of Epoch: 65; Loss 1.421594 \n",
      "Step 30 of Epoch: 65; Loss 1.421897 \n",
      "Step 31 of Epoch: 65; Loss 1.422394 \n",
      "Step 32 of Epoch: 65; Loss 1.422682 \n",
      "Step 33 of Epoch: 65; Loss 1.423353 \n",
      "Step 34 of Epoch: 65; Loss 1.423834 \n",
      "Step 35 of Epoch: 65; Loss 1.424218 \n",
      "Step 36 of Epoch: 65; Loss 1.424522 \n",
      "Step 37 of Epoch: 65; Loss 1.424864 \n",
      "Step 38 of Epoch: 65; Loss 1.425110 \n",
      "Step 39 of Epoch: 65; Loss 1.425342 \n",
      "Final Result for Epoch 65: Loss 1.425434; Val Acc 0.319439; Train Acc 0.435455\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 66; Loss 1.380526 \n",
      "Step 2 of Epoch: 66; Loss 1.402166 \n",
      "Step 3 of Epoch: 66; Loss 1.409979 \n",
      "Step 4 of Epoch: 66; Loss 1.409813 \n",
      "Step 5 of Epoch: 66; Loss 1.411655 \n",
      "Step 6 of Epoch: 66; Loss 1.411000 \n",
      "Step 7 of Epoch: 66; Loss 1.412205 \n",
      "Step 8 of Epoch: 66; Loss 1.412025 \n",
      "Step 9 of Epoch: 66; Loss 1.413088 \n",
      "Step 10 of Epoch: 66; Loss 1.414343 \n",
      "Step 11 of Epoch: 66; Loss 1.415481 \n",
      "Step 12 of Epoch: 66; Loss 1.416900 \n",
      "Step 13 of Epoch: 66; Loss 1.416420 \n",
      "Step 14 of Epoch: 66; Loss 1.416503 \n",
      "Step 15 of Epoch: 66; Loss 1.416244 \n",
      "Step 16 of Epoch: 66; Loss 1.416863 \n",
      "Step 17 of Epoch: 66; Loss 1.417769 \n",
      "Step 18 of Epoch: 66; Loss 1.418209 \n",
      "Step 19 of Epoch: 66; Loss 1.418749 \n",
      "Step 20 of Epoch: 66; Loss 1.419760 \n",
      "Step 21 of Epoch: 66; Loss 1.419303 \n",
      "Step 22 of Epoch: 66; Loss 1.419307 \n",
      "Step 23 of Epoch: 66; Loss 1.419967 \n",
      "Step 24 of Epoch: 66; Loss 1.420399 \n",
      "Step 25 of Epoch: 66; Loss 1.419769 \n",
      "Step 26 of Epoch: 66; Loss 1.420333 \n",
      "Step 27 of Epoch: 66; Loss 1.420864 \n",
      "Step 28 of Epoch: 66; Loss 1.421547 \n",
      "Step 29 of Epoch: 66; Loss 1.421966 \n",
      "Step 30 of Epoch: 66; Loss 1.421551 \n",
      "Step 31 of Epoch: 66; Loss 1.421921 \n",
      "Step 32 of Epoch: 66; Loss 1.422320 \n",
      "Step 33 of Epoch: 66; Loss 1.422207 \n",
      "Step 34 of Epoch: 66; Loss 1.422414 \n",
      "Step 35 of Epoch: 66; Loss 1.423070 \n",
      "Step 36 of Epoch: 66; Loss 1.423225 \n",
      "Step 37 of Epoch: 66; Loss 1.423377 \n",
      "Step 38 of Epoch: 66; Loss 1.423793 \n",
      "Step 39 of Epoch: 66; Loss 1.423719 \n",
      "Final Result for Epoch 66: Loss 1.423548; Val Acc 0.319599; Train Acc 0.435330\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 67; Loss 1.396341 \n",
      "Step 2 of Epoch: 67; Loss 1.406528 \n",
      "Step 3 of Epoch: 67; Loss 1.411739 \n",
      "Step 4 of Epoch: 67; Loss 1.415174 \n",
      "Step 5 of Epoch: 67; Loss 1.411693 \n",
      "Step 6 of Epoch: 67; Loss 1.411560 \n",
      "Step 7 of Epoch: 67; Loss 1.412387 \n",
      "Step 8 of Epoch: 67; Loss 1.413055 \n",
      "Step 9 of Epoch: 67; Loss 1.414372 \n",
      "Step 10 of Epoch: 67; Loss 1.415470 \n",
      "Step 11 of Epoch: 67; Loss 1.415780 \n",
      "Step 12 of Epoch: 67; Loss 1.415918 \n",
      "Step 13 of Epoch: 67; Loss 1.415557 \n",
      "Step 14 of Epoch: 67; Loss 1.416349 \n",
      "Step 15 of Epoch: 67; Loss 1.416890 \n",
      "Step 16 of Epoch: 67; Loss 1.417266 \n",
      "Step 17 of Epoch: 67; Loss 1.418196 \n",
      "Step 18 of Epoch: 67; Loss 1.419318 \n",
      "Step 19 of Epoch: 67; Loss 1.419094 \n",
      "Step 20 of Epoch: 67; Loss 1.419255 \n",
      "Step 21 of Epoch: 67; Loss 1.418964 \n",
      "Step 22 of Epoch: 67; Loss 1.419749 \n",
      "Step 23 of Epoch: 67; Loss 1.420211 \n",
      "Step 24 of Epoch: 67; Loss 1.420827 \n",
      "Step 25 of Epoch: 67; Loss 1.420451 \n",
      "Step 26 of Epoch: 67; Loss 1.420822 \n",
      "Step 27 of Epoch: 67; Loss 1.421312 \n",
      "Step 28 of Epoch: 67; Loss 1.421484 \n",
      "Step 29 of Epoch: 67; Loss 1.421994 \n",
      "Step 30 of Epoch: 67; Loss 1.422129 \n",
      "Step 31 of Epoch: 67; Loss 1.422133 \n",
      "Step 32 of Epoch: 67; Loss 1.422513 \n",
      "Step 33 of Epoch: 67; Loss 1.423247 \n",
      "Step 34 of Epoch: 67; Loss 1.423624 \n",
      "Step 35 of Epoch: 67; Loss 1.423638 \n",
      "Step 36 of Epoch: 67; Loss 1.423518 \n",
      "Step 37 of Epoch: 67; Loss 1.423720 \n",
      "Step 38 of Epoch: 67; Loss 1.423382 \n",
      "Step 39 of Epoch: 67; Loss 1.423202 \n",
      "Final Result for Epoch 67: Loss 1.423238; Val Acc 0.317315; Train Acc 0.434284\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 68; Loss 1.393145 \n",
      "Step 2 of Epoch: 68; Loss 1.400322 \n",
      "Step 3 of Epoch: 68; Loss 1.408334 \n",
      "Step 4 of Epoch: 68; Loss 1.411143 \n",
      "Step 5 of Epoch: 68; Loss 1.411243 \n",
      "Step 6 of Epoch: 68; Loss 1.412643 \n",
      "Step 7 of Epoch: 68; Loss 1.409857 \n",
      "Step 8 of Epoch: 68; Loss 1.411381 \n",
      "Step 9 of Epoch: 68; Loss 1.410167 \n",
      "Step 10 of Epoch: 68; Loss 1.411202 \n",
      "Step 11 of Epoch: 68; Loss 1.413255 \n",
      "Step 12 of Epoch: 68; Loss 1.414180 \n",
      "Step 13 of Epoch: 68; Loss 1.415062 \n",
      "Step 14 of Epoch: 68; Loss 1.414688 \n",
      "Step 15 of Epoch: 68; Loss 1.415055 \n",
      "Step 16 of Epoch: 68; Loss 1.415391 \n",
      "Step 17 of Epoch: 68; Loss 1.416618 \n",
      "Step 18 of Epoch: 68; Loss 1.416250 \n",
      "Step 19 of Epoch: 68; Loss 1.416159 \n",
      "Step 20 of Epoch: 68; Loss 1.416355 \n",
      "Step 21 of Epoch: 68; Loss 1.417037 \n",
      "Step 22 of Epoch: 68; Loss 1.417419 \n",
      "Step 23 of Epoch: 68; Loss 1.417745 \n",
      "Step 24 of Epoch: 68; Loss 1.418616 \n",
      "Step 25 of Epoch: 68; Loss 1.419090 \n",
      "Step 26 of Epoch: 68; Loss 1.419183 \n",
      "Step 27 of Epoch: 68; Loss 1.419267 \n",
      "Step 28 of Epoch: 68; Loss 1.419475 \n",
      "Step 29 of Epoch: 68; Loss 1.419987 \n",
      "Step 30 of Epoch: 68; Loss 1.420305 \n",
      "Step 31 of Epoch: 68; Loss 1.420834 \n",
      "Step 32 of Epoch: 68; Loss 1.421238 \n",
      "Step 33 of Epoch: 68; Loss 1.422061 \n",
      "Step 34 of Epoch: 68; Loss 1.422165 \n",
      "Step 35 of Epoch: 68; Loss 1.422068 \n",
      "Step 36 of Epoch: 68; Loss 1.422402 \n",
      "Step 37 of Epoch: 68; Loss 1.422550 \n",
      "Step 38 of Epoch: 68; Loss 1.422296 \n",
      "Step 39 of Epoch: 68; Loss 1.422674 \n",
      "Final Result for Epoch 68: Loss 1.422592; Val Acc 0.320200; Train Acc 0.436431\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 69; Loss 1.406794 \n",
      "Step 2 of Epoch: 69; Loss 1.409616 \n",
      "Step 3 of Epoch: 69; Loss 1.405537 \n",
      "Step 4 of Epoch: 69; Loss 1.409135 \n",
      "Step 5 of Epoch: 69; Loss 1.406843 \n",
      "Step 6 of Epoch: 69; Loss 1.407072 \n",
      "Step 7 of Epoch: 69; Loss 1.407114 \n",
      "Step 8 of Epoch: 69; Loss 1.409435 \n",
      "Step 9 of Epoch: 69; Loss 1.409471 \n",
      "Step 10 of Epoch: 69; Loss 1.410877 \n",
      "Step 11 of Epoch: 69; Loss 1.412132 \n",
      "Step 12 of Epoch: 69; Loss 1.412050 \n",
      "Step 13 of Epoch: 69; Loss 1.412080 \n",
      "Step 14 of Epoch: 69; Loss 1.412848 \n",
      "Step 15 of Epoch: 69; Loss 1.412756 \n",
      "Step 16 of Epoch: 69; Loss 1.413976 \n",
      "Step 17 of Epoch: 69; Loss 1.414125 \n",
      "Step 18 of Epoch: 69; Loss 1.414372 \n",
      "Step 19 of Epoch: 69; Loss 1.415091 \n",
      "Step 20 of Epoch: 69; Loss 1.416433 \n",
      "Step 21 of Epoch: 69; Loss 1.417141 \n",
      "Step 22 of Epoch: 69; Loss 1.417643 \n",
      "Step 23 of Epoch: 69; Loss 1.419000 \n",
      "Step 24 of Epoch: 69; Loss 1.419336 \n",
      "Step 25 of Epoch: 69; Loss 1.419408 \n",
      "Step 26 of Epoch: 69; Loss 1.419817 \n",
      "Step 27 of Epoch: 69; Loss 1.420383 \n",
      "Step 28 of Epoch: 69; Loss 1.420845 \n",
      "Step 29 of Epoch: 69; Loss 1.420694 \n",
      "Step 30 of Epoch: 69; Loss 1.421239 \n",
      "Step 31 of Epoch: 69; Loss 1.421245 \n",
      "Step 32 of Epoch: 69; Loss 1.421700 \n",
      "Step 33 of Epoch: 69; Loss 1.422083 \n",
      "Step 34 of Epoch: 69; Loss 1.421616 \n",
      "Step 35 of Epoch: 69; Loss 1.422418 \n",
      "Step 36 of Epoch: 69; Loss 1.422082 \n",
      "Step 37 of Epoch: 69; Loss 1.422751 \n",
      "Step 38 of Epoch: 69; Loss 1.423147 \n",
      "Step 39 of Epoch: 69; Loss 1.423254 \n",
      "Final Result for Epoch 69: Loss 1.422999; Val Acc 0.317194; Train Acc 0.435561\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 70; Loss 1.392558 \n",
      "Step 2 of Epoch: 70; Loss 1.384878 \n",
      "Step 3 of Epoch: 70; Loss 1.387977 \n",
      "Step 4 of Epoch: 70; Loss 1.393852 \n",
      "Step 5 of Epoch: 70; Loss 1.397378 \n",
      "Step 6 of Epoch: 70; Loss 1.398372 \n",
      "Step 7 of Epoch: 70; Loss 1.399979 \n",
      "Step 8 of Epoch: 70; Loss 1.401029 \n",
      "Step 9 of Epoch: 70; Loss 1.402323 \n",
      "Step 10 of Epoch: 70; Loss 1.405036 \n",
      "Step 11 of Epoch: 70; Loss 1.405505 \n",
      "Step 12 of Epoch: 70; Loss 1.405164 \n",
      "Step 13 of Epoch: 70; Loss 1.404711 \n",
      "Step 14 of Epoch: 70; Loss 1.406145 \n",
      "Step 15 of Epoch: 70; Loss 1.407675 \n",
      "Step 16 of Epoch: 70; Loss 1.408982 \n",
      "Step 17 of Epoch: 70; Loss 1.409866 \n",
      "Step 18 of Epoch: 70; Loss 1.411122 \n",
      "Step 19 of Epoch: 70; Loss 1.411816 \n",
      "Step 20 of Epoch: 70; Loss 1.412965 \n",
      "Step 21 of Epoch: 70; Loss 1.414222 \n",
      "Step 22 of Epoch: 70; Loss 1.415216 \n",
      "Step 23 of Epoch: 70; Loss 1.415202 \n",
      "Step 24 of Epoch: 70; Loss 1.415779 \n",
      "Step 25 of Epoch: 70; Loss 1.416060 \n",
      "Step 26 of Epoch: 70; Loss 1.416842 \n",
      "Step 27 of Epoch: 70; Loss 1.416710 \n",
      "Step 28 of Epoch: 70; Loss 1.417019 \n",
      "Step 29 of Epoch: 70; Loss 1.417543 \n",
      "Step 30 of Epoch: 70; Loss 1.418097 \n",
      "Step 31 of Epoch: 70; Loss 1.417950 \n",
      "Step 32 of Epoch: 70; Loss 1.418226 \n",
      "Step 33 of Epoch: 70; Loss 1.418897 \n",
      "Step 34 of Epoch: 70; Loss 1.419572 \n",
      "Step 35 of Epoch: 70; Loss 1.419555 \n",
      "Step 36 of Epoch: 70; Loss 1.420096 \n",
      "Step 37 of Epoch: 70; Loss 1.420497 \n",
      "Step 38 of Epoch: 70; Loss 1.420640 \n",
      "Step 39 of Epoch: 70; Loss 1.421404 \n",
      "Final Result for Epoch 70: Loss 1.421761; Val Acc 0.317796; Train Acc 0.436622\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 71; Loss 1.414799 \n",
      "Step 2 of Epoch: 71; Loss 1.415149 \n",
      "Step 3 of Epoch: 71; Loss 1.412113 \n",
      "Step 4 of Epoch: 71; Loss 1.408188 \n",
      "Step 5 of Epoch: 71; Loss 1.410064 \n",
      "Step 6 of Epoch: 71; Loss 1.409544 \n",
      "Step 7 of Epoch: 71; Loss 1.410988 \n",
      "Step 8 of Epoch: 71; Loss 1.410659 \n",
      "Step 9 of Epoch: 71; Loss 1.411248 \n",
      "Step 10 of Epoch: 71; Loss 1.410044 \n",
      "Step 11 of Epoch: 71; Loss 1.409351 \n",
      "Step 12 of Epoch: 71; Loss 1.410911 \n",
      "Step 13 of Epoch: 71; Loss 1.411566 \n",
      "Step 14 of Epoch: 71; Loss 1.412545 \n",
      "Step 15 of Epoch: 71; Loss 1.411533 \n",
      "Step 16 of Epoch: 71; Loss 1.412324 \n",
      "Step 17 of Epoch: 71; Loss 1.412614 \n",
      "Step 18 of Epoch: 71; Loss 1.413247 \n",
      "Step 19 of Epoch: 71; Loss 1.413863 \n",
      "Step 20 of Epoch: 71; Loss 1.413535 \n",
      "Step 21 of Epoch: 71; Loss 1.413536 \n",
      "Step 22 of Epoch: 71; Loss 1.414913 \n",
      "Step 23 of Epoch: 71; Loss 1.415231 \n",
      "Step 24 of Epoch: 71; Loss 1.415098 \n",
      "Step 25 of Epoch: 71; Loss 1.415103 \n",
      "Step 26 of Epoch: 71; Loss 1.415893 \n",
      "Step 27 of Epoch: 71; Loss 1.416794 \n",
      "Step 28 of Epoch: 71; Loss 1.416514 \n",
      "Step 29 of Epoch: 71; Loss 1.417212 \n",
      "Step 30 of Epoch: 71; Loss 1.417537 \n",
      "Step 31 of Epoch: 71; Loss 1.417636 \n",
      "Step 32 of Epoch: 71; Loss 1.418148 \n",
      "Step 33 of Epoch: 71; Loss 1.418703 \n",
      "Step 34 of Epoch: 71; Loss 1.419100 \n",
      "Step 35 of Epoch: 71; Loss 1.419353 \n",
      "Step 36 of Epoch: 71; Loss 1.419679 \n",
      "Step 37 of Epoch: 71; Loss 1.420205 \n",
      "Step 38 of Epoch: 71; Loss 1.420597 \n",
      "Step 39 of Epoch: 71; Loss 1.420806 \n",
      "Final Result for Epoch 71: Loss 1.421219; Val Acc 0.319038; Train Acc 0.436612\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 72; Loss 1.389220 \n",
      "Step 2 of Epoch: 72; Loss 1.397363 \n",
      "Step 3 of Epoch: 72; Loss 1.392528 \n",
      "Step 4 of Epoch: 72; Loss 1.396314 \n",
      "Step 5 of Epoch: 72; Loss 1.401314 \n",
      "Step 6 of Epoch: 72; Loss 1.399920 \n",
      "Step 7 of Epoch: 72; Loss 1.400173 \n",
      "Step 8 of Epoch: 72; Loss 1.401192 \n",
      "Step 9 of Epoch: 72; Loss 1.401791 \n",
      "Step 10 of Epoch: 72; Loss 1.401811 \n",
      "Step 11 of Epoch: 72; Loss 1.404432 \n",
      "Step 12 of Epoch: 72; Loss 1.407067 \n",
      "Step 13 of Epoch: 72; Loss 1.409137 \n",
      "Step 14 of Epoch: 72; Loss 1.409323 \n",
      "Step 15 of Epoch: 72; Loss 1.409668 \n",
      "Step 16 of Epoch: 72; Loss 1.410723 \n",
      "Step 17 of Epoch: 72; Loss 1.412403 \n",
      "Step 18 of Epoch: 72; Loss 1.412989 \n",
      "Step 19 of Epoch: 72; Loss 1.413685 \n",
      "Step 20 of Epoch: 72; Loss 1.413639 \n",
      "Step 21 of Epoch: 72; Loss 1.413769 \n",
      "Step 22 of Epoch: 72; Loss 1.414778 \n",
      "Step 23 of Epoch: 72; Loss 1.415485 \n",
      "Step 24 of Epoch: 72; Loss 1.416019 \n",
      "Step 25 of Epoch: 72; Loss 1.416469 \n",
      "Step 26 of Epoch: 72; Loss 1.417253 \n",
      "Step 27 of Epoch: 72; Loss 1.417291 \n",
      "Step 28 of Epoch: 72; Loss 1.417810 \n",
      "Step 29 of Epoch: 72; Loss 1.417999 \n",
      "Step 30 of Epoch: 72; Loss 1.418041 \n",
      "Step 31 of Epoch: 72; Loss 1.418057 \n",
      "Step 32 of Epoch: 72; Loss 1.418760 \n",
      "Step 33 of Epoch: 72; Loss 1.418940 \n",
      "Step 34 of Epoch: 72; Loss 1.419478 \n",
      "Step 35 of Epoch: 72; Loss 1.420073 \n",
      "Step 36 of Epoch: 72; Loss 1.420809 \n",
      "Step 37 of Epoch: 72; Loss 1.421858 \n",
      "Step 38 of Epoch: 72; Loss 1.421974 \n",
      "Step 39 of Epoch: 72; Loss 1.422240 \n",
      "Final Result for Epoch 72: Loss 1.422129; Val Acc 0.317916; Train Acc 0.434605\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 73; Loss 1.403999 \n",
      "Step 2 of Epoch: 73; Loss 1.403412 \n",
      "Step 3 of Epoch: 73; Loss 1.404335 \n",
      "Step 4 of Epoch: 73; Loss 1.409965 \n",
      "Step 5 of Epoch: 73; Loss 1.410960 \n",
      "Step 6 of Epoch: 73; Loss 1.409112 \n",
      "Step 7 of Epoch: 73; Loss 1.409960 \n",
      "Step 8 of Epoch: 73; Loss 1.410572 \n",
      "Step 9 of Epoch: 73; Loss 1.411424 \n",
      "Step 10 of Epoch: 73; Loss 1.413763 \n",
      "Step 11 of Epoch: 73; Loss 1.414294 \n",
      "Step 12 of Epoch: 73; Loss 1.413542 \n",
      "Step 13 of Epoch: 73; Loss 1.413826 \n",
      "Step 14 of Epoch: 73; Loss 1.414114 \n",
      "Step 15 of Epoch: 73; Loss 1.415368 \n",
      "Step 16 of Epoch: 73; Loss 1.414699 \n",
      "Step 17 of Epoch: 73; Loss 1.414451 \n",
      "Step 18 of Epoch: 73; Loss 1.413873 \n",
      "Step 19 of Epoch: 73; Loss 1.413938 \n",
      "Step 20 of Epoch: 73; Loss 1.414349 \n",
      "Step 21 of Epoch: 73; Loss 1.415290 \n",
      "Step 22 of Epoch: 73; Loss 1.416126 \n",
      "Step 23 of Epoch: 73; Loss 1.416789 \n",
      "Step 24 of Epoch: 73; Loss 1.416548 \n",
      "Step 25 of Epoch: 73; Loss 1.416812 \n",
      "Step 26 of Epoch: 73; Loss 1.416651 \n",
      "Step 27 of Epoch: 73; Loss 1.416457 \n",
      "Step 28 of Epoch: 73; Loss 1.417219 \n",
      "Step 29 of Epoch: 73; Loss 1.417885 \n",
      "Step 30 of Epoch: 73; Loss 1.417791 \n",
      "Step 31 of Epoch: 73; Loss 1.418211 \n",
      "Step 32 of Epoch: 73; Loss 1.418493 \n",
      "Step 33 of Epoch: 73; Loss 1.418314 \n",
      "Step 34 of Epoch: 73; Loss 1.418279 \n",
      "Step 35 of Epoch: 73; Loss 1.418428 \n",
      "Step 36 of Epoch: 73; Loss 1.419292 \n",
      "Step 37 of Epoch: 73; Loss 1.419526 \n",
      "Step 38 of Epoch: 73; Loss 1.420068 \n",
      "Step 39 of Epoch: 73; Loss 1.420354 \n",
      "Final Result for Epoch 73: Loss 1.420125; Val Acc 0.320401; Train Acc 0.435190\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 74; Loss 1.402059 \n",
      "Step 2 of Epoch: 74; Loss 1.409464 \n",
      "Step 3 of Epoch: 74; Loss 1.407367 \n",
      "Step 4 of Epoch: 74; Loss 1.410542 \n",
      "Step 5 of Epoch: 74; Loss 1.410670 \n",
      "Step 6 of Epoch: 74; Loss 1.412098 \n",
      "Step 7 of Epoch: 74; Loss 1.411864 \n",
      "Step 8 of Epoch: 74; Loss 1.411952 \n",
      "Step 9 of Epoch: 74; Loss 1.410896 \n",
      "Step 10 of Epoch: 74; Loss 1.411247 \n",
      "Step 11 of Epoch: 74; Loss 1.411317 \n",
      "Step 12 of Epoch: 74; Loss 1.411394 \n",
      "Step 13 of Epoch: 74; Loss 1.412332 \n",
      "Step 14 of Epoch: 74; Loss 1.412435 \n",
      "Step 15 of Epoch: 74; Loss 1.413789 \n",
      "Step 16 of Epoch: 74; Loss 1.415126 \n",
      "Step 17 of Epoch: 74; Loss 1.415120 \n",
      "Step 18 of Epoch: 74; Loss 1.413835 \n",
      "Step 19 of Epoch: 74; Loss 1.413959 \n",
      "Step 20 of Epoch: 74; Loss 1.414113 \n",
      "Step 21 of Epoch: 74; Loss 1.413620 \n",
      "Step 22 of Epoch: 74; Loss 1.415095 \n",
      "Step 23 of Epoch: 74; Loss 1.415386 \n",
      "Step 24 of Epoch: 74; Loss 1.415622 \n",
      "Step 25 of Epoch: 74; Loss 1.415371 \n",
      "Step 26 of Epoch: 74; Loss 1.415804 \n",
      "Step 27 of Epoch: 74; Loss 1.415059 \n",
      "Step 28 of Epoch: 74; Loss 1.415740 \n",
      "Step 29 of Epoch: 74; Loss 1.416660 \n",
      "Step 30 of Epoch: 74; Loss 1.417347 \n",
      "Step 31 of Epoch: 74; Loss 1.417152 \n",
      "Step 32 of Epoch: 74; Loss 1.417874 \n",
      "Step 33 of Epoch: 74; Loss 1.418388 \n",
      "Step 34 of Epoch: 74; Loss 1.418809 \n",
      "Step 35 of Epoch: 74; Loss 1.419045 \n",
      "Step 36 of Epoch: 74; Loss 1.419410 \n",
      "Step 37 of Epoch: 74; Loss 1.419725 \n",
      "Step 38 of Epoch: 74; Loss 1.419965 \n",
      "Step 39 of Epoch: 74; Loss 1.420273 \n",
      "Final Result for Epoch 74: Loss 1.420594; Val Acc 0.318076; Train Acc 0.436707\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 75; Loss 1.410670 \n",
      "Step 2 of Epoch: 75; Loss 1.404716 \n",
      "Step 3 of Epoch: 75; Loss 1.402967 \n",
      "Step 4 of Epoch: 75; Loss 1.398137 \n",
      "Step 5 of Epoch: 75; Loss 1.399122 \n",
      "Step 6 of Epoch: 75; Loss 1.398892 \n",
      "Step 7 of Epoch: 75; Loss 1.401821 \n",
      "Step 8 of Epoch: 75; Loss 1.404594 \n",
      "Step 9 of Epoch: 75; Loss 1.403830 \n",
      "Step 10 of Epoch: 75; Loss 1.404793 \n",
      "Step 11 of Epoch: 75; Loss 1.405948 \n",
      "Step 12 of Epoch: 75; Loss 1.407400 \n",
      "Step 13 of Epoch: 75; Loss 1.407635 \n",
      "Step 14 of Epoch: 75; Loss 1.407874 \n",
      "Step 15 of Epoch: 75; Loss 1.407308 \n",
      "Step 16 of Epoch: 75; Loss 1.407788 \n",
      "Step 17 of Epoch: 75; Loss 1.409173 \n",
      "Step 18 of Epoch: 75; Loss 1.409599 \n",
      "Step 19 of Epoch: 75; Loss 1.409846 \n",
      "Step 20 of Epoch: 75; Loss 1.410614 \n",
      "Step 21 of Epoch: 75; Loss 1.411995 \n",
      "Step 22 of Epoch: 75; Loss 1.412338 \n",
      "Step 23 of Epoch: 75; Loss 1.412503 \n",
      "Step 24 of Epoch: 75; Loss 1.412753 \n",
      "Step 25 of Epoch: 75; Loss 1.412901 \n",
      "Step 26 of Epoch: 75; Loss 1.413649 \n",
      "Step 27 of Epoch: 75; Loss 1.414126 \n",
      "Step 28 of Epoch: 75; Loss 1.414037 \n",
      "Step 29 of Epoch: 75; Loss 1.414534 \n",
      "Step 30 of Epoch: 75; Loss 1.415243 \n",
      "Step 31 of Epoch: 75; Loss 1.415620 \n",
      "Step 32 of Epoch: 75; Loss 1.415863 \n",
      "Step 33 of Epoch: 75; Loss 1.416240 \n",
      "Step 34 of Epoch: 75; Loss 1.416991 \n",
      "Step 35 of Epoch: 75; Loss 1.417227 \n",
      "Step 36 of Epoch: 75; Loss 1.417972 \n",
      "Step 37 of Epoch: 75; Loss 1.418451 \n",
      "Step 38 of Epoch: 75; Loss 1.418589 \n",
      "Step 39 of Epoch: 75; Loss 1.418911 \n",
      "Final Result for Epoch 75: Loss 1.419337; Val Acc 0.315150; Train Acc 0.436326\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 76; Loss 1.400921 \n",
      "Step 2 of Epoch: 76; Loss 1.401807 \n",
      "Step 3 of Epoch: 76; Loss 1.402961 \n",
      "Step 4 of Epoch: 76; Loss 1.402667 \n",
      "Step 5 of Epoch: 76; Loss 1.400262 \n",
      "Step 6 of Epoch: 76; Loss 1.403026 \n",
      "Step 7 of Epoch: 76; Loss 1.402991 \n",
      "Step 8 of Epoch: 76; Loss 1.403336 \n",
      "Step 9 of Epoch: 76; Loss 1.403430 \n",
      "Step 10 of Epoch: 76; Loss 1.402904 \n",
      "Step 11 of Epoch: 76; Loss 1.403772 \n",
      "Step 12 of Epoch: 76; Loss 1.405262 \n",
      "Step 13 of Epoch: 76; Loss 1.404829 \n",
      "Step 14 of Epoch: 76; Loss 1.403661 \n",
      "Step 15 of Epoch: 76; Loss 1.405259 \n",
      "Step 16 of Epoch: 76; Loss 1.406099 \n",
      "Step 17 of Epoch: 76; Loss 1.405751 \n",
      "Step 18 of Epoch: 76; Loss 1.406099 \n",
      "Step 19 of Epoch: 76; Loss 1.406853 \n",
      "Step 20 of Epoch: 76; Loss 1.407555 \n",
      "Step 21 of Epoch: 76; Loss 1.408179 \n",
      "Step 22 of Epoch: 76; Loss 1.408477 \n",
      "Step 23 of Epoch: 76; Loss 1.409517 \n",
      "Step 24 of Epoch: 76; Loss 1.410588 \n",
      "Step 25 of Epoch: 76; Loss 1.411184 \n",
      "Step 26 of Epoch: 76; Loss 1.412153 \n",
      "Step 27 of Epoch: 76; Loss 1.412347 \n",
      "Step 28 of Epoch: 76; Loss 1.412960 \n",
      "Step 29 of Epoch: 76; Loss 1.413665 \n",
      "Step 30 of Epoch: 76; Loss 1.414017 \n",
      "Step 31 of Epoch: 76; Loss 1.414821 \n",
      "Step 32 of Epoch: 76; Loss 1.415618 \n",
      "Step 33 of Epoch: 76; Loss 1.416130 \n",
      "Step 34 of Epoch: 76; Loss 1.416679 \n",
      "Step 35 of Epoch: 76; Loss 1.416999 \n",
      "Step 36 of Epoch: 76; Loss 1.417452 \n",
      "Step 37 of Epoch: 76; Loss 1.418186 \n",
      "Step 38 of Epoch: 76; Loss 1.418423 \n",
      "Step 39 of Epoch: 76; Loss 1.418730 \n",
      "Final Result for Epoch 76: Loss 1.418822; Val Acc 0.318437; Train Acc 0.436737\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 77; Loss 1.403894 \n",
      "Step 2 of Epoch: 77; Loss 1.399485 \n",
      "Step 3 of Epoch: 77; Loss 1.397715 \n",
      "Step 4 of Epoch: 77; Loss 1.397341 \n",
      "Step 5 of Epoch: 77; Loss 1.395234 \n",
      "Step 6 of Epoch: 77; Loss 1.398528 \n",
      "Step 7 of Epoch: 77; Loss 1.398665 \n",
      "Step 8 of Epoch: 77; Loss 1.399194 \n",
      "Step 9 of Epoch: 77; Loss 1.400708 \n",
      "Step 10 of Epoch: 77; Loss 1.401329 \n",
      "Step 11 of Epoch: 77; Loss 1.402305 \n",
      "Step 12 of Epoch: 77; Loss 1.403094 \n",
      "Step 13 of Epoch: 77; Loss 1.403988 \n",
      "Step 14 of Epoch: 77; Loss 1.405986 \n",
      "Step 15 of Epoch: 77; Loss 1.407607 \n",
      "Step 16 of Epoch: 77; Loss 1.409509 \n",
      "Step 17 of Epoch: 77; Loss 1.410362 \n",
      "Step 18 of Epoch: 77; Loss 1.410505 \n",
      "Step 19 of Epoch: 77; Loss 1.411323 \n",
      "Step 20 of Epoch: 77; Loss 1.411201 \n",
      "Step 21 of Epoch: 77; Loss 1.411425 \n",
      "Step 22 of Epoch: 77; Loss 1.411494 \n",
      "Step 23 of Epoch: 77; Loss 1.411359 \n",
      "Step 24 of Epoch: 77; Loss 1.411798 \n",
      "Step 25 of Epoch: 77; Loss 1.412280 \n",
      "Step 26 of Epoch: 77; Loss 1.413688 \n",
      "Step 27 of Epoch: 77; Loss 1.414097 \n",
      "Step 28 of Epoch: 77; Loss 1.414270 \n",
      "Step 29 of Epoch: 77; Loss 1.414165 \n",
      "Step 30 of Epoch: 77; Loss 1.414327 \n",
      "Step 31 of Epoch: 77; Loss 1.414589 \n",
      "Step 32 of Epoch: 77; Loss 1.415144 \n",
      "Step 33 of Epoch: 77; Loss 1.415903 \n",
      "Step 34 of Epoch: 77; Loss 1.416295 \n",
      "Step 35 of Epoch: 77; Loss 1.416687 \n",
      "Step 36 of Epoch: 77; Loss 1.416369 \n",
      "Step 37 of Epoch: 77; Loss 1.416291 \n",
      "Step 38 of Epoch: 77; Loss 1.417111 \n",
      "Step 39 of Epoch: 77; Loss 1.417511 \n",
      "Final Result for Epoch 77: Loss 1.417846; Val Acc 0.315591; Train Acc 0.436977\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 78; Loss 1.419667 \n",
      "Step 2 of Epoch: 78; Loss 1.407980 \n",
      "Step 3 of Epoch: 78; Loss 1.403788 \n",
      "Step 4 of Epoch: 78; Loss 1.403617 \n",
      "Step 5 of Epoch: 78; Loss 1.400854 \n",
      "Step 6 of Epoch: 78; Loss 1.401118 \n",
      "Step 7 of Epoch: 78; Loss 1.399030 \n",
      "Step 8 of Epoch: 78; Loss 1.401748 \n",
      "Step 9 of Epoch: 78; Loss 1.403306 \n",
      "Step 10 of Epoch: 78; Loss 1.404754 \n",
      "Step 11 of Epoch: 78; Loss 1.404836 \n",
      "Step 12 of Epoch: 78; Loss 1.404429 \n",
      "Step 13 of Epoch: 78; Loss 1.405894 \n",
      "Step 14 of Epoch: 78; Loss 1.405139 \n",
      "Step 15 of Epoch: 78; Loss 1.405732 \n",
      "Step 16 of Epoch: 78; Loss 1.406389 \n",
      "Step 17 of Epoch: 78; Loss 1.406330 \n",
      "Step 18 of Epoch: 78; Loss 1.406932 \n",
      "Step 19 of Epoch: 78; Loss 1.407395 \n",
      "Step 20 of Epoch: 78; Loss 1.408747 \n",
      "Step 21 of Epoch: 78; Loss 1.409355 \n",
      "Step 22 of Epoch: 78; Loss 1.410327 \n",
      "Step 23 of Epoch: 78; Loss 1.410996 \n",
      "Step 24 of Epoch: 78; Loss 1.411258 \n",
      "Step 25 of Epoch: 78; Loss 1.412321 \n",
      "Step 26 of Epoch: 78; Loss 1.412496 \n",
      "Step 27 of Epoch: 78; Loss 1.413106 \n",
      "Step 28 of Epoch: 78; Loss 1.413633 \n",
      "Step 29 of Epoch: 78; Loss 1.413757 \n",
      "Step 30 of Epoch: 78; Loss 1.414021 \n",
      "Step 31 of Epoch: 78; Loss 1.414782 \n",
      "Step 32 of Epoch: 78; Loss 1.415137 \n",
      "Step 33 of Epoch: 78; Loss 1.415726 \n",
      "Step 34 of Epoch: 78; Loss 1.416199 \n",
      "Step 35 of Epoch: 78; Loss 1.416911 \n",
      "Step 36 of Epoch: 78; Loss 1.416982 \n",
      "Step 37 of Epoch: 78; Loss 1.416922 \n",
      "Step 38 of Epoch: 78; Loss 1.417390 \n",
      "Step 39 of Epoch: 78; Loss 1.417684 \n",
      "Final Result for Epoch 78: Loss 1.417974; Val Acc 0.314910; Train Acc 0.436977\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 79; Loss 1.401933 \n",
      "Step 2 of Epoch: 79; Loss 1.406773 \n",
      "Step 3 of Epoch: 79; Loss 1.404156 \n",
      "Step 4 of Epoch: 79; Loss 1.404141 \n",
      "Step 5 of Epoch: 79; Loss 1.404883 \n",
      "Step 6 of Epoch: 79; Loss 1.405098 \n",
      "Step 7 of Epoch: 79; Loss 1.402953 \n",
      "Step 8 of Epoch: 79; Loss 1.404106 \n",
      "Step 9 of Epoch: 79; Loss 1.405047 \n",
      "Step 10 of Epoch: 79; Loss 1.404280 \n",
      "Step 11 of Epoch: 79; Loss 1.404068 \n",
      "Step 12 of Epoch: 79; Loss 1.403001 \n",
      "Step 13 of Epoch: 79; Loss 1.404256 \n",
      "Step 14 of Epoch: 79; Loss 1.404957 \n",
      "Step 15 of Epoch: 79; Loss 1.406676 \n",
      "Step 16 of Epoch: 79; Loss 1.408455 \n",
      "Step 17 of Epoch: 79; Loss 1.409175 \n",
      "Step 18 of Epoch: 79; Loss 1.408439 \n",
      "Step 19 of Epoch: 79; Loss 1.408393 \n",
      "Step 20 of Epoch: 79; Loss 1.410384 \n",
      "Step 21 of Epoch: 79; Loss 1.411343 \n",
      "Step 22 of Epoch: 79; Loss 1.412029 \n",
      "Step 23 of Epoch: 79; Loss 1.412307 \n",
      "Step 24 of Epoch: 79; Loss 1.412174 \n",
      "Step 25 of Epoch: 79; Loss 1.412724 \n",
      "Step 26 of Epoch: 79; Loss 1.412545 \n",
      "Step 27 of Epoch: 79; Loss 1.413198 \n",
      "Step 28 of Epoch: 79; Loss 1.413429 \n",
      "Step 29 of Epoch: 79; Loss 1.413763 \n",
      "Step 30 of Epoch: 79; Loss 1.414505 \n",
      "Step 31 of Epoch: 79; Loss 1.414725 \n",
      "Step 32 of Epoch: 79; Loss 1.414660 \n",
      "Step 33 of Epoch: 79; Loss 1.414986 \n",
      "Step 34 of Epoch: 79; Loss 1.415430 \n",
      "Step 35 of Epoch: 79; Loss 1.416046 \n",
      "Step 36 of Epoch: 79; Loss 1.415979 \n",
      "Step 37 of Epoch: 79; Loss 1.415949 \n",
      "Step 38 of Epoch: 79; Loss 1.416360 \n",
      "Step 39 of Epoch: 79; Loss 1.416767 \n",
      "Final Result for Epoch 79: Loss 1.417567; Val Acc 0.319279; Train Acc 0.437708\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 80; Loss 1.402858 \n",
      "Step 2 of Epoch: 80; Loss 1.400712 \n",
      "Step 3 of Epoch: 80; Loss 1.403035 \n",
      "Step 4 of Epoch: 80; Loss 1.404448 \n",
      "Step 5 of Epoch: 80; Loss 1.406207 \n",
      "Step 6 of Epoch: 80; Loss 1.407620 \n",
      "Step 7 of Epoch: 80; Loss 1.405416 \n",
      "Step 8 of Epoch: 80; Loss 1.406705 \n",
      "Step 9 of Epoch: 80; Loss 1.406556 \n",
      "Step 10 of Epoch: 80; Loss 1.405411 \n",
      "Step 11 of Epoch: 80; Loss 1.406099 \n",
      "Step 12 of Epoch: 80; Loss 1.407276 \n",
      "Step 13 of Epoch: 80; Loss 1.408432 \n",
      "Step 14 of Epoch: 80; Loss 1.407330 \n",
      "Step 15 of Epoch: 80; Loss 1.407982 \n",
      "Step 16 of Epoch: 80; Loss 1.409263 \n",
      "Step 17 of Epoch: 80; Loss 1.410011 \n",
      "Step 18 of Epoch: 80; Loss 1.410368 \n",
      "Step 19 of Epoch: 80; Loss 1.411285 \n",
      "Step 20 of Epoch: 80; Loss 1.411008 \n",
      "Step 21 of Epoch: 80; Loss 1.412057 \n",
      "Step 22 of Epoch: 80; Loss 1.412347 \n",
      "Step 23 of Epoch: 80; Loss 1.412080 \n",
      "Step 24 of Epoch: 80; Loss 1.412658 \n",
      "Step 25 of Epoch: 80; Loss 1.412652 \n",
      "Step 26 of Epoch: 80; Loss 1.413271 \n",
      "Step 27 of Epoch: 80; Loss 1.414057 \n",
      "Step 28 of Epoch: 80; Loss 1.413838 \n",
      "Step 29 of Epoch: 80; Loss 1.413642 \n",
      "Step 30 of Epoch: 80; Loss 1.414402 \n",
      "Step 31 of Epoch: 80; Loss 1.414698 \n",
      "Step 32 of Epoch: 80; Loss 1.414987 \n",
      "Step 33 of Epoch: 80; Loss 1.415207 \n",
      "Step 34 of Epoch: 80; Loss 1.415432 \n",
      "Step 35 of Epoch: 80; Loss 1.415800 \n",
      "Step 36 of Epoch: 80; Loss 1.416230 \n",
      "Step 37 of Epoch: 80; Loss 1.416587 \n",
      "Step 38 of Epoch: 80; Loss 1.417082 \n",
      "Step 39 of Epoch: 80; Loss 1.417479 \n",
      "Final Result for Epoch 80: Loss 1.417830; Val Acc 0.314990; Train Acc 0.436196\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 81; Loss 1.391871 \n",
      "Step 2 of Epoch: 81; Loss 1.402890 \n",
      "Step 3 of Epoch: 81; Loss 1.392809 \n",
      "Step 4 of Epoch: 81; Loss 1.393180 \n",
      "Step 5 of Epoch: 81; Loss 1.393321 \n",
      "Step 6 of Epoch: 81; Loss 1.392410 \n",
      "Step 7 of Epoch: 81; Loss 1.395332 \n",
      "Step 8 of Epoch: 81; Loss 1.396927 \n",
      "Step 9 of Epoch: 81; Loss 1.398285 \n",
      "Step 10 of Epoch: 81; Loss 1.398277 \n",
      "Step 11 of Epoch: 81; Loss 1.401167 \n",
      "Step 12 of Epoch: 81; Loss 1.403869 \n",
      "Step 13 of Epoch: 81; Loss 1.405844 \n",
      "Step 14 of Epoch: 81; Loss 1.405267 \n",
      "Step 15 of Epoch: 81; Loss 1.405786 \n",
      "Step 16 of Epoch: 81; Loss 1.407465 \n",
      "Step 17 of Epoch: 81; Loss 1.408295 \n",
      "Step 18 of Epoch: 81; Loss 1.408457 \n",
      "Step 19 of Epoch: 81; Loss 1.408790 \n",
      "Step 20 of Epoch: 81; Loss 1.409154 \n",
      "Step 21 of Epoch: 81; Loss 1.408887 \n",
      "Step 22 of Epoch: 81; Loss 1.409732 \n",
      "Step 23 of Epoch: 81; Loss 1.409241 \n",
      "Step 24 of Epoch: 81; Loss 1.410110 \n",
      "Step 25 of Epoch: 81; Loss 1.410100 \n",
      "Step 26 of Epoch: 81; Loss 1.411159 \n",
      "Step 27 of Epoch: 81; Loss 1.411513 \n",
      "Step 28 of Epoch: 81; Loss 1.411891 \n",
      "Step 29 of Epoch: 81; Loss 1.412070 \n",
      "Step 30 of Epoch: 81; Loss 1.412717 \n",
      "Step 31 of Epoch: 81; Loss 1.413219 \n",
      "Step 32 of Epoch: 81; Loss 1.413766 \n",
      "Step 33 of Epoch: 81; Loss 1.414171 \n",
      "Step 34 of Epoch: 81; Loss 1.414595 \n",
      "Step 35 of Epoch: 81; Loss 1.414719 \n",
      "Step 36 of Epoch: 81; Loss 1.414930 \n",
      "Step 37 of Epoch: 81; Loss 1.415739 \n",
      "Step 38 of Epoch: 81; Loss 1.416354 \n",
      "Step 39 of Epoch: 81; Loss 1.416398 \n",
      "Final Result for Epoch 81: Loss 1.416510; Val Acc 0.317475; Train Acc 0.437598\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 82; Loss 1.415508 \n",
      "Step 2 of Epoch: 82; Loss 1.404779 \n",
      "Step 3 of Epoch: 82; Loss 1.405069 \n",
      "Step 4 of Epoch: 82; Loss 1.402962 \n",
      "Step 5 of Epoch: 82; Loss 1.400763 \n",
      "Step 6 of Epoch: 82; Loss 1.399713 \n",
      "Step 7 of Epoch: 82; Loss 1.401569 \n",
      "Step 8 of Epoch: 82; Loss 1.402909 \n",
      "Step 9 of Epoch: 82; Loss 1.403446 \n",
      "Step 10 of Epoch: 82; Loss 1.404714 \n",
      "Step 11 of Epoch: 82; Loss 1.405199 \n",
      "Step 12 of Epoch: 82; Loss 1.404934 \n",
      "Step 13 of Epoch: 82; Loss 1.405738 \n",
      "Step 14 of Epoch: 82; Loss 1.406245 \n",
      "Step 15 of Epoch: 82; Loss 1.406277 \n",
      "Step 16 of Epoch: 82; Loss 1.404986 \n",
      "Step 17 of Epoch: 82; Loss 1.407434 \n",
      "Step 18 of Epoch: 82; Loss 1.407582 \n",
      "Step 19 of Epoch: 82; Loss 1.407621 \n",
      "Step 20 of Epoch: 82; Loss 1.408897 \n",
      "Step 21 of Epoch: 82; Loss 1.409645 \n",
      "Step 22 of Epoch: 82; Loss 1.411428 \n",
      "Step 23 of Epoch: 82; Loss 1.411917 \n",
      "Step 24 of Epoch: 82; Loss 1.412198 \n",
      "Step 25 of Epoch: 82; Loss 1.412116 \n",
      "Step 26 of Epoch: 82; Loss 1.412099 \n",
      "Step 27 of Epoch: 82; Loss 1.412436 \n",
      "Step 28 of Epoch: 82; Loss 1.412749 \n",
      "Step 29 of Epoch: 82; Loss 1.413004 \n",
      "Step 30 of Epoch: 82; Loss 1.413472 \n",
      "Step 31 of Epoch: 82; Loss 1.413529 \n",
      "Step 32 of Epoch: 82; Loss 1.413727 \n",
      "Step 33 of Epoch: 82; Loss 1.413796 \n",
      "Step 34 of Epoch: 82; Loss 1.414148 \n",
      "Step 35 of Epoch: 82; Loss 1.414994 \n",
      "Step 36 of Epoch: 82; Loss 1.415112 \n",
      "Step 37 of Epoch: 82; Loss 1.415578 \n",
      "Step 38 of Epoch: 82; Loss 1.415622 \n",
      "Step 39 of Epoch: 82; Loss 1.416053 \n",
      "Final Result for Epoch 82: Loss 1.416130; Val Acc 0.315471; Train Acc 0.434940\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 83; Loss 1.397727 \n",
      "Step 2 of Epoch: 83; Loss 1.401650 \n",
      "Step 3 of Epoch: 83; Loss 1.398658 \n",
      "Step 4 of Epoch: 83; Loss 1.402163 \n",
      "Step 5 of Epoch: 83; Loss 1.402443 \n",
      "Step 6 of Epoch: 83; Loss 1.401972 \n",
      "Step 7 of Epoch: 83; Loss 1.401919 \n",
      "Step 8 of Epoch: 83; Loss 1.404686 \n",
      "Step 9 of Epoch: 83; Loss 1.403468 \n",
      "Step 10 of Epoch: 83; Loss 1.403848 \n",
      "Step 11 of Epoch: 83; Loss 1.404723 \n",
      "Step 12 of Epoch: 83; Loss 1.404543 \n",
      "Step 13 of Epoch: 83; Loss 1.406207 \n",
      "Step 14 of Epoch: 83; Loss 1.407465 \n",
      "Step 15 of Epoch: 83; Loss 1.407876 \n",
      "Step 16 of Epoch: 83; Loss 1.408542 \n",
      "Step 17 of Epoch: 83; Loss 1.409642 \n",
      "Step 18 of Epoch: 83; Loss 1.408882 \n",
      "Step 19 of Epoch: 83; Loss 1.409400 \n",
      "Step 20 of Epoch: 83; Loss 1.409298 \n",
      "Step 21 of Epoch: 83; Loss 1.409500 \n",
      "Step 22 of Epoch: 83; Loss 1.409997 \n",
      "Step 23 of Epoch: 83; Loss 1.409520 \n",
      "Step 24 of Epoch: 83; Loss 1.410730 \n",
      "Step 25 of Epoch: 83; Loss 1.410738 \n",
      "Step 26 of Epoch: 83; Loss 1.411890 \n",
      "Step 27 of Epoch: 83; Loss 1.412021 \n",
      "Step 28 of Epoch: 83; Loss 1.412484 \n",
      "Step 29 of Epoch: 83; Loss 1.413048 \n",
      "Step 30 of Epoch: 83; Loss 1.413034 \n",
      "Step 31 of Epoch: 83; Loss 1.412942 \n",
      "Step 32 of Epoch: 83; Loss 1.413221 \n",
      "Step 33 of Epoch: 83; Loss 1.413599 \n",
      "Step 34 of Epoch: 83; Loss 1.414321 \n",
      "Step 35 of Epoch: 83; Loss 1.414431 \n",
      "Step 36 of Epoch: 83; Loss 1.414085 \n",
      "Step 37 of Epoch: 83; Loss 1.414239 \n",
      "Step 38 of Epoch: 83; Loss 1.414564 \n",
      "Step 39 of Epoch: 83; Loss 1.415339 \n",
      "Final Result for Epoch 83: Loss 1.415644; Val Acc 0.315992; Train Acc 0.436221\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 84; Loss 1.391324 \n",
      "Step 2 of Epoch: 84; Loss 1.395933 \n",
      "Step 3 of Epoch: 84; Loss 1.393920 \n",
      "Step 4 of Epoch: 84; Loss 1.401275 \n",
      "Step 5 of Epoch: 84; Loss 1.399298 \n",
      "Step 6 of Epoch: 84; Loss 1.398392 \n",
      "Step 7 of Epoch: 84; Loss 1.400658 \n",
      "Step 8 of Epoch: 84; Loss 1.400845 \n",
      "Step 9 of Epoch: 84; Loss 1.401659 \n",
      "Step 10 of Epoch: 84; Loss 1.402024 \n",
      "Step 11 of Epoch: 84; Loss 1.403338 \n",
      "Step 12 of Epoch: 84; Loss 1.406007 \n",
      "Step 13 of Epoch: 84; Loss 1.406610 \n",
      "Step 14 of Epoch: 84; Loss 1.408278 \n",
      "Step 15 of Epoch: 84; Loss 1.408520 \n",
      "Step 16 of Epoch: 84; Loss 1.409099 \n",
      "Step 17 of Epoch: 84; Loss 1.408606 \n",
      "Step 18 of Epoch: 84; Loss 1.408928 \n",
      "Step 19 of Epoch: 84; Loss 1.408469 \n",
      "Step 20 of Epoch: 84; Loss 1.408690 \n",
      "Step 21 of Epoch: 84; Loss 1.409332 \n",
      "Step 22 of Epoch: 84; Loss 1.409260 \n",
      "Step 23 of Epoch: 84; Loss 1.410453 \n",
      "Step 24 of Epoch: 84; Loss 1.410837 \n",
      "Step 25 of Epoch: 84; Loss 1.410555 \n",
      "Step 26 of Epoch: 84; Loss 1.411035 \n",
      "Step 27 of Epoch: 84; Loss 1.411652 \n",
      "Step 28 of Epoch: 84; Loss 1.412522 \n",
      "Step 29 of Epoch: 84; Loss 1.413132 \n",
      "Step 30 of Epoch: 84; Loss 1.413001 \n",
      "Step 31 of Epoch: 84; Loss 1.412646 \n",
      "Step 32 of Epoch: 84; Loss 1.413224 \n",
      "Step 33 of Epoch: 84; Loss 1.413497 \n",
      "Step 34 of Epoch: 84; Loss 1.413825 \n",
      "Step 35 of Epoch: 84; Loss 1.414626 \n",
      "Step 36 of Epoch: 84; Loss 1.414984 \n",
      "Step 37 of Epoch: 84; Loss 1.415073 \n",
      "Step 38 of Epoch: 84; Loss 1.415785 \n",
      "Step 39 of Epoch: 84; Loss 1.415885 \n",
      "Final Result for Epoch 84: Loss 1.415871; Val Acc 0.315511; Train Acc 0.437778\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 85; Loss 1.414543 \n",
      "Step 2 of Epoch: 85; Loss 1.399133 \n",
      "Step 3 of Epoch: 85; Loss 1.402994 \n",
      "Step 4 of Epoch: 85; Loss 1.407491 \n",
      "Step 5 of Epoch: 85; Loss 1.407026 \n",
      "Step 6 of Epoch: 85; Loss 1.407345 \n",
      "Step 7 of Epoch: 85; Loss 1.416137 \n",
      "Step 8 of Epoch: 85; Loss 1.424940 \n",
      "Step 9 of Epoch: 85; Loss 1.429493 \n",
      "Step 10 of Epoch: 85; Loss 1.431838 \n",
      "Step 11 of Epoch: 85; Loss 1.435250 \n",
      "Step 12 of Epoch: 85; Loss 1.435108 \n",
      "Step 13 of Epoch: 85; Loss 1.435897 \n",
      "Step 14 of Epoch: 85; Loss 1.437047 \n",
      "Step 15 of Epoch: 85; Loss 1.437731 \n",
      "Step 16 of Epoch: 85; Loss 1.438406 \n",
      "Step 17 of Epoch: 85; Loss 1.439753 \n",
      "Step 18 of Epoch: 85; Loss 1.440433 \n",
      "Step 19 of Epoch: 85; Loss 1.440573 \n",
      "Step 20 of Epoch: 85; Loss 1.441195 \n",
      "Step 21 of Epoch: 85; Loss 1.441463 \n",
      "Step 22 of Epoch: 85; Loss 1.441485 \n",
      "Step 23 of Epoch: 85; Loss 1.441697 \n",
      "Step 24 of Epoch: 85; Loss 1.441773 \n",
      "Step 25 of Epoch: 85; Loss 1.442001 \n",
      "Step 26 of Epoch: 85; Loss 1.441719 \n",
      "Step 27 of Epoch: 85; Loss 1.441989 \n",
      "Step 28 of Epoch: 85; Loss 1.442051 \n",
      "Step 29 of Epoch: 85; Loss 1.442362 \n",
      "Step 30 of Epoch: 85; Loss 1.442767 \n",
      "Step 31 of Epoch: 85; Loss 1.443320 \n",
      "Step 32 of Epoch: 85; Loss 1.443426 \n",
      "Step 33 of Epoch: 85; Loss 1.443597 \n",
      "Step 34 of Epoch: 85; Loss 1.443858 \n",
      "Step 35 of Epoch: 85; Loss 1.444503 \n",
      "Step 36 of Epoch: 85; Loss 1.444749 \n",
      "Step 37 of Epoch: 85; Loss 1.445155 \n",
      "Step 38 of Epoch: 85; Loss 1.445437 \n",
      "Step 39 of Epoch: 85; Loss 1.445532 \n",
      "Final Result for Epoch 85: Loss 1.445466; Val Acc 0.315752; Train Acc 0.424489\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 86; Loss 1.410661 \n",
      "Step 2 of Epoch: 86; Loss 1.424713 \n",
      "Step 3 of Epoch: 86; Loss 1.424858 \n",
      "Step 4 of Epoch: 86; Loss 1.435185 \n",
      "Step 5 of Epoch: 86; Loss 1.433745 \n",
      "Step 6 of Epoch: 86; Loss 1.435928 \n",
      "Step 7 of Epoch: 86; Loss 1.435376 \n",
      "Step 8 of Epoch: 86; Loss 1.434114 \n",
      "Step 9 of Epoch: 86; Loss 1.433545 \n",
      "Step 10 of Epoch: 86; Loss 1.431964 \n",
      "Step 11 of Epoch: 86; Loss 1.431889 \n",
      "Step 12 of Epoch: 86; Loss 1.432905 \n",
      "Step 13 of Epoch: 86; Loss 1.433734 \n",
      "Step 14 of Epoch: 86; Loss 1.434386 \n",
      "Step 15 of Epoch: 86; Loss 1.435488 \n",
      "Step 16 of Epoch: 86; Loss 1.435881 \n",
      "Step 17 of Epoch: 86; Loss 1.436883 \n",
      "Step 18 of Epoch: 86; Loss 1.437302 \n",
      "Step 19 of Epoch: 86; Loss 1.437635 \n",
      "Step 20 of Epoch: 86; Loss 1.437938 \n",
      "Step 21 of Epoch: 86; Loss 1.437612 \n",
      "Step 22 of Epoch: 86; Loss 1.437418 \n",
      "Step 23 of Epoch: 86; Loss 1.437879 \n",
      "Step 24 of Epoch: 86; Loss 1.438727 \n",
      "Step 25 of Epoch: 86; Loss 1.439090 \n",
      "Step 26 of Epoch: 86; Loss 1.438471 \n",
      "Step 27 of Epoch: 86; Loss 1.437964 \n",
      "Step 28 of Epoch: 86; Loss 1.438443 \n",
      "Step 29 of Epoch: 86; Loss 1.438445 \n",
      "Step 30 of Epoch: 86; Loss 1.438371 \n",
      "Step 31 of Epoch: 86; Loss 1.438831 \n",
      "Step 32 of Epoch: 86; Loss 1.439040 \n",
      "Step 33 of Epoch: 86; Loss 1.438607 \n",
      "Step 34 of Epoch: 86; Loss 1.439278 \n",
      "Step 35 of Epoch: 86; Loss 1.439707 \n",
      "Step 36 of Epoch: 86; Loss 1.439828 \n",
      "Step 37 of Epoch: 86; Loss 1.439596 \n",
      "Step 38 of Epoch: 86; Loss 1.439277 \n",
      "Step 39 of Epoch: 86; Loss 1.438824 \n",
      "Final Result for Epoch 86: Loss 1.438780; Val Acc 0.317715; Train Acc 0.431031\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 87; Loss 1.422202 \n",
      "Step 2 of Epoch: 87; Loss 1.417384 \n",
      "Step 3 of Epoch: 87; Loss 1.412161 \n",
      "Step 4 of Epoch: 87; Loss 1.413003 \n",
      "Step 5 of Epoch: 87; Loss 1.413954 \n",
      "Step 6 of Epoch: 87; Loss 1.413422 \n",
      "Step 7 of Epoch: 87; Loss 1.414345 \n",
      "Step 8 of Epoch: 87; Loss 1.415203 \n",
      "Step 9 of Epoch: 87; Loss 1.418185 \n",
      "Step 10 of Epoch: 87; Loss 1.415571 \n",
      "Step 11 of Epoch: 87; Loss 1.414673 \n",
      "Step 12 of Epoch: 87; Loss 1.413844 \n",
      "Step 13 of Epoch: 87; Loss 1.414423 \n",
      "Step 14 of Epoch: 87; Loss 1.414321 \n",
      "Step 15 of Epoch: 87; Loss 1.415046 \n",
      "Step 16 of Epoch: 87; Loss 1.414568 \n",
      "Step 17 of Epoch: 87; Loss 1.414643 \n",
      "Step 18 of Epoch: 87; Loss 1.415398 \n",
      "Step 19 of Epoch: 87; Loss 1.415076 \n",
      "Step 20 of Epoch: 87; Loss 1.415038 \n",
      "Step 21 of Epoch: 87; Loss 1.415644 \n",
      "Step 22 of Epoch: 87; Loss 1.415615 \n",
      "Step 23 of Epoch: 87; Loss 1.415256 \n",
      "Step 24 of Epoch: 87; Loss 1.415761 \n",
      "Step 25 of Epoch: 87; Loss 1.415793 \n",
      "Step 26 of Epoch: 87; Loss 1.416398 \n",
      "Step 27 of Epoch: 87; Loss 1.416098 \n",
      "Step 28 of Epoch: 87; Loss 1.416397 \n",
      "Step 29 of Epoch: 87; Loss 1.416509 \n",
      "Step 30 of Epoch: 87; Loss 1.416058 \n",
      "Step 31 of Epoch: 87; Loss 1.416636 \n",
      "Step 32 of Epoch: 87; Loss 1.416729 \n",
      "Step 33 of Epoch: 87; Loss 1.416548 \n",
      "Step 34 of Epoch: 87; Loss 1.416824 \n",
      "Step 35 of Epoch: 87; Loss 1.416702 \n",
      "Step 36 of Epoch: 87; Loss 1.417514 \n",
      "Step 37 of Epoch: 87; Loss 1.417404 \n",
      "Step 38 of Epoch: 87; Loss 1.417045 \n",
      "Step 39 of Epoch: 87; Loss 1.416930 \n",
      "Final Result for Epoch 87: Loss 1.416679; Val Acc 0.318597; Train Acc 0.436892\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 88; Loss 1.387561 \n",
      "Step 2 of Epoch: 88; Loss 1.402729 \n",
      "Step 3 of Epoch: 88; Loss 1.395509 \n",
      "Step 4 of Epoch: 88; Loss 1.395016 \n",
      "Step 5 of Epoch: 88; Loss 1.399935 \n",
      "Step 6 of Epoch: 88; Loss 1.402933 \n",
      "Step 7 of Epoch: 88; Loss 1.402699 \n",
      "Step 8 of Epoch: 88; Loss 1.402158 \n",
      "Step 9 of Epoch: 88; Loss 1.403017 \n",
      "Step 10 of Epoch: 88; Loss 1.403210 \n",
      "Step 11 of Epoch: 88; Loss 1.402839 \n",
      "Step 12 of Epoch: 88; Loss 1.404412 \n",
      "Step 13 of Epoch: 88; Loss 1.406621 \n",
      "Step 14 of Epoch: 88; Loss 1.406774 \n",
      "Step 15 of Epoch: 88; Loss 1.406592 \n",
      "Step 16 of Epoch: 88; Loss 1.406653 \n",
      "Step 17 of Epoch: 88; Loss 1.407571 \n",
      "Step 18 of Epoch: 88; Loss 1.407287 \n",
      "Step 19 of Epoch: 88; Loss 1.407829 \n",
      "Step 20 of Epoch: 88; Loss 1.408210 \n",
      "Step 21 of Epoch: 88; Loss 1.408346 \n",
      "Step 22 of Epoch: 88; Loss 1.409096 \n",
      "Step 23 of Epoch: 88; Loss 1.409288 \n",
      "Step 24 of Epoch: 88; Loss 1.409360 \n",
      "Step 25 of Epoch: 88; Loss 1.409770 \n",
      "Step 26 of Epoch: 88; Loss 1.410064 \n",
      "Step 27 of Epoch: 88; Loss 1.410519 \n",
      "Step 28 of Epoch: 88; Loss 1.409898 \n",
      "Step 29 of Epoch: 88; Loss 1.410141 \n",
      "Step 30 of Epoch: 88; Loss 1.410144 \n",
      "Step 31 of Epoch: 88; Loss 1.410149 \n",
      "Step 32 of Epoch: 88; Loss 1.410434 \n",
      "Step 33 of Epoch: 88; Loss 1.410228 \n",
      "Step 34 of Epoch: 88; Loss 1.410324 \n",
      "Step 35 of Epoch: 88; Loss 1.410344 \n",
      "Step 36 of Epoch: 88; Loss 1.410732 \n",
      "Step 37 of Epoch: 88; Loss 1.411027 \n",
      "Step 38 of Epoch: 88; Loss 1.410942 \n",
      "Step 39 of Epoch: 88; Loss 1.411546 \n",
      "Final Result for Epoch 88: Loss 1.411758; Val Acc 0.315872; Train Acc 0.436777\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 89; Loss 1.399616 \n",
      "Step 2 of Epoch: 89; Loss 1.396117 \n",
      "Step 3 of Epoch: 89; Loss 1.396058 \n",
      "Step 4 of Epoch: 89; Loss 1.398456 \n",
      "Step 5 of Epoch: 89; Loss 1.401127 \n",
      "Step 6 of Epoch: 89; Loss 1.400526 \n",
      "Step 7 of Epoch: 89; Loss 1.400157 \n",
      "Step 8 of Epoch: 89; Loss 1.400650 \n",
      "Step 9 of Epoch: 89; Loss 1.402445 \n",
      "Step 10 of Epoch: 89; Loss 1.403333 \n",
      "Step 11 of Epoch: 89; Loss 1.404993 \n",
      "Step 12 of Epoch: 89; Loss 1.405244 \n",
      "Step 13 of Epoch: 89; Loss 1.405131 \n",
      "Step 14 of Epoch: 89; Loss 1.404481 \n",
      "Step 15 of Epoch: 89; Loss 1.405881 \n",
      "Step 16 of Epoch: 89; Loss 1.406261 \n",
      "Step 17 of Epoch: 89; Loss 1.406593 \n",
      "Step 18 of Epoch: 89; Loss 1.405406 \n",
      "Step 19 of Epoch: 89; Loss 1.407188 \n",
      "Step 20 of Epoch: 89; Loss 1.406925 \n",
      "Step 21 of Epoch: 89; Loss 1.406754 \n",
      "Step 22 of Epoch: 89; Loss 1.406472 \n",
      "Step 23 of Epoch: 89; Loss 1.406146 \n",
      "Step 24 of Epoch: 89; Loss 1.405910 \n",
      "Step 25 of Epoch: 89; Loss 1.405927 \n",
      "Step 26 of Epoch: 89; Loss 1.406112 \n",
      "Step 27 of Epoch: 89; Loss 1.406005 \n",
      "Step 28 of Epoch: 89; Loss 1.407144 \n",
      "Step 29 of Epoch: 89; Loss 1.406736 \n",
      "Step 30 of Epoch: 89; Loss 1.407210 \n",
      "Step 31 of Epoch: 89; Loss 1.407329 \n",
      "Step 32 of Epoch: 89; Loss 1.407634 \n",
      "Step 33 of Epoch: 89; Loss 1.408284 \n",
      "Step 34 of Epoch: 89; Loss 1.408696 \n",
      "Step 35 of Epoch: 89; Loss 1.408661 \n",
      "Step 36 of Epoch: 89; Loss 1.409657 \n",
      "Step 37 of Epoch: 89; Loss 1.409880 \n",
      "Step 38 of Epoch: 89; Loss 1.410319 \n",
      "Step 39 of Epoch: 89; Loss 1.410677 \n",
      "Final Result for Epoch 89: Loss 1.410736; Val Acc 0.317355; Train Acc 0.437908\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 90; Loss 1.418149 \n",
      "Step 2 of Epoch: 90; Loss 1.400997 \n",
      "Step 3 of Epoch: 90; Loss 1.398574 \n",
      "Step 4 of Epoch: 90; Loss 1.397477 \n",
      "Step 5 of Epoch: 90; Loss 1.394970 \n",
      "Step 6 of Epoch: 90; Loss 1.397924 \n",
      "Step 7 of Epoch: 90; Loss 1.400034 \n",
      "Step 8 of Epoch: 90; Loss 1.399046 \n",
      "Step 9 of Epoch: 90; Loss 1.400449 \n",
      "Step 10 of Epoch: 90; Loss 1.400937 \n",
      "Step 11 of Epoch: 90; Loss 1.400458 \n",
      "Step 12 of Epoch: 90; Loss 1.400680 \n",
      "Step 13 of Epoch: 90; Loss 1.401420 \n",
      "Step 14 of Epoch: 90; Loss 1.401502 \n",
      "Step 15 of Epoch: 90; Loss 1.402225 \n",
      "Step 16 of Epoch: 90; Loss 1.403196 \n",
      "Step 17 of Epoch: 90; Loss 1.404852 \n",
      "Step 18 of Epoch: 90; Loss 1.406159 \n",
      "Step 19 of Epoch: 90; Loss 1.407003 \n",
      "Step 20 of Epoch: 90; Loss 1.407114 \n",
      "Step 21 of Epoch: 90; Loss 1.407743 \n",
      "Step 22 of Epoch: 90; Loss 1.407235 \n",
      "Step 23 of Epoch: 90; Loss 1.407677 \n",
      "Step 24 of Epoch: 90; Loss 1.407654 \n",
      "Step 25 of Epoch: 90; Loss 1.408204 \n",
      "Step 26 of Epoch: 90; Loss 1.408332 \n",
      "Step 27 of Epoch: 90; Loss 1.409126 \n",
      "Step 28 of Epoch: 90; Loss 1.409214 \n",
      "Step 29 of Epoch: 90; Loss 1.409631 \n",
      "Step 30 of Epoch: 90; Loss 1.409650 \n",
      "Step 31 of Epoch: 90; Loss 1.410064 \n",
      "Step 32 of Epoch: 90; Loss 1.410344 \n",
      "Step 33 of Epoch: 90; Loss 1.410952 \n",
      "Step 34 of Epoch: 90; Loss 1.411287 \n",
      "Step 35 of Epoch: 90; Loss 1.411689 \n",
      "Step 36 of Epoch: 90; Loss 1.411902 \n",
      "Step 37 of Epoch: 90; Loss 1.411484 \n",
      "Step 38 of Epoch: 90; Loss 1.412536 \n",
      "Step 39 of Epoch: 90; Loss 1.412952 \n",
      "Final Result for Epoch 90: Loss 1.412804; Val Acc 0.315511; Train Acc 0.440826\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 91; Loss 1.402359 \n",
      "Step 2 of Epoch: 91; Loss 1.402618 \n",
      "Step 3 of Epoch: 91; Loss 1.401730 \n",
      "Step 4 of Epoch: 91; Loss 1.399282 \n",
      "Step 5 of Epoch: 91; Loss 1.400045 \n",
      "Step 6 of Epoch: 91; Loss 1.400486 \n",
      "Step 7 of Epoch: 91; Loss 1.402931 \n",
      "Step 8 of Epoch: 91; Loss 1.404483 \n",
      "Step 9 of Epoch: 91; Loss 1.404524 \n",
      "Step 10 of Epoch: 91; Loss 1.406205 \n",
      "Step 11 of Epoch: 91; Loss 1.408194 \n",
      "Step 12 of Epoch: 91; Loss 1.408566 \n",
      "Step 13 of Epoch: 91; Loss 1.409960 \n",
      "Step 14 of Epoch: 91; Loss 1.410138 \n",
      "Step 15 of Epoch: 91; Loss 1.410493 \n",
      "Step 16 of Epoch: 91; Loss 1.409526 \n",
      "Step 17 of Epoch: 91; Loss 1.409152 \n",
      "Step 18 of Epoch: 91; Loss 1.409158 \n",
      "Step 19 of Epoch: 91; Loss 1.409429 \n",
      "Step 20 of Epoch: 91; Loss 1.409238 \n",
      "Step 21 of Epoch: 91; Loss 1.408823 \n",
      "Step 22 of Epoch: 91; Loss 1.409185 \n",
      "Step 23 of Epoch: 91; Loss 1.409342 \n",
      "Step 24 of Epoch: 91; Loss 1.410397 \n",
      "Step 25 of Epoch: 91; Loss 1.411592 \n",
      "Step 26 of Epoch: 91; Loss 1.411629 \n",
      "Step 27 of Epoch: 91; Loss 1.411945 \n",
      "Step 28 of Epoch: 91; Loss 1.411920 \n",
      "Step 29 of Epoch: 91; Loss 1.412089 \n",
      "Step 30 of Epoch: 91; Loss 1.412389 \n",
      "Step 31 of Epoch: 91; Loss 1.412660 \n",
      "Step 32 of Epoch: 91; Loss 1.413050 \n",
      "Step 33 of Epoch: 91; Loss 1.413550 \n",
      "Step 34 of Epoch: 91; Loss 1.413915 \n",
      "Step 35 of Epoch: 91; Loss 1.414108 \n",
      "Step 36 of Epoch: 91; Loss 1.414030 \n",
      "Step 37 of Epoch: 91; Loss 1.414573 \n",
      "Step 38 of Epoch: 91; Loss 1.414392 \n",
      "Step 39 of Epoch: 91; Loss 1.414550 \n",
      "Final Result for Epoch 91: Loss 1.414895; Val Acc 0.313467; Train Acc 0.436491\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 92; Loss 1.395683 \n",
      "Step 2 of Epoch: 92; Loss 1.389234 \n",
      "Step 3 of Epoch: 92; Loss 1.387180 \n",
      "Step 4 of Epoch: 92; Loss 1.386474 \n",
      "Step 5 of Epoch: 92; Loss 1.388612 \n",
      "Step 6 of Epoch: 92; Loss 1.389730 \n",
      "Step 7 of Epoch: 92; Loss 1.393370 \n",
      "Step 8 of Epoch: 92; Loss 1.396470 \n",
      "Step 9 of Epoch: 92; Loss 1.397549 \n",
      "Step 10 of Epoch: 92; Loss 1.397586 \n",
      "Step 11 of Epoch: 92; Loss 1.399598 \n",
      "Step 12 of Epoch: 92; Loss 1.400269 \n",
      "Step 13 of Epoch: 92; Loss 1.400669 \n",
      "Step 14 of Epoch: 92; Loss 1.402796 \n",
      "Step 15 of Epoch: 92; Loss 1.404565 \n",
      "Step 16 of Epoch: 92; Loss 1.405950 \n",
      "Step 17 of Epoch: 92; Loss 1.406637 \n",
      "Step 18 of Epoch: 92; Loss 1.406450 \n",
      "Step 19 of Epoch: 92; Loss 1.406506 \n",
      "Step 20 of Epoch: 92; Loss 1.408017 \n",
      "Step 21 of Epoch: 92; Loss 1.408647 \n",
      "Step 22 of Epoch: 92; Loss 1.409390 \n",
      "Step 23 of Epoch: 92; Loss 1.410344 \n",
      "Step 24 of Epoch: 92; Loss 1.410326 \n",
      "Step 25 of Epoch: 92; Loss 1.410594 \n",
      "Step 26 of Epoch: 92; Loss 1.410439 \n",
      "Step 27 of Epoch: 92; Loss 1.411096 \n",
      "Step 28 of Epoch: 92; Loss 1.410893 \n",
      "Step 29 of Epoch: 92; Loss 1.410990 \n",
      "Step 30 of Epoch: 92; Loss 1.411969 \n",
      "Step 31 of Epoch: 92; Loss 1.412772 \n",
      "Step 32 of Epoch: 92; Loss 1.412943 \n",
      "Step 33 of Epoch: 92; Loss 1.412984 \n",
      "Step 34 of Epoch: 92; Loss 1.413490 \n",
      "Step 35 of Epoch: 92; Loss 1.413847 \n",
      "Step 36 of Epoch: 92; Loss 1.414059 \n",
      "Step 37 of Epoch: 92; Loss 1.413850 \n",
      "Step 38 of Epoch: 92; Loss 1.414378 \n",
      "Step 39 of Epoch: 92; Loss 1.414294 \n",
      "Final Result for Epoch 92: Loss 1.414091; Val Acc 0.316473; Train Acc 0.437523\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 93; Loss 1.403979 \n",
      "Step 2 of Epoch: 93; Loss 1.396217 \n",
      "Step 3 of Epoch: 93; Loss 1.392852 \n",
      "Step 4 of Epoch: 93; Loss 1.394458 \n",
      "Step 5 of Epoch: 93; Loss 1.394613 \n",
      "Step 6 of Epoch: 93; Loss 1.396029 \n",
      "Step 7 of Epoch: 93; Loss 1.396529 \n",
      "Step 8 of Epoch: 93; Loss 1.398250 \n",
      "Step 9 of Epoch: 93; Loss 1.397350 \n",
      "Step 10 of Epoch: 93; Loss 1.397613 \n",
      "Step 11 of Epoch: 93; Loss 1.398007 \n",
      "Step 12 of Epoch: 93; Loss 1.399247 \n",
      "Step 13 of Epoch: 93; Loss 1.400427 \n",
      "Step 14 of Epoch: 93; Loss 1.401325 \n",
      "Step 15 of Epoch: 93; Loss 1.402366 \n",
      "Step 16 of Epoch: 93; Loss 1.403169 \n",
      "Step 17 of Epoch: 93; Loss 1.403970 \n",
      "Step 18 of Epoch: 93; Loss 1.404258 \n",
      "Step 19 of Epoch: 93; Loss 1.404753 \n",
      "Step 20 of Epoch: 93; Loss 1.405744 \n",
      "Step 21 of Epoch: 93; Loss 1.407226 \n",
      "Step 22 of Epoch: 93; Loss 1.406571 \n",
      "Step 23 of Epoch: 93; Loss 1.406519 \n",
      "Step 24 of Epoch: 93; Loss 1.406850 \n",
      "Step 25 of Epoch: 93; Loss 1.407777 \n",
      "Step 26 of Epoch: 93; Loss 1.408435 \n",
      "Step 27 of Epoch: 93; Loss 1.408857 \n",
      "Step 28 of Epoch: 93; Loss 1.409125 \n",
      "Step 29 of Epoch: 93; Loss 1.409724 \n",
      "Step 30 of Epoch: 93; Loss 1.410310 \n",
      "Step 31 of Epoch: 93; Loss 1.410884 \n",
      "Step 32 of Epoch: 93; Loss 1.411838 \n",
      "Step 33 of Epoch: 93; Loss 1.411671 \n",
      "Step 34 of Epoch: 93; Loss 1.412160 \n",
      "Step 35 of Epoch: 93; Loss 1.412338 \n",
      "Step 36 of Epoch: 93; Loss 1.412502 \n",
      "Step 37 of Epoch: 93; Loss 1.412489 \n",
      "Step 38 of Epoch: 93; Loss 1.412768 \n",
      "Step 39 of Epoch: 93; Loss 1.412851 \n",
      "Final Result for Epoch 93: Loss 1.413585; Val Acc 0.316673; Train Acc 0.435616\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 94; Loss 1.384341 \n",
      "Step 2 of Epoch: 94; Loss 1.385813 \n",
      "Step 3 of Epoch: 94; Loss 1.391573 \n",
      "Step 4 of Epoch: 94; Loss 1.394338 \n",
      "Step 5 of Epoch: 94; Loss 1.395722 \n",
      "Step 6 of Epoch: 94; Loss 1.396662 \n",
      "Step 7 of Epoch: 94; Loss 1.399864 \n",
      "Step 8 of Epoch: 94; Loss 1.398621 \n",
      "Step 9 of Epoch: 94; Loss 1.400822 \n",
      "Step 10 of Epoch: 94; Loss 1.401437 \n",
      "Step 11 of Epoch: 94; Loss 1.402227 \n",
      "Step 12 of Epoch: 94; Loss 1.404758 \n",
      "Step 13 of Epoch: 94; Loss 1.406610 \n",
      "Step 14 of Epoch: 94; Loss 1.406134 \n",
      "Step 15 of Epoch: 94; Loss 1.406063 \n",
      "Step 16 of Epoch: 94; Loss 1.405963 \n",
      "Step 17 of Epoch: 94; Loss 1.405933 \n",
      "Step 18 of Epoch: 94; Loss 1.405318 \n",
      "Step 19 of Epoch: 94; Loss 1.405431 \n",
      "Step 20 of Epoch: 94; Loss 1.406338 \n",
      "Step 21 of Epoch: 94; Loss 1.406406 \n",
      "Step 22 of Epoch: 94; Loss 1.407686 \n",
      "Step 23 of Epoch: 94; Loss 1.408045 \n",
      "Step 24 of Epoch: 94; Loss 1.407534 \n",
      "Step 25 of Epoch: 94; Loss 1.408504 \n",
      "Step 26 of Epoch: 94; Loss 1.408163 \n",
      "Step 27 of Epoch: 94; Loss 1.408035 \n",
      "Step 28 of Epoch: 94; Loss 1.408401 \n",
      "Step 29 of Epoch: 94; Loss 1.409283 \n",
      "Step 30 of Epoch: 94; Loss 1.409607 \n",
      "Step 31 of Epoch: 94; Loss 1.410292 \n",
      "Step 32 of Epoch: 94; Loss 1.410727 \n",
      "Step 33 of Epoch: 94; Loss 1.411490 \n",
      "Step 34 of Epoch: 94; Loss 1.412315 \n",
      "Step 35 of Epoch: 94; Loss 1.412233 \n",
      "Step 36 of Epoch: 94; Loss 1.412584 \n",
      "Step 37 of Epoch: 94; Loss 1.412527 \n",
      "Step 38 of Epoch: 94; Loss 1.413006 \n",
      "Step 39 of Epoch: 94; Loss 1.413047 \n",
      "Final Result for Epoch 94: Loss 1.413672; Val Acc 0.314669; Train Acc 0.439139\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 95; Loss 1.397026 \n",
      "Step 2 of Epoch: 95; Loss 1.388222 \n",
      "Step 3 of Epoch: 95; Loss 1.385174 \n",
      "Step 4 of Epoch: 95; Loss 1.389477 \n",
      "Step 5 of Epoch: 95; Loss 1.390415 \n",
      "Step 6 of Epoch: 95; Loss 1.390045 \n",
      "Step 7 of Epoch: 95; Loss 1.390958 \n",
      "Step 8 of Epoch: 95; Loss 1.391643 \n",
      "Step 9 of Epoch: 95; Loss 1.392991 \n",
      "Step 10 of Epoch: 95; Loss 1.395560 \n",
      "Step 11 of Epoch: 95; Loss 1.397102 \n",
      "Step 12 of Epoch: 95; Loss 1.398566 \n",
      "Step 13 of Epoch: 95; Loss 1.400428 \n",
      "Step 14 of Epoch: 95; Loss 1.401924 \n",
      "Step 15 of Epoch: 95; Loss 1.402900 \n",
      "Step 16 of Epoch: 95; Loss 1.403829 \n",
      "Step 17 of Epoch: 95; Loss 1.404427 \n",
      "Step 18 of Epoch: 95; Loss 1.405047 \n",
      "Step 19 of Epoch: 95; Loss 1.405313 \n",
      "Step 20 of Epoch: 95; Loss 1.405744 \n",
      "Step 21 of Epoch: 95; Loss 1.405489 \n",
      "Step 22 of Epoch: 95; Loss 1.406371 \n",
      "Step 23 of Epoch: 95; Loss 1.407116 \n",
      "Step 24 of Epoch: 95; Loss 1.406815 \n",
      "Step 25 of Epoch: 95; Loss 1.406790 \n",
      "Step 26 of Epoch: 95; Loss 1.407267 \n",
      "Step 27 of Epoch: 95; Loss 1.407696 \n",
      "Step 28 of Epoch: 95; Loss 1.408344 \n",
      "Step 29 of Epoch: 95; Loss 1.408671 \n",
      "Step 30 of Epoch: 95; Loss 1.409124 \n",
      "Step 31 of Epoch: 95; Loss 1.409500 \n",
      "Step 32 of Epoch: 95; Loss 1.410025 \n",
      "Step 33 of Epoch: 95; Loss 1.410675 \n",
      "Step 34 of Epoch: 95; Loss 1.411277 \n",
      "Step 35 of Epoch: 95; Loss 1.411812 \n",
      "Step 36 of Epoch: 95; Loss 1.411947 \n",
      "Step 37 of Epoch: 95; Loss 1.412248 \n",
      "Step 38 of Epoch: 95; Loss 1.412721 \n",
      "Step 39 of Epoch: 95; Loss 1.412823 \n",
      "Final Result for Epoch 95: Loss 1.412940; Val Acc 0.318437; Train Acc 0.439655\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 96; Loss 1.396754 \n",
      "Step 2 of Epoch: 96; Loss 1.399836 \n",
      "Step 3 of Epoch: 96; Loss 1.399759 \n",
      "Step 4 of Epoch: 96; Loss 1.399374 \n",
      "Step 5 of Epoch: 96; Loss 1.399233 \n",
      "Step 6 of Epoch: 96; Loss 1.398867 \n",
      "Step 7 of Epoch: 96; Loss 1.401775 \n",
      "Step 8 of Epoch: 96; Loss 1.404688 \n",
      "Step 9 of Epoch: 96; Loss 1.405809 \n",
      "Step 10 of Epoch: 96; Loss 1.405137 \n",
      "Step 11 of Epoch: 96; Loss 1.405811 \n",
      "Step 12 of Epoch: 96; Loss 1.405634 \n",
      "Step 13 of Epoch: 96; Loss 1.405362 \n",
      "Step 14 of Epoch: 96; Loss 1.405820 \n",
      "Step 15 of Epoch: 96; Loss 1.405091 \n",
      "Step 16 of Epoch: 96; Loss 1.404838 \n",
      "Step 17 of Epoch: 96; Loss 1.405225 \n",
      "Step 18 of Epoch: 96; Loss 1.404578 \n",
      "Step 19 of Epoch: 96; Loss 1.405196 \n",
      "Step 20 of Epoch: 96; Loss 1.406026 \n",
      "Step 21 of Epoch: 96; Loss 1.406911 \n",
      "Step 22 of Epoch: 96; Loss 1.407201 \n",
      "Step 23 of Epoch: 96; Loss 1.408245 \n",
      "Step 24 of Epoch: 96; Loss 1.408568 \n",
      "Step 25 of Epoch: 96; Loss 1.408616 \n",
      "Step 26 of Epoch: 96; Loss 1.408965 \n",
      "Step 27 of Epoch: 96; Loss 1.409248 \n",
      "Step 28 of Epoch: 96; Loss 1.410436 \n",
      "Step 29 of Epoch: 96; Loss 1.409902 \n",
      "Step 30 of Epoch: 96; Loss 1.410187 \n",
      "Step 31 of Epoch: 96; Loss 1.410551 \n",
      "Step 32 of Epoch: 96; Loss 1.410783 \n",
      "Step 33 of Epoch: 96; Loss 1.410668 \n",
      "Step 34 of Epoch: 96; Loss 1.410586 \n",
      "Step 35 of Epoch: 96; Loss 1.411055 \n",
      "Step 36 of Epoch: 96; Loss 1.411086 \n",
      "Step 37 of Epoch: 96; Loss 1.411361 \n",
      "Step 38 of Epoch: 96; Loss 1.412087 \n",
      "Step 39 of Epoch: 96; Loss 1.412240 \n",
      "Final Result for Epoch 96: Loss 1.412105; Val Acc 0.315952; Train Acc 0.435616\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 97; Loss 1.399560 \n",
      "Step 2 of Epoch: 97; Loss 1.405780 \n",
      "Step 3 of Epoch: 97; Loss 1.400758 \n",
      "Step 4 of Epoch: 97; Loss 1.406173 \n",
      "Step 5 of Epoch: 97; Loss 1.403929 \n",
      "Step 6 of Epoch: 97; Loss 1.402926 \n",
      "Step 7 of Epoch: 97; Loss 1.404186 \n",
      "Step 8 of Epoch: 97; Loss 1.403562 \n",
      "Step 9 of Epoch: 97; Loss 1.403806 \n",
      "Step 10 of Epoch: 97; Loss 1.403705 \n",
      "Step 11 of Epoch: 97; Loss 1.405528 \n",
      "Step 12 of Epoch: 97; Loss 1.406877 \n",
      "Step 13 of Epoch: 97; Loss 1.406655 \n",
      "Step 14 of Epoch: 97; Loss 1.405758 \n",
      "Step 15 of Epoch: 97; Loss 1.407694 \n",
      "Step 16 of Epoch: 97; Loss 1.408232 \n",
      "Step 17 of Epoch: 97; Loss 1.408139 \n",
      "Step 18 of Epoch: 97; Loss 1.408726 \n",
      "Step 19 of Epoch: 97; Loss 1.409246 \n",
      "Step 20 of Epoch: 97; Loss 1.409673 \n",
      "Step 21 of Epoch: 97; Loss 1.409391 \n",
      "Step 22 of Epoch: 97; Loss 1.408492 \n",
      "Step 23 of Epoch: 97; Loss 1.409154 \n",
      "Step 24 of Epoch: 97; Loss 1.410010 \n",
      "Step 25 of Epoch: 97; Loss 1.410323 \n",
      "Step 26 of Epoch: 97; Loss 1.410446 \n",
      "Step 27 of Epoch: 97; Loss 1.409935 \n",
      "Step 28 of Epoch: 97; Loss 1.410399 \n",
      "Step 29 of Epoch: 97; Loss 1.411223 \n",
      "Step 30 of Epoch: 97; Loss 1.411443 \n",
      "Step 31 of Epoch: 97; Loss 1.411575 \n",
      "Step 32 of Epoch: 97; Loss 1.411236 \n",
      "Step 33 of Epoch: 97; Loss 1.411287 \n",
      "Step 34 of Epoch: 97; Loss 1.412147 \n",
      "Step 35 of Epoch: 97; Loss 1.412729 \n",
      "Step 36 of Epoch: 97; Loss 1.412782 \n",
      "Step 37 of Epoch: 97; Loss 1.413338 \n",
      "Step 38 of Epoch: 97; Loss 1.412896 \n",
      "Step 39 of Epoch: 97; Loss 1.413143 \n",
      "Final Result for Epoch 97: Loss 1.413399; Val Acc 0.314228; Train Acc 0.437482\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 98; Loss 1.387494 \n",
      "Step 2 of Epoch: 98; Loss 1.383754 \n",
      "Step 3 of Epoch: 98; Loss 1.389145 \n",
      "Step 4 of Epoch: 98; Loss 1.392940 \n",
      "Step 5 of Epoch: 98; Loss 1.394807 \n",
      "Step 6 of Epoch: 98; Loss 1.396027 \n",
      "Step 7 of Epoch: 98; Loss 1.395546 \n",
      "Step 8 of Epoch: 98; Loss 1.397488 \n",
      "Step 9 of Epoch: 98; Loss 1.399545 \n",
      "Step 10 of Epoch: 98; Loss 1.400418 \n",
      "Step 11 of Epoch: 98; Loss 1.399768 \n",
      "Step 12 of Epoch: 98; Loss 1.402748 \n",
      "Step 13 of Epoch: 98; Loss 1.404148 \n",
      "Step 14 of Epoch: 98; Loss 1.403395 \n",
      "Step 15 of Epoch: 98; Loss 1.403850 \n",
      "Step 16 of Epoch: 98; Loss 1.404076 \n",
      "Step 17 of Epoch: 98; Loss 1.403726 \n",
      "Step 18 of Epoch: 98; Loss 1.404700 \n",
      "Step 19 of Epoch: 98; Loss 1.405315 \n",
      "Step 20 of Epoch: 98; Loss 1.405653 \n",
      "Step 21 of Epoch: 98; Loss 1.406192 \n",
      "Step 22 of Epoch: 98; Loss 1.406723 \n",
      "Step 23 of Epoch: 98; Loss 1.406794 \n",
      "Step 24 of Epoch: 98; Loss 1.407398 \n",
      "Step 25 of Epoch: 98; Loss 1.407421 \n",
      "Step 26 of Epoch: 98; Loss 1.407777 \n",
      "Step 27 of Epoch: 98; Loss 1.408457 \n",
      "Step 28 of Epoch: 98; Loss 1.409057 \n",
      "Step 29 of Epoch: 98; Loss 1.408969 \n",
      "Step 30 of Epoch: 98; Loss 1.409428 \n",
      "Step 31 of Epoch: 98; Loss 1.410146 \n",
      "Step 32 of Epoch: 98; Loss 1.411509 \n",
      "Step 33 of Epoch: 98; Loss 1.411703 \n",
      "Step 34 of Epoch: 98; Loss 1.411921 \n",
      "Step 35 of Epoch: 98; Loss 1.411803 \n",
      "Step 36 of Epoch: 98; Loss 1.412203 \n",
      "Step 37 of Epoch: 98; Loss 1.412196 \n",
      "Step 38 of Epoch: 98; Loss 1.412257 \n",
      "Step 39 of Epoch: 98; Loss 1.412136 \n",
      "Final Result for Epoch 98: Loss 1.412210; Val Acc 0.315271; Train Acc 0.439234\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 99; Loss 1.383733 \n",
      "Step 2 of Epoch: 99; Loss 1.383189 \n",
      "Step 3 of Epoch: 99; Loss 1.389932 \n",
      "Step 4 of Epoch: 99; Loss 1.390931 \n",
      "Step 5 of Epoch: 99; Loss 1.394290 \n",
      "Step 6 of Epoch: 99; Loss 1.395024 \n",
      "Step 7 of Epoch: 99; Loss 1.396043 \n",
      "Step 8 of Epoch: 99; Loss 1.396293 \n",
      "Step 9 of Epoch: 99; Loss 1.397879 \n",
      "Step 10 of Epoch: 99; Loss 1.399222 \n",
      "Step 11 of Epoch: 99; Loss 1.401375 \n",
      "Step 12 of Epoch: 99; Loss 1.403203 \n",
      "Step 13 of Epoch: 99; Loss 1.403333 \n",
      "Step 14 of Epoch: 99; Loss 1.403508 \n",
      "Step 15 of Epoch: 99; Loss 1.402937 \n",
      "Step 16 of Epoch: 99; Loss 1.403991 \n",
      "Step 17 of Epoch: 99; Loss 1.405588 \n",
      "Step 18 of Epoch: 99; Loss 1.405027 \n",
      "Step 19 of Epoch: 99; Loss 1.405571 \n",
      "Step 20 of Epoch: 99; Loss 1.405208 \n",
      "Step 21 of Epoch: 99; Loss 1.405562 \n",
      "Step 22 of Epoch: 99; Loss 1.405962 \n",
      "Step 23 of Epoch: 99; Loss 1.406585 \n",
      "Step 24 of Epoch: 99; Loss 1.406809 \n",
      "Step 25 of Epoch: 99; Loss 1.407620 \n",
      "Step 26 of Epoch: 99; Loss 1.407068 \n",
      "Step 27 of Epoch: 99; Loss 1.407956 \n",
      "Step 28 of Epoch: 99; Loss 1.408123 \n",
      "Step 29 of Epoch: 99; Loss 1.408431 \n",
      "Step 30 of Epoch: 99; Loss 1.409602 \n",
      "Step 31 of Epoch: 99; Loss 1.409911 \n",
      "Step 32 of Epoch: 99; Loss 1.410319 \n",
      "Step 33 of Epoch: 99; Loss 1.410066 \n",
      "Step 34 of Epoch: 99; Loss 1.410559 \n",
      "Step 35 of Epoch: 99; Loss 1.410687 \n",
      "Step 36 of Epoch: 99; Loss 1.410783 \n",
      "Step 37 of Epoch: 99; Loss 1.411801 \n",
      "Step 38 of Epoch: 99; Loss 1.411793 \n",
      "Step 39 of Epoch: 99; Loss 1.412001 \n",
      "Final Result for Epoch 99: Loss 1.411909; Val Acc 0.316152; Train Acc 0.438078\n",
      "-------------------------------------------------------------\n",
      "Step 1 of Epoch: 100; Loss 1.408814 \n",
      "Step 2 of Epoch: 100; Loss 1.393864 \n",
      "Step 3 of Epoch: 100; Loss 1.388019 \n",
      "Step 4 of Epoch: 100; Loss 1.391580 \n",
      "Step 5 of Epoch: 100; Loss 1.394526 \n",
      "Step 6 of Epoch: 100; Loss 1.396963 \n",
      "Step 7 of Epoch: 100; Loss 1.398307 \n",
      "Step 8 of Epoch: 100; Loss 1.400447 \n",
      "Step 9 of Epoch: 100; Loss 1.402430 \n",
      "Step 10 of Epoch: 100; Loss 1.404488 \n",
      "Step 11 of Epoch: 100; Loss 1.405824 \n",
      "Step 12 of Epoch: 100; Loss 1.405202 \n",
      "Step 13 of Epoch: 100; Loss 1.404658 \n",
      "Step 14 of Epoch: 100; Loss 1.404762 \n",
      "Step 15 of Epoch: 100; Loss 1.405060 \n",
      "Step 16 of Epoch: 100; Loss 1.405369 \n",
      "Step 17 of Epoch: 100; Loss 1.405772 \n",
      "Step 18 of Epoch: 100; Loss 1.405112 \n",
      "Step 19 of Epoch: 100; Loss 1.405311 \n",
      "Step 20 of Epoch: 100; Loss 1.406101 \n",
      "Step 21 of Epoch: 100; Loss 1.405607 \n",
      "Step 22 of Epoch: 100; Loss 1.406129 \n",
      "Step 23 of Epoch: 100; Loss 1.406177 \n",
      "Step 24 of Epoch: 100; Loss 1.406189 \n",
      "Step 25 of Epoch: 100; Loss 1.406187 \n",
      "Step 26 of Epoch: 100; Loss 1.406293 \n",
      "Step 27 of Epoch: 100; Loss 1.406305 \n",
      "Step 28 of Epoch: 100; Loss 1.406484 \n",
      "Step 29 of Epoch: 100; Loss 1.406885 \n",
      "Step 30 of Epoch: 100; Loss 1.407439 \n",
      "Step 31 of Epoch: 100; Loss 1.408497 \n",
      "Step 32 of Epoch: 100; Loss 1.408561 \n",
      "Step 33 of Epoch: 100; Loss 1.408638 \n",
      "Step 34 of Epoch: 100; Loss 1.409164 \n",
      "Step 35 of Epoch: 100; Loss 1.409706 \n",
      "Step 36 of Epoch: 100; Loss 1.410188 \n",
      "Step 37 of Epoch: 100; Loss 1.410657 \n",
      "Step 38 of Epoch: 100; Loss 1.411085 \n",
      "Step 39 of Epoch: 100; Loss 1.411298 \n",
      "Final Result for Epoch 100: Loss 1.411764; Val Acc 0.316232; Train Acc 0.438504\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZ+wkwqnNcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training time: 5519 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "model = TweetLSTM(input_size=50, hidden_size=50, num_classes=num_classes,use_gpu=True)\n",
    "train_rnn_network(model, train_loader, valid_loader, num_epochs=100, learning_rate=1e-3,use_gpu=True)\n",
    "end = time.time()\n",
    "print(\"total training time:\", int(end -start), \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA282W2hdZau",
    "colab_type": "text"
   },
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "iGIvu-ysukwk",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint_biLSTM.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0TSZ1O2dfo-",
    "colab_type": "text"
   },
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "h9plkTXediGq",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('checkpoint_biLSTM.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "eGTAC6Wudkv3",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqbUdJgD0iec",
    "colab_type": "text"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "TMF8kPVVvh91",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Tweet Test\n",
    "happy_tweet = 'Im happy'\n",
    "sad_tweet = 'Im sad'\n",
    "angry = 'Im angry'\n",
    "surprised_tweet = 'Im surprised'\n",
    "disgusted_tweet = 'Im disgusted'\n",
    "afraid_tweet = 'Im afraid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "eO3IhcoX0vqy",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def tweet_to_glove_index(tweet,glove_dict):\n",
    "    tweets_ints = []\n",
    "    tweet = tweet.lower()\n",
    "    idxs = [glove_dict.stoi[w]        # lookup the index of word\n",
    "            for w in tweet.split()\n",
    "            if w in glove_dict.stoi] # keep words that has an embedding\n",
    "    tweets_ints.append(idxs)\n",
    "    return tweets_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "eBnsPtrJ1NgH",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "int_to_emotion = {0:'Happy', 1:'Sad' , 2:'Angry', 3:'Surprised', 4:'Disgusted', 5:'Afraid'}\n",
    "\n",
    "def predict(model, test_tweet, sequence_length=max(tweets_lens),use_gpu=True):\n",
    "    \n",
    "    \n",
    "    # tokenize tweet\n",
    "    test_ints = tweet_to_glove_index(test_tweet,glove)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    if use_gpu:\n",
    "      feature_tensor = torch.from_numpy(features).cuda()\n",
    "    else:\n",
    "      feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    \n",
    "    # get the output from the model\n",
    "    output = model(feature_tensor)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    output_prob = nn.functional.softmax(output,dim=1)\n",
    "    top_n_pred = output_prob.topk(3,dim=1) ## top 3 preds\n",
    "    top_n_pred_prob, top_n_pred_index = top_n_pred[0].detach().cpu().numpy()[0], top_n_pred[1].detach().cpu().numpy()[0]\n",
    "    print(test_tweet)\n",
    "    print('Prediction:')\n",
    "    for prob,index in zip(top_n_pred_prob,top_n_pred_index):\n",
    "      print(int_to_emotion[index] , 'with' , str(int(prob*100))+\"%\", 'confidence')\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "TyMRFkX43fhn",
    "colab_type": "code",
    "outputId": "a20d410f-c3c5-4896-bd04-2c625f8b2656",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im happy\n",
      "Prediction:\n",
      "Happy with 84% confidence\n",
      "Angry with 4% confidence\n",
      "Sad with 3% confidence\n",
      "---------------\n",
      "Im sad\n",
      "Prediction:\n",
      "Sad with 77% confidence\n",
      "Disgusted with 7% confidence\n",
      "Angry with 5% confidence\n",
      "---------------\n",
      "Im angry\n",
      "Prediction:\n",
      "Angry with 39% confidence\n",
      "Disgusted with 25% confidence\n",
      "Afraid with 12% confidence\n",
      "---------------\n",
      "Im surprised\n",
      "Prediction:\n",
      "Happy with 36% confidence\n",
      "Afraid with 24% confidence\n",
      "Surprised with 22% confidence\n",
      "---------------\n",
      "Im disgusted\n",
      "Prediction:\n",
      "Disgusted with 48% confidence\n",
      "Sad with 32% confidence\n",
      "Angry with 13% confidence\n",
      "---------------\n",
      "Im afraid\n",
      "Prediction:\n",
      "Sad with 38% confidence\n",
      "Afraid with 18% confidence\n",
      "Happy with 16% confidence\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "predict(model,happy_tweet)\n",
    "predict(model,sad_tweet)\n",
    "predict(model,angry)\n",
    "predict(model,surprised_tweet)\n",
    "predict(model,disgusted_tweet)\n",
    "predict(model,afraid_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "AUhEs_I4hJ8M",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "tweets = ['Im so happy',\n",
    "          'I feel so down today',\n",
    "          'It boils my blood to see you',\n",
    "         'wow what a nice car',\n",
    "         'im sick of this shit',\n",
    "         'theres a stranger at my home']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "MollcketAEYZ",
    "colab_type": "code",
    "outputId": "313d360d-082b-4830-b2e0-46c457dd55f4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im so happy\n",
      "Prediction:\n",
      "Happy with 60% confidence\n",
      "Sad with 19% confidence\n",
      "Angry with 7% confidence\n",
      "---------------\n",
      "I feel so down today\n",
      "Prediction:\n",
      "Sad with 51% confidence\n",
      "Disgusted with 17% confidence\n",
      "Angry with 10% confidence\n",
      "---------------\n",
      "It boils my blood to see you\n",
      "Prediction:\n",
      "Angry with 32% confidence\n",
      "Disgusted with 19% confidence\n",
      "Sad with 18% confidence\n",
      "---------------\n",
      "wow what a nice car\n",
      "Prediction:\n",
      "Surprised with 49% confidence\n",
      "Afraid with 36% confidence\n",
      "Happy with 9% confidence\n",
      "---------------\n",
      "im sick of this shit\n",
      "Prediction:\n",
      "Disgusted with 28% confidence\n",
      "Angry with 25% confidence\n",
      "Sad with 25% confidence\n",
      "---------------\n",
      "theres a stranger at my home\n",
      "Prediction:\n",
      "Surprised with 22% confidence\n",
      "Sad with 19% confidence\n",
      "Happy with 18% confidence\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets:\n",
    "  predict(model, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ttE3WT7qAGgM",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tweaked_Base_Model.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
